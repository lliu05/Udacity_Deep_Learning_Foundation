{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7e9916630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "#import sklearn\n",
    "#from sklearn.preprocessing import normalize\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #return np.array(x/255.0)\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x)\n",
    "    labels = np.array(x)\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(labels)\n",
    "    lb.classes_ = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "    return lb.transform(labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None]+list(image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None]+[n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_depth = int(x_tensor.get_shape()[3])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([*conv_ksize, input_depth, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([conv_num_outputs]))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, [1, *conv_strides, 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, [1, *pool_ksize, 1], [1, *pool_ksize, 1], padding='SAME') \n",
    "\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    flatten_size = x_shape[1] * x_shape[2] * x_shape[3]\n",
    "    \n",
    "    return tf.reshape(x_tensor, [-1, flatten_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dim = int(x_tensor.shape[1])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([dim, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    \n",
    "    fully_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    fully_layer = tf.nn.relu(fully_layer)\n",
    "    \n",
    "    return fully_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dim = int(x_tensor.shape[1])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([dim, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Layer1\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (8,8)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (1,1)\n",
    "    keep_prob = 0.5\n",
    "    num_outputs = 10\n",
    "    conv_layer = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Layer2\n",
    "    conv_num_outputs = 128\n",
    "    conv_ksize = (4, 4)\n",
    "    pool_ksize = (3, 3)\n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "     \n",
    "    # Layer3    \n",
    "    conv_num_outputs = 256\n",
    "    conv_ksize = (2, 2)\n",
    "    pool_ksize = (2, 2) \n",
    "    conv_layer = conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_layer = flatten(conv_layer)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Layer1\n",
    "    fully_layer = fully_conn(flatten_layer, 1500)\n",
    "    fully_layer = tf.nn.dropout(fully_layer, keep_prob)\n",
    "    \n",
    "    # Layer2\n",
    "    fully_layer = fully_conn(fully_layer, 1000)\n",
    "    fully_layer = tf.nn.dropout(fully_layer, keep_prob)\n",
    "    \n",
    "    # Layer3\n",
    "    fully_layer = fully_conn(fully_layer, 500)\n",
    "    fully_layer = tf.nn.dropout(fully_layer, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_layer = output(fully_layer, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    the_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "\n",
    "    print('Loss: ', loss, ' Accuracy: ', the_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 500\n",
    "batch_size = 2000\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  18.8016  Accuracy:  0.107\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  8.58027  Accuracy:  0.1194\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  5.45483  Accuracy:  0.1312\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  4.17377  Accuracy:  0.146\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  3.40092  Accuracy:  0.146\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  3.0129  Accuracy:  0.1522\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  2.70446  Accuracy:  0.159\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  2.6447  Accuracy:  0.1714\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  2.52656  Accuracy:  0.1776\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  2.39138  Accuracy:  0.1826\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  2.36847  Accuracy:  0.188\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  2.30555  Accuracy:  0.1992\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  2.29237  Accuracy:  0.2092\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  2.25465  Accuracy:  0.204\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  2.18714  Accuracy:  0.2154\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  2.19402  Accuracy:  0.2162\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  2.15062  Accuracy:  0.2244\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  2.21047  Accuracy:  0.2206\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  2.10702  Accuracy:  0.2246\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  2.07721  Accuracy:  0.2456\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  2.05565  Accuracy:  0.2406\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  2.02977  Accuracy:  0.2478\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  2.0695  Accuracy:  0.2436\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  2.00137  Accuracy:  0.2612\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  2.04274  Accuracy:  0.2622\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  1.95327  Accuracy:  0.2624\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  2.00966  Accuracy:  0.2622\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  1.94477  Accuracy:  0.2716\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  1.92471  Accuracy:  0.2754\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  1.9252  Accuracy:  0.2822\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  1.92094  Accuracy:  0.2882\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  1.86168  Accuracy:  0.2802\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  1.87069  Accuracy:  0.2924\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  1.83901  Accuracy:  0.3032\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  1.87764  Accuracy:  0.2982\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  1.84006  Accuracy:  0.303\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  1.79736  Accuracy:  0.305\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  1.81937  Accuracy:  0.3168\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  1.84055  Accuracy:  0.321\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  1.79987  Accuracy:  0.314\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  1.83393  Accuracy:  0.3208\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  1.80112  Accuracy:  0.3242\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  1.77231  Accuracy:  0.3254\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  1.77802  Accuracy:  0.3258\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  1.73783  Accuracy:  0.3342\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  1.68441  Accuracy:  0.3508\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  1.69317  Accuracy:  0.3458\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  1.66513  Accuracy:  0.349\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  1.68537  Accuracy:  0.3508\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  1.66858  Accuracy:  0.3464\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  1.65035  Accuracy:  0.3514\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  1.63954  Accuracy:  0.3652\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  1.62888  Accuracy:  0.3586\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  1.65361  Accuracy:  0.3684\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  1.61641  Accuracy:  0.3732\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  1.5645  Accuracy:  0.3748\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  1.59909  Accuracy:  0.3728\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  1.57388  Accuracy:  0.3774\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  1.60765  Accuracy:  0.3778\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  1.52512  Accuracy:  0.3804\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  1.57465  Accuracy:  0.3962\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  1.53865  Accuracy:  0.3876\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  1.55182  Accuracy:  0.3906\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  1.50148  Accuracy:  0.3864\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  1.48843  Accuracy:  0.4032\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  1.51301  Accuracy:  0.399\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  1.48245  Accuracy:  0.4004\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  1.49097  Accuracy:  0.402\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  1.50768  Accuracy:  0.401\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  1.45465  Accuracy:  0.4102\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  1.4136  Accuracy:  0.4156\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  1.4166  Accuracy:  0.4128\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  1.43571  Accuracy:  0.4088\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  1.44229  Accuracy:  0.406\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  1.45878  Accuracy:  0.403\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  1.45384  Accuracy:  0.407\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  1.39419  Accuracy:  0.4166\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  1.39432  Accuracy:  0.4094\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  1.35715  Accuracy:  0.4148\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  1.33719  Accuracy:  0.4174\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  1.33051  Accuracy:  0.4408\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  1.33513  Accuracy:  0.427\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  1.34808  Accuracy:  0.422\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  1.34945  Accuracy:  0.4336\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  1.30326  Accuracy:  0.4424\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  1.30991  Accuracy:  0.434\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  1.25123  Accuracy:  0.4392\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  1.23158  Accuracy:  0.4472\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  1.32283  Accuracy:  0.4346\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  1.25591  Accuracy:  0.4392\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  1.26796  Accuracy:  0.435\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  1.27368  Accuracy:  0.4436\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  1.22351  Accuracy:  0.4426\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  1.25063  Accuracy:  0.446\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  1.19577  Accuracy:  0.4518\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  1.18991  Accuracy:  0.4538\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  1.16648  Accuracy:  0.4416\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  1.22392  Accuracy:  0.4414\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  1.18432  Accuracy:  0.4538\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  1.14964  Accuracy:  0.4532\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  1.16617  Accuracy:  0.4666\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  1.10775  Accuracy:  0.4658\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  1.15455  Accuracy:  0.4524\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  1.12899  Accuracy:  0.4638\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  1.172  Accuracy:  0.4486\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  1.14877  Accuracy:  0.4564\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  1.15343  Accuracy:  0.4556\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  1.16525  Accuracy:  0.456\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  1.14362  Accuracy:  0.4526\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  1.06448  Accuracy:  0.4644\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  1.08714  Accuracy:  0.4614\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  1.09581  Accuracy:  0.4676\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  1.05518  Accuracy:  0.476\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  1.06092  Accuracy:  0.4776\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  1.0645  Accuracy:  0.4668\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  1.11245  Accuracy:  0.4722\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  0.993834  Accuracy:  0.474\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  0.994828  Accuracy:  0.4734\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  0.989212  Accuracy:  0.4724\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  0.970962  Accuracy:  0.4866\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  0.935545  Accuracy:  0.4892\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  0.948999  Accuracy:  0.4692\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  0.973593  Accuracy:  0.4834\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  0.981147  Accuracy:  0.4804\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  0.948365  Accuracy:  0.4888\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  0.913652  Accuracy:  0.4746\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  0.918711  Accuracy:  0.4876\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  0.902368  Accuracy:  0.4756\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  0.877383  Accuracy:  0.4934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  0.877449  Accuracy:  0.4976\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  0.87412  Accuracy:  0.4882\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  0.926033  Accuracy:  0.4764\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  0.877363  Accuracy:  0.483\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  0.861563  Accuracy:  0.4958\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  0.896488  Accuracy:  0.4808\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  0.980363  Accuracy:  0.4566\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  1.00203  Accuracy:  0.4566\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  0.961751  Accuracy:  0.4702\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  0.905078  Accuracy:  0.4792\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  0.889246  Accuracy:  0.494\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  0.819541  Accuracy:  0.4884\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  0.817081  Accuracy:  0.4934\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  0.786707  Accuracy:  0.4852\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  0.810019  Accuracy:  0.4956\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  0.782243  Accuracy:  0.504\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  0.753256  Accuracy:  0.5042\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  0.75442  Accuracy:  0.4924\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  0.747401  Accuracy:  0.5046\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  0.766105  Accuracy:  0.495\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  0.721359  Accuracy:  0.5036\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  0.713017  Accuracy:  0.4972\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  0.702305  Accuracy:  0.5036\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  0.760115  Accuracy:  0.4906\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  0.862978  Accuracy:  0.475\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  0.787366  Accuracy:  0.4934\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  0.747225  Accuracy:  0.4822\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  0.717666  Accuracy:  0.4986\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  0.746425  Accuracy:  0.4946\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  0.734526  Accuracy:  0.49\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  0.659761  Accuracy:  0.5096\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  0.697292  Accuracy:  0.4996\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  0.916638  Accuracy:  0.4658\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  0.697366  Accuracy:  0.499\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.811762  Accuracy:  0.4672\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.724955  Accuracy:  0.501\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  0.799381  Accuracy:  0.4846\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.706706  Accuracy:  0.4946\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.711389  Accuracy:  0.4898\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.691671  Accuracy:  0.4858\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.639693  Accuracy:  0.5022\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.633457  Accuracy:  0.5008\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.639905  Accuracy:  0.5054\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.662562  Accuracy:  0.5034\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.602279  Accuracy:  0.5106\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.621672  Accuracy:  0.5058\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.585306  Accuracy:  0.5084\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:  0.605921  Accuracy:  0.5036\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.514959  Accuracy:  0.513\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.545832  Accuracy:  0.518\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.524908  Accuracy:  0.5158\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.512727  Accuracy:  0.5112\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.526165  Accuracy:  0.5152\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.490133  Accuracy:  0.5148\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.511603  Accuracy:  0.5108\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.575326  Accuracy:  0.5016\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.564211  Accuracy:  0.4986\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.536229  Accuracy:  0.5074\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.537966  Accuracy:  0.49\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.599104  Accuracy:  0.4936\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.502125  Accuracy:  0.5006\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.517691  Accuracy:  0.4954\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.59696  Accuracy:  0.474\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  1.18182  Accuracy:  0.4338\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  1.04734  Accuracy:  0.4236\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.846448  Accuracy:  0.4616\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.747735  Accuracy:  0.4808\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.650927  Accuracy:  0.5012\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.645479  Accuracy:  0.5052\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.645873  Accuracy:  0.4962\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.599675  Accuracy:  0.4946\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.64903  Accuracy:  0.4888\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.516187  Accuracy:  0.502\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.490366  Accuracy:  0.509\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.543326  Accuracy:  0.486\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.644273  Accuracy:  0.4662\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.534345  Accuracy:  0.5066\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.574375  Accuracy:  0.4968\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.669553  Accuracy:  0.4748\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.707393  Accuracy:  0.4572\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.807389  Accuracy:  0.4482\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.643214  Accuracy:  0.4782\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.599115  Accuracy:  0.4866\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.628666  Accuracy:  0.4648\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.744717  Accuracy:  0.4688\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.604627  Accuracy:  0.475\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.499705  Accuracy:  0.5148\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.58319  Accuracy:  0.5072\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.419094  Accuracy:  0.52\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.42823  Accuracy:  0.51\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.431424  Accuracy:  0.5134\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.448168  Accuracy:  0.5174\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.358421  Accuracy:  0.519\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.386872  Accuracy:  0.5128\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.400277  Accuracy:  0.5124\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.362379  Accuracy:  0.5098\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.379279  Accuracy:  0.5198\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.357886  Accuracy:  0.527\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.349791  Accuracy:  0.5192\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.385857  Accuracy:  0.5078\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.356189  Accuracy:  0.5082\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.322996  Accuracy:  0.5172\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.316706  Accuracy:  0.5186\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.3315  Accuracy:  0.516\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.359032  Accuracy:  0.4964\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.432382  Accuracy:  0.4826\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.542201  Accuracy:  0.4644\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.398035  Accuracy:  0.4976\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.414571  Accuracy:  0.5106\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.388533  Accuracy:  0.4966\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.345288  Accuracy:  0.5192\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.309149  Accuracy:  0.5194\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.274102  Accuracy:  0.5298\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.262769  Accuracy:  0.5162\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.253629  Accuracy:  0.5234\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.239541  Accuracy:  0.5184\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.247427  Accuracy:  0.5244\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.236506  Accuracy:  0.5166\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.236439  Accuracy:  0.521\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.212564  Accuracy:  0.525\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.218337  Accuracy:  0.5312\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.220871  Accuracy:  0.5252\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.251459  Accuracy:  0.5126\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.292724  Accuracy:  0.5022\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.269117  Accuracy:  0.5116\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.270236  Accuracy:  0.5102\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.36766  Accuracy:  0.4782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257, CIFAR-10 Batch 1:  Loss:  0.384824  Accuracy:  0.4772\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:  0.294823  Accuracy:  0.505\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:  0.355202  Accuracy:  0.4966\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:  0.336519  Accuracy:  0.4992\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:  0.392133  Accuracy:  0.501\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:  0.317637  Accuracy:  0.5156\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:  0.263845  Accuracy:  0.5186\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:  0.234372  Accuracy:  0.5134\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:  0.235697  Accuracy:  0.531\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:  0.289338  Accuracy:  0.5122\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:  0.328292  Accuracy:  0.5076\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:  0.269861  Accuracy:  0.5172\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:  0.199315  Accuracy:  0.525\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:  0.206254  Accuracy:  0.5174\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:  0.204074  Accuracy:  0.516\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:  0.166185  Accuracy:  0.5314\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:  0.166108  Accuracy:  0.529\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:  0.157885  Accuracy:  0.5248\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:  0.150746  Accuracy:  0.525\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:  0.15904  Accuracy:  0.5344\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:  0.234207  Accuracy:  0.5092\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:  0.333586  Accuracy:  0.498\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:  0.252935  Accuracy:  0.5064\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:  0.35859  Accuracy:  0.4878\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:  0.291551  Accuracy:  0.496\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:  0.239942  Accuracy:  0.507\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:  0.206837  Accuracy:  0.512\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:  0.251727  Accuracy:  0.51\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:  0.212275  Accuracy:  0.515\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:  0.196011  Accuracy:  0.516\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:  0.15493  Accuracy:  0.5234\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:  0.14531  Accuracy:  0.5306\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:  0.142946  Accuracy:  0.5356\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:  0.125638  Accuracy:  0.532\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:  0.139893  Accuracy:  0.5182\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:  0.112717  Accuracy:  0.5248\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:  0.115245  Accuracy:  0.5418\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:  0.0943661  Accuracy:  0.5286\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:  0.100435  Accuracy:  0.5208\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:  0.0821647  Accuracy:  0.5308\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:  0.102383  Accuracy:  0.529\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:  0.109607  Accuracy:  0.5282\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:  0.123982  Accuracy:  0.5242\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:  0.139454  Accuracy:  0.528\n",
      "Epoch 301, CIFAR-10 Batch 1:  Loss:  0.17776  Accuracy:  0.509\n",
      "Epoch 302, CIFAR-10 Batch 1:  Loss:  0.198364  Accuracy:  0.495\n",
      "Epoch 303, CIFAR-10 Batch 1:  Loss:  0.18339  Accuracy:  0.5122\n",
      "Epoch 304, CIFAR-10 Batch 1:  Loss:  0.148747  Accuracy:  0.5158\n",
      "Epoch 305, CIFAR-10 Batch 1:  Loss:  0.187031  Accuracy:  0.511\n",
      "Epoch 306, CIFAR-10 Batch 1:  Loss:  0.107608  Accuracy:  0.5286\n",
      "Epoch 307, CIFAR-10 Batch 1:  Loss:  0.120718  Accuracy:  0.5236\n",
      "Epoch 308, CIFAR-10 Batch 1:  Loss:  0.152202  Accuracy:  0.522\n",
      "Epoch 309, CIFAR-10 Batch 1:  Loss:  0.132057  Accuracy:  0.5214\n",
      "Epoch 310, CIFAR-10 Batch 1:  Loss:  0.109433  Accuracy:  0.5302\n",
      "Epoch 311, CIFAR-10 Batch 1:  Loss:  0.0856202  Accuracy:  0.5268\n",
      "Epoch 312, CIFAR-10 Batch 1:  Loss:  0.0925197  Accuracy:  0.525\n",
      "Epoch 313, CIFAR-10 Batch 1:  Loss:  0.0773042  Accuracy:  0.5248\n",
      "Epoch 314, CIFAR-10 Batch 1:  Loss:  0.0898088  Accuracy:  0.525\n",
      "Epoch 315, CIFAR-10 Batch 1:  Loss:  0.0738709  Accuracy:  0.5278\n",
      "Epoch 316, CIFAR-10 Batch 1:  Loss:  0.0817202  Accuracy:  0.522\n",
      "Epoch 317, CIFAR-10 Batch 1:  Loss:  0.118594  Accuracy:  0.5294\n",
      "Epoch 318, CIFAR-10 Batch 1:  Loss:  0.108565  Accuracy:  0.5262\n",
      "Epoch 319, CIFAR-10 Batch 1:  Loss:  0.103775  Accuracy:  0.5284\n",
      "Epoch 320, CIFAR-10 Batch 1:  Loss:  0.10975  Accuracy:  0.5056\n",
      "Epoch 321, CIFAR-10 Batch 1:  Loss:  0.09823  Accuracy:  0.5112\n",
      "Epoch 322, CIFAR-10 Batch 1:  Loss:  0.085448  Accuracy:  0.5128\n",
      "Epoch 323, CIFAR-10 Batch 1:  Loss:  0.138793  Accuracy:  0.5132\n",
      "Epoch 324, CIFAR-10 Batch 1:  Loss:  0.0949591  Accuracy:  0.5196\n",
      "Epoch 325, CIFAR-10 Batch 1:  Loss:  0.0815444  Accuracy:  0.5214\n",
      "Epoch 326, CIFAR-10 Batch 1:  Loss:  0.0895912  Accuracy:  0.5246\n",
      "Epoch 327, CIFAR-10 Batch 1:  Loss:  0.0716236  Accuracy:  0.5212\n",
      "Epoch 328, CIFAR-10 Batch 1:  Loss:  0.0812365  Accuracy:  0.5306\n",
      "Epoch 329, CIFAR-10 Batch 1:  Loss:  0.0576042  Accuracy:  0.5262\n",
      "Epoch 330, CIFAR-10 Batch 1:  Loss:  0.0671044  Accuracy:  0.5234\n",
      "Epoch 331, CIFAR-10 Batch 1:  Loss:  0.0574074  Accuracy:  0.5364\n",
      "Epoch 332, CIFAR-10 Batch 1:  Loss:  0.0812247  Accuracy:  0.5336\n",
      "Epoch 333, CIFAR-10 Batch 1:  Loss:  0.0746141  Accuracy:  0.5298\n",
      "Epoch 334, CIFAR-10 Batch 1:  Loss:  0.0865497  Accuracy:  0.5254\n",
      "Epoch 335, CIFAR-10 Batch 1:  Loss:  0.116897  Accuracy:  0.5158\n",
      "Epoch 336, CIFAR-10 Batch 1:  Loss:  0.121997  Accuracy:  0.5142\n",
      "Epoch 337, CIFAR-10 Batch 1:  Loss:  0.126681  Accuracy:  0.5162\n",
      "Epoch 338, CIFAR-10 Batch 1:  Loss:  0.138121  Accuracy:  0.5108\n",
      "Epoch 339, CIFAR-10 Batch 1:  Loss:  0.111065  Accuracy:  0.51\n",
      "Epoch 340, CIFAR-10 Batch 1:  Loss:  0.139589  Accuracy:  0.5052\n",
      "Epoch 341, CIFAR-10 Batch 1:  Loss:  0.0800359  Accuracy:  0.5228\n",
      "Epoch 342, CIFAR-10 Batch 1:  Loss:  0.0710214  Accuracy:  0.5162\n",
      "Epoch 343, CIFAR-10 Batch 1:  Loss:  0.0668685  Accuracy:  0.52\n",
      "Epoch 344, CIFAR-10 Batch 1:  Loss:  0.0828241  Accuracy:  0.5164\n",
      "Epoch 345, CIFAR-10 Batch 1:  Loss:  0.0618945  Accuracy:  0.5282\n",
      "Epoch 346, CIFAR-10 Batch 1:  Loss:  0.0710552  Accuracy:  0.5262\n",
      "Epoch 347, CIFAR-10 Batch 1:  Loss:  0.0963556  Accuracy:  0.5222\n",
      "Epoch 348, CIFAR-10 Batch 1:  Loss:  0.0614129  Accuracy:  0.5266\n",
      "Epoch 349, CIFAR-10 Batch 1:  Loss:  0.0691959  Accuracy:  0.5162\n",
      "Epoch 350, CIFAR-10 Batch 1:  Loss:  0.046362  Accuracy:  0.5232\n",
      "Epoch 351, CIFAR-10 Batch 1:  Loss:  0.0459584  Accuracy:  0.53\n",
      "Epoch 352, CIFAR-10 Batch 1:  Loss:  0.0508373  Accuracy:  0.5336\n",
      "Epoch 353, CIFAR-10 Batch 1:  Loss:  0.0713856  Accuracy:  0.522\n",
      "Epoch 354, CIFAR-10 Batch 1:  Loss:  0.0388421  Accuracy:  0.5306\n",
      "Epoch 355, CIFAR-10 Batch 1:  Loss:  0.049676  Accuracy:  0.5262\n",
      "Epoch 356, CIFAR-10 Batch 1:  Loss:  0.0425697  Accuracy:  0.5366\n",
      "Epoch 357, CIFAR-10 Batch 1:  Loss:  0.0396194  Accuracy:  0.5296\n",
      "Epoch 358, CIFAR-10 Batch 1:  Loss:  0.0443453  Accuracy:  0.5256\n",
      "Epoch 359, CIFAR-10 Batch 1:  Loss:  0.0411146  Accuracy:  0.5326\n",
      "Epoch 360, CIFAR-10 Batch 1:  Loss:  0.0436532  Accuracy:  0.5354\n",
      "Epoch 361, CIFAR-10 Batch 1:  Loss:  0.0426322  Accuracy:  0.5252\n",
      "Epoch 362, CIFAR-10 Batch 1:  Loss:  0.0425285  Accuracy:  0.5278\n",
      "Epoch 363, CIFAR-10 Batch 1:  Loss:  0.0584942  Accuracy:  0.5264\n",
      "Epoch 364, CIFAR-10 Batch 1:  Loss:  0.0337532  Accuracy:  0.5248\n",
      "Epoch 365, CIFAR-10 Batch 1:  Loss:  0.024231  Accuracy:  0.5256\n",
      "Epoch 366, CIFAR-10 Batch 1:  Loss:  0.0297308  Accuracy:  0.5312\n",
      "Epoch 367, CIFAR-10 Batch 1:  Loss:  0.0342483  Accuracy:  0.5336\n",
      "Epoch 368, CIFAR-10 Batch 1:  Loss:  0.0442952  Accuracy:  0.528\n",
      "Epoch 369, CIFAR-10 Batch 1:  Loss:  0.0302508  Accuracy:  0.5278\n",
      "Epoch 370, CIFAR-10 Batch 1:  Loss:  0.047216  Accuracy:  0.5358\n",
      "Epoch 371, CIFAR-10 Batch 1:  Loss:  0.0279435  Accuracy:  0.5232\n",
      "Epoch 372, CIFAR-10 Batch 1:  Loss:  0.0425552  Accuracy:  0.5216\n",
      "Epoch 373, CIFAR-10 Batch 1:  Loss:  0.049151  Accuracy:  0.5172\n",
      "Epoch 374, CIFAR-10 Batch 1:  Loss:  0.0378934  Accuracy:  0.527\n",
      "Epoch 375, CIFAR-10 Batch 1:  Loss:  0.0429497  Accuracy:  0.534\n",
      "Epoch 376, CIFAR-10 Batch 1:  Loss:  0.052826  Accuracy:  0.5208\n",
      "Epoch 377, CIFAR-10 Batch 1:  Loss:  0.12088  Accuracy:  0.5194\n",
      "Epoch 378, CIFAR-10 Batch 1:  Loss:  0.0871058  Accuracy:  0.5102\n",
      "Epoch 379, CIFAR-10 Batch 1:  Loss:  0.162151  Accuracy:  0.5004\n",
      "Epoch 380, CIFAR-10 Batch 1:  Loss:  0.20657  Accuracy:  0.494\n",
      "Epoch 381, CIFAR-10 Batch 1:  Loss:  0.0787117  Accuracy:  0.5184\n",
      "Epoch 382, CIFAR-10 Batch 1:  Loss:  0.131638  Accuracy:  0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383, CIFAR-10 Batch 1:  Loss:  0.063205  Accuracy:  0.519\n",
      "Epoch 384, CIFAR-10 Batch 1:  Loss:  0.0600538  Accuracy:  0.526\n",
      "Epoch 385, CIFAR-10 Batch 1:  Loss:  0.0759049  Accuracy:  0.5272\n",
      "Epoch 386, CIFAR-10 Batch 1:  Loss:  0.062158  Accuracy:  0.5136\n",
      "Epoch 387, CIFAR-10 Batch 1:  Loss:  0.0483051  Accuracy:  0.527\n",
      "Epoch 388, CIFAR-10 Batch 1:  Loss:  0.0368185  Accuracy:  0.5288\n",
      "Epoch 389, CIFAR-10 Batch 1:  Loss:  0.0386866  Accuracy:  0.5372\n",
      "Epoch 390, CIFAR-10 Batch 1:  Loss:  0.0427096  Accuracy:  0.5324\n",
      "Epoch 391, CIFAR-10 Batch 1:  Loss:  0.0256623  Accuracy:  0.5294\n",
      "Epoch 392, CIFAR-10 Batch 1:  Loss:  0.0404098  Accuracy:  0.5288\n",
      "Epoch 393, CIFAR-10 Batch 1:  Loss:  0.0226435  Accuracy:  0.5364\n",
      "Epoch 394, CIFAR-10 Batch 1:  Loss:  0.0238813  Accuracy:  0.5316\n",
      "Epoch 395, CIFAR-10 Batch 1:  Loss:  0.0255823  Accuracy:  0.5218\n",
      "Epoch 396, CIFAR-10 Batch 1:  Loss:  0.0394641  Accuracy:  0.5202\n",
      "Epoch 397, CIFAR-10 Batch 1:  Loss:  0.0304394  Accuracy:  0.5322\n",
      "Epoch 398, CIFAR-10 Batch 1:  Loss:  0.0209637  Accuracy:  0.537\n",
      "Epoch 399, CIFAR-10 Batch 1:  Loss:  0.0255398  Accuracy:  0.523\n",
      "Epoch 400, CIFAR-10 Batch 1:  Loss:  0.0264003  Accuracy:  0.5322\n",
      "Epoch 401, CIFAR-10 Batch 1:  Loss:  0.0165196  Accuracy:  0.5258\n",
      "Epoch 402, CIFAR-10 Batch 1:  Loss:  0.0198307  Accuracy:  0.5366\n",
      "Epoch 403, CIFAR-10 Batch 1:  Loss:  0.0225987  Accuracy:  0.5372\n",
      "Epoch 404, CIFAR-10 Batch 1:  Loss:  0.0320537  Accuracy:  0.525\n",
      "Epoch 405, CIFAR-10 Batch 1:  Loss:  0.0683033  Accuracy:  0.5248\n",
      "Epoch 406, CIFAR-10 Batch 1:  Loss:  0.0267677  Accuracy:  0.5406\n",
      "Epoch 407, CIFAR-10 Batch 1:  Loss:  0.0379102  Accuracy:  0.5222\n",
      "Epoch 408, CIFAR-10 Batch 1:  Loss:  0.0641425  Accuracy:  0.5252\n",
      "Epoch 409, CIFAR-10 Batch 1:  Loss:  0.056113  Accuracy:  0.5172\n",
      "Epoch 410, CIFAR-10 Batch 1:  Loss:  0.086931  Accuracy:  0.5318\n",
      "Epoch 411, CIFAR-10 Batch 1:  Loss:  0.0537227  Accuracy:  0.5278\n",
      "Epoch 412, CIFAR-10 Batch 1:  Loss:  0.112164  Accuracy:  0.4998\n",
      "Epoch 413, CIFAR-10 Batch 1:  Loss:  0.243856  Accuracy:  0.4984\n",
      "Epoch 414, CIFAR-10 Batch 1:  Loss:  0.0757616  Accuracy:  0.505\n",
      "Epoch 415, CIFAR-10 Batch 1:  Loss:  0.0299633  Accuracy:  0.5268\n",
      "Epoch 416, CIFAR-10 Batch 1:  Loss:  0.0980219  Accuracy:  0.53\n",
      "Epoch 417, CIFAR-10 Batch 1:  Loss:  0.0850001  Accuracy:  0.5244\n",
      "Epoch 418, CIFAR-10 Batch 1:  Loss:  0.0816725  Accuracy:  0.518\n",
      "Epoch 419, CIFAR-10 Batch 1:  Loss:  0.0988252  Accuracy:  0.5022\n",
      "Epoch 420, CIFAR-10 Batch 1:  Loss:  0.0730992  Accuracy:  0.5096\n",
      "Epoch 421, CIFAR-10 Batch 1:  Loss:  0.090487  Accuracy:  0.5146\n",
      "Epoch 422, CIFAR-10 Batch 1:  Loss:  0.0802409  Accuracy:  0.5108\n",
      "Epoch 423, CIFAR-10 Batch 1:  Loss:  0.0524849  Accuracy:  0.527\n",
      "Epoch 424, CIFAR-10 Batch 1:  Loss:  0.0547309  Accuracy:  0.5252\n",
      "Epoch 425, CIFAR-10 Batch 1:  Loss:  0.089672  Accuracy:  0.5254\n",
      "Epoch 426, CIFAR-10 Batch 1:  Loss:  0.0602522  Accuracy:  0.5222\n",
      "Epoch 427, CIFAR-10 Batch 1:  Loss:  0.0462485  Accuracy:  0.5178\n",
      "Epoch 428, CIFAR-10 Batch 1:  Loss:  0.0333072  Accuracy:  0.5266\n",
      "Epoch 429, CIFAR-10 Batch 1:  Loss:  0.0253687  Accuracy:  0.5332\n",
      "Epoch 430, CIFAR-10 Batch 1:  Loss:  0.022834  Accuracy:  0.536\n",
      "Epoch 431, CIFAR-10 Batch 1:  Loss:  0.0246034  Accuracy:  0.5306\n",
      "Epoch 432, CIFAR-10 Batch 1:  Loss:  0.0270979  Accuracy:  0.5326\n",
      "Epoch 433, CIFAR-10 Batch 1:  Loss:  0.0283488  Accuracy:  0.5328\n",
      "Epoch 434, CIFAR-10 Batch 1:  Loss:  0.021775  Accuracy:  0.5318\n",
      "Epoch 435, CIFAR-10 Batch 1:  Loss:  0.0182255  Accuracy:  0.5282\n",
      "Epoch 436, CIFAR-10 Batch 1:  Loss:  0.029635  Accuracy:  0.5354\n",
      "Epoch 437, CIFAR-10 Batch 1:  Loss:  0.0162385  Accuracy:  0.532\n",
      "Epoch 438, CIFAR-10 Batch 1:  Loss:  0.0143558  Accuracy:  0.5344\n",
      "Epoch 439, CIFAR-10 Batch 1:  Loss:  0.0270247  Accuracy:  0.531\n",
      "Epoch 440, CIFAR-10 Batch 1:  Loss:  0.0163906  Accuracy:  0.5328\n",
      "Epoch 441, CIFAR-10 Batch 1:  Loss:  0.0185412  Accuracy:  0.5268\n",
      "Epoch 442, CIFAR-10 Batch 1:  Loss:  0.0310999  Accuracy:  0.5294\n",
      "Epoch 443, CIFAR-10 Batch 1:  Loss:  0.0112181  Accuracy:  0.5292\n",
      "Epoch 444, CIFAR-10 Batch 1:  Loss:  0.0248382  Accuracy:  0.5326\n",
      "Epoch 445, CIFAR-10 Batch 1:  Loss:  0.0209422  Accuracy:  0.5258\n",
      "Epoch 446, CIFAR-10 Batch 1:  Loss:  0.0195695  Accuracy:  0.5372\n",
      "Epoch 447, CIFAR-10 Batch 1:  Loss:  0.010889  Accuracy:  0.5314\n",
      "Epoch 448, CIFAR-10 Batch 1:  Loss:  0.0208681  Accuracy:  0.5308\n",
      "Epoch 449, CIFAR-10 Batch 1:  Loss:  0.0156983  Accuracy:  0.5318\n",
      "Epoch 450, CIFAR-10 Batch 1:  Loss:  0.0168789  Accuracy:  0.533\n",
      "Epoch 451, CIFAR-10 Batch 1:  Loss:  0.010907  Accuracy:  0.5386\n",
      "Epoch 452, CIFAR-10 Batch 1:  Loss:  0.0116486  Accuracy:  0.5264\n",
      "Epoch 453, CIFAR-10 Batch 1:  Loss:  0.0126572  Accuracy:  0.5336\n",
      "Epoch 454, CIFAR-10 Batch 1:  Loss:  0.0152821  Accuracy:  0.5348\n",
      "Epoch 455, CIFAR-10 Batch 1:  Loss:  0.0191568  Accuracy:  0.5424\n",
      "Epoch 456, CIFAR-10 Batch 1:  Loss:  0.0199931  Accuracy:  0.5388\n",
      "Epoch 457, CIFAR-10 Batch 1:  Loss:  0.0172424  Accuracy:  0.5304\n",
      "Epoch 458, CIFAR-10 Batch 1:  Loss:  0.0173371  Accuracy:  0.5316\n",
      "Epoch 459, CIFAR-10 Batch 1:  Loss:  0.0194482  Accuracy:  0.5268\n",
      "Epoch 460, CIFAR-10 Batch 1:  Loss:  0.0228351  Accuracy:  0.5242\n",
      "Epoch 461, CIFAR-10 Batch 1:  Loss:  0.0322344  Accuracy:  0.5232\n",
      "Epoch 462, CIFAR-10 Batch 1:  Loss:  0.0291572  Accuracy:  0.5312\n",
      "Epoch 463, CIFAR-10 Batch 1:  Loss:  0.0284568  Accuracy:  0.5274\n",
      "Epoch 464, CIFAR-10 Batch 1:  Loss:  0.00722415  Accuracy:  0.5276\n",
      "Epoch 465, CIFAR-10 Batch 1:  Loss:  0.0317358  Accuracy:  0.5308\n",
      "Epoch 466, CIFAR-10 Batch 1:  Loss:  0.020704  Accuracy:  0.5332\n",
      "Epoch 467, CIFAR-10 Batch 1:  Loss:  0.0207606  Accuracy:  0.5314\n",
      "Epoch 468, CIFAR-10 Batch 1:  Loss:  0.0166073  Accuracy:  0.5336\n",
      "Epoch 469, CIFAR-10 Batch 1:  Loss:  0.0181986  Accuracy:  0.5358\n",
      "Epoch 470, CIFAR-10 Batch 1:  Loss:  0.0249394  Accuracy:  0.5294\n",
      "Epoch 471, CIFAR-10 Batch 1:  Loss:  0.0369925  Accuracy:  0.5318\n",
      "Epoch 472, CIFAR-10 Batch 1:  Loss:  0.0156806  Accuracy:  0.5356\n",
      "Epoch 473, CIFAR-10 Batch 1:  Loss:  0.0171274  Accuracy:  0.53\n",
      "Epoch 474, CIFAR-10 Batch 1:  Loss:  0.0210888  Accuracy:  0.5344\n",
      "Epoch 475, CIFAR-10 Batch 1:  Loss:  0.0189399  Accuracy:  0.5304\n",
      "Epoch 476, CIFAR-10 Batch 1:  Loss:  0.0138795  Accuracy:  0.5302\n",
      "Epoch 477, CIFAR-10 Batch 1:  Loss:  0.0183105  Accuracy:  0.5424\n",
      "Epoch 478, CIFAR-10 Batch 1:  Loss:  0.0255201  Accuracy:  0.5426\n",
      "Epoch 479, CIFAR-10 Batch 1:  Loss:  0.0147166  Accuracy:  0.5378\n",
      "Epoch 480, CIFAR-10 Batch 1:  Loss:  0.010058  Accuracy:  0.536\n",
      "Epoch 481, CIFAR-10 Batch 1:  Loss:  0.0103221  Accuracy:  0.532\n",
      "Epoch 482, CIFAR-10 Batch 1:  Loss:  0.0179761  Accuracy:  0.531\n",
      "Epoch 483, CIFAR-10 Batch 1:  Loss:  0.0119535  Accuracy:  0.5302\n",
      "Epoch 484, CIFAR-10 Batch 1:  Loss:  0.0110715  Accuracy:  0.5306\n",
      "Epoch 485, CIFAR-10 Batch 1:  Loss:  0.0201428  Accuracy:  0.5296\n",
      "Epoch 486, CIFAR-10 Batch 1:  Loss:  0.0125553  Accuracy:  0.5326\n",
      "Epoch 487, CIFAR-10 Batch 1:  Loss:  0.0155174  Accuracy:  0.5284\n",
      "Epoch 488, CIFAR-10 Batch 1:  Loss:  0.0201209  Accuracy:  0.542\n",
      "Epoch 489, CIFAR-10 Batch 1:  Loss:  0.041566  Accuracy:  0.5298\n",
      "Epoch 490, CIFAR-10 Batch 1:  Loss:  0.0147614  Accuracy:  0.5298\n",
      "Epoch 491, CIFAR-10 Batch 1:  Loss:  0.0190106  Accuracy:  0.5282\n",
      "Epoch 492, CIFAR-10 Batch 1:  Loss:  0.0216552  Accuracy:  0.5418\n",
      "Epoch 493, CIFAR-10 Batch 1:  Loss:  0.0211554  Accuracy:  0.5328\n",
      "Epoch 494, CIFAR-10 Batch 1:  Loss:  0.01309  Accuracy:  0.5336\n",
      "Epoch 495, CIFAR-10 Batch 1:  Loss:  0.0125841  Accuracy:  0.5316\n",
      "Epoch 496, CIFAR-10 Batch 1:  Loss:  0.0098342  Accuracy:  0.5342\n",
      "Epoch 497, CIFAR-10 Batch 1:  Loss:  0.00771045  Accuracy:  0.5372\n",
      "Epoch 498, CIFAR-10 Batch 1:  Loss:  0.0322633  Accuracy:  0.5428\n",
      "Epoch 499, CIFAR-10 Batch 1:  Loss:  0.00842537  Accuracy:  0.5316\n",
      "Epoch 500, CIFAR-10 Batch 1:  Loss:  0.0290686  Accuracy:  0.5164\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  22.7363  Accuracy:  0.1114\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:  10.5342  Accuracy:  0.1212\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:  6.67319  Accuracy:  0.1322\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:  4.63228  Accuracy:  0.144\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:  3.95487  Accuracy:  0.1414\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  3.31419  Accuracy:  0.1318\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:  3.06681  Accuracy:  0.1454\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:  2.85832  Accuracy:  0.1504\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:  2.66722  Accuracy:  0.1634\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:  2.51381  Accuracy:  0.164\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  2.54082  Accuracy:  0.17\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:  2.44865  Accuracy:  0.1648\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:  2.42638  Accuracy:  0.1918\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:  2.31779  Accuracy:  0.1986\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:  2.27021  Accuracy:  0.1974\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  2.26174  Accuracy:  0.2156\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:  2.25027  Accuracy:  0.2172\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:  2.28047  Accuracy:  0.2192\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:  2.185  Accuracy:  0.2172\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:  2.23905  Accuracy:  0.224\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  2.15497  Accuracy:  0.241\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:  2.15502  Accuracy:  0.2258\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:  2.11802  Accuracy:  0.227\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:  2.10243  Accuracy:  0.2376\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:  2.10507  Accuracy:  0.2464\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  2.08968  Accuracy:  0.246\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:  2.06834  Accuracy:  0.2534\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:  2.03668  Accuracy:  0.2646\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:  2.04438  Accuracy:  0.2716\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:  2.04232  Accuracy:  0.2698\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  1.98167  Accuracy:  0.2734\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:  1.97757  Accuracy:  0.2508\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:  1.99507  Accuracy:  0.2782\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:  1.97658  Accuracy:  0.2866\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:  1.94909  Accuracy:  0.2884\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  1.94552  Accuracy:  0.2844\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:  1.93121  Accuracy:  0.2864\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:  1.91048  Accuracy:  0.3004\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:  1.89462  Accuracy:  0.2962\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:  1.94293  Accuracy:  0.306\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  1.86201  Accuracy:  0.3042\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:  1.84937  Accuracy:  0.3102\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:  1.86963  Accuracy:  0.3068\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:  1.85238  Accuracy:  0.3058\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:  1.85953  Accuracy:  0.3132\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  1.86631  Accuracy:  0.3222\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:  1.80757  Accuracy:  0.3274\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:  1.79037  Accuracy:  0.3278\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:  1.82821  Accuracy:  0.333\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:  1.86079  Accuracy:  0.3238\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  1.8551  Accuracy:  0.3316\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:  1.77631  Accuracy:  0.334\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:  1.78106  Accuracy:  0.3414\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:  1.77014  Accuracy:  0.3432\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:  1.78909  Accuracy:  0.3466\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  1.77667  Accuracy:  0.3598\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:  1.74598  Accuracy:  0.3406\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:  1.745  Accuracy:  0.3538\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:  1.71989  Accuracy:  0.341\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:  1.76605  Accuracy:  0.3452\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  1.75588  Accuracy:  0.3622\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:  1.70407  Accuracy:  0.3616\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:  1.66066  Accuracy:  0.3632\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:  1.70357  Accuracy:  0.3642\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:  1.74273  Accuracy:  0.386\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  1.71172  Accuracy:  0.3678\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:  1.65406  Accuracy:  0.3632\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:  1.67008  Accuracy:  0.3788\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:  1.69888  Accuracy:  0.3734\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:  1.68103  Accuracy:  0.3876\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  1.64398  Accuracy:  0.3828\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:  1.62052  Accuracy:  0.3936\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:  1.62822  Accuracy:  0.3966\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:  1.59139  Accuracy:  0.3916\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:  1.67007  Accuracy:  0.38\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  1.65883  Accuracy:  0.3962\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:  1.61205  Accuracy:  0.4008\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:  1.61014  Accuracy:  0.4046\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:  1.58239  Accuracy:  0.4006\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:  1.63361  Accuracy:  0.4086\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  1.62188  Accuracy:  0.397\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:  1.54892  Accuracy:  0.4012\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:  1.64538  Accuracy:  0.4018\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:  1.6256  Accuracy:  0.3964\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:  1.58483  Accuracy:  0.4008\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  1.60574  Accuracy:  0.4114\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:  1.58078  Accuracy:  0.4072\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:  1.60213  Accuracy:  0.4152\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:  1.56998  Accuracy:  0.4086\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:  1.60653  Accuracy:  0.4238\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  1.55712  Accuracy:  0.4236\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:  1.50536  Accuracy:  0.4372\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:  1.56581  Accuracy:  0.4212\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:  1.50778  Accuracy:  0.423\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:  1.52026  Accuracy:  0.4292\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  1.52311  Accuracy:  0.4364\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:  1.5493  Accuracy:  0.4092\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:  1.51581  Accuracy:  0.4366\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:  1.49919  Accuracy:  0.4208\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:  1.5289  Accuracy:  0.4352\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  1.51263  Accuracy:  0.4414\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:  1.53413  Accuracy:  0.4344\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:  1.47097  Accuracy:  0.433\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:  1.46695  Accuracy:  0.445\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:  1.51293  Accuracy:  0.4438\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  1.51026  Accuracy:  0.4488\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:  1.4812  Accuracy:  0.4562\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:  1.52832  Accuracy:  0.4526\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:  1.47642  Accuracy:  0.4462\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:  1.4758  Accuracy:  0.439\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  1.43889  Accuracy:  0.4648\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:  1.47917  Accuracy:  0.453\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:  1.43493  Accuracy:  0.4524\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:  1.45072  Accuracy:  0.4468\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:  1.43528  Accuracy:  0.4568\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  1.45843  Accuracy:  0.4738\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:  1.42568  Accuracy:  0.464\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:  1.46654  Accuracy:  0.4524\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:  1.41664  Accuracy:  0.4504\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:  1.44669  Accuracy:  0.4522\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  1.41384  Accuracy:  0.458\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:  1.43703  Accuracy:  0.4602\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:  1.3762  Accuracy:  0.4636\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:  1.38653  Accuracy:  0.4574\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:  1.43229  Accuracy:  0.4538\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  1.41341  Accuracy:  0.4684\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:  1.41164  Accuracy:  0.4666\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:  1.39167  Accuracy:  0.463\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:  1.40154  Accuracy:  0.4766\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:  1.42304  Accuracy:  0.4794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  1.38855  Accuracy:  0.483\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:  1.38254  Accuracy:  0.481\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:  1.34934  Accuracy:  0.48\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:  1.3688  Accuracy:  0.4834\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:  1.33519  Accuracy:  0.4888\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  1.34318  Accuracy:  0.4846\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:  1.3899  Accuracy:  0.468\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:  1.36774  Accuracy:  0.4804\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:  1.37275  Accuracy:  0.4768\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:  1.38156  Accuracy:  0.4732\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  1.34473  Accuracy:  0.4914\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:  1.30671  Accuracy:  0.493\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:  1.3067  Accuracy:  0.4994\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:  1.34474  Accuracy:  0.4922\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:  1.36364  Accuracy:  0.4922\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  1.33787  Accuracy:  0.4864\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:  1.34846  Accuracy:  0.4864\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:  1.26841  Accuracy:  0.5002\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:  1.37008  Accuracy:  0.483\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:  1.31598  Accuracy:  0.4986\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  1.3206  Accuracy:  0.4974\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:  1.3115  Accuracy:  0.4936\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:  1.31032  Accuracy:  0.4974\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:  1.32235  Accuracy:  0.4992\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:  1.30378  Accuracy:  0.5072\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  1.29896  Accuracy:  0.4974\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:  1.25767  Accuracy:  0.5066\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:  1.273  Accuracy:  0.5118\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:  1.26629  Accuracy:  0.5056\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:  1.31928  Accuracy:  0.4954\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  1.27315  Accuracy:  0.5062\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:  1.2392  Accuracy:  0.5084\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:  1.26048  Accuracy:  0.5066\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:  1.26019  Accuracy:  0.5048\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:  1.2226  Accuracy:  0.5212\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  1.28742  Accuracy:  0.517\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:  1.27901  Accuracy:  0.498\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:  1.21324  Accuracy:  0.5218\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:  1.2361  Accuracy:  0.511\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:  1.26659  Accuracy:  0.5134\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  1.27786  Accuracy:  0.5086\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:  1.2411  Accuracy:  0.4914\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:  1.25545  Accuracy:  0.5098\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:  1.23577  Accuracy:  0.5216\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:  1.24091  Accuracy:  0.5124\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  1.2294  Accuracy:  0.5238\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:  1.22977  Accuracy:  0.5182\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:  1.18774  Accuracy:  0.525\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:  1.16696  Accuracy:  0.5254\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:  1.23837  Accuracy:  0.5202\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  1.22188  Accuracy:  0.5262\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:  1.2235  Accuracy:  0.5134\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:  1.20372  Accuracy:  0.5248\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:  1.17524  Accuracy:  0.523\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:  1.2351  Accuracy:  0.5272\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  1.17101  Accuracy:  0.5328\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:  1.21673  Accuracy:  0.5186\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:  1.19309  Accuracy:  0.5196\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:  1.27329  Accuracy:  0.5226\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:  1.22202  Accuracy:  0.5084\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  1.19456  Accuracy:  0.5306\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:  1.20117  Accuracy:  0.5242\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:  1.15427  Accuracy:  0.523\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:  1.13958  Accuracy:  0.5398\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:  1.16068  Accuracy:  0.5296\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  1.16572  Accuracy:  0.5398\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:  1.14262  Accuracy:  0.5288\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:  1.1448  Accuracy:  0.5256\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:  1.15559  Accuracy:  0.5338\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:  1.16182  Accuracy:  0.536\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  1.14387  Accuracy:  0.53\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:  1.16653  Accuracy:  0.53\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:  1.142  Accuracy:  0.5288\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:  1.11187  Accuracy:  0.5412\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:  1.1465  Accuracy:  0.5276\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  1.16465  Accuracy:  0.5412\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:  1.15839  Accuracy:  0.5376\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:  1.16967  Accuracy:  0.5282\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:  1.17993  Accuracy:  0.516\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:  1.14491  Accuracy:  0.5296\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  1.15396  Accuracy:  0.5342\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:  1.13537  Accuracy:  0.5498\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:  1.09937  Accuracy:  0.541\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:  1.1467  Accuracy:  0.531\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:  1.18696  Accuracy:  0.5352\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  1.13864  Accuracy:  0.529\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:  1.12139  Accuracy:  0.5398\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:  1.08261  Accuracy:  0.541\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:  1.08942  Accuracy:  0.5422\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:  1.10079  Accuracy:  0.5544\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  1.09139  Accuracy:  0.5466\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:  1.071  Accuracy:  0.557\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:  1.07517  Accuracy:  0.557\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:  1.09186  Accuracy:  0.5426\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:  1.08175  Accuracy:  0.5592\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  1.13665  Accuracy:  0.5406\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:  1.09432  Accuracy:  0.5476\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:  1.07123  Accuracy:  0.5564\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:  1.09604  Accuracy:  0.5508\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:  1.04718  Accuracy:  0.5576\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  1.07986  Accuracy:  0.556\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:  1.07179  Accuracy:  0.5448\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:  1.03561  Accuracy:  0.545\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:  1.09031  Accuracy:  0.544\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:  1.1027  Accuracy:  0.5478\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  1.09479  Accuracy:  0.557\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:  1.06511  Accuracy:  0.5532\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:  1.0375  Accuracy:  0.546\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:  1.06163  Accuracy:  0.5566\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:  1.06494  Accuracy:  0.5534\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  1.04277  Accuracy:  0.5562\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:  1.01909  Accuracy:  0.5612\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:  1.02591  Accuracy:  0.5584\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:  1.05749  Accuracy:  0.5554\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:  0.997859  Accuracy:  0.5674\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  1.05785  Accuracy:  0.554\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:  1.04943  Accuracy:  0.559\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:  1.00747  Accuracy:  0.5636\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:  1.02387  Accuracy:  0.5636\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:  1.0156  Accuracy:  0.555\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  1.07359  Accuracy:  0.556\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:  1.02041  Accuracy:  0.5634\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:  1.06305  Accuracy:  0.563\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:  1.02416  Accuracy:  0.5568\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:  0.986824  Accuracy:  0.5608\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  0.993728  Accuracy:  0.5606\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:  0.997997  Accuracy:  0.573\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:  0.975374  Accuracy:  0.5654\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:  0.993093  Accuracy:  0.5626\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:  0.984386  Accuracy:  0.5742\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  1.0079  Accuracy:  0.5542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, CIFAR-10 Batch 2:  Loss:  1.00852  Accuracy:  0.5572\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:  1.00999  Accuracy:  0.5534\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:  1.07632  Accuracy:  0.5504\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:  1.044  Accuracy:  0.5458\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  1.05747  Accuracy:  0.561\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:  1.03866  Accuracy:  0.5696\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:  0.969672  Accuracy:  0.5516\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:  0.963441  Accuracy:  0.5662\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:  1.03588  Accuracy:  0.5622\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  0.997489  Accuracy:  0.5696\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:  0.974568  Accuracy:  0.572\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:  0.946564  Accuracy:  0.5732\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:  1.00105  Accuracy:  0.5732\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:  0.946096  Accuracy:  0.5686\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  0.968209  Accuracy:  0.5618\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:  0.955221  Accuracy:  0.5768\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:  0.915861  Accuracy:  0.5796\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:  0.926651  Accuracy:  0.571\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:  0.953305  Accuracy:  0.5746\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  0.958103  Accuracy:  0.5716\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:  0.963863  Accuracy:  0.569\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:  0.941474  Accuracy:  0.5712\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:  0.965946  Accuracy:  0.562\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:  0.947334  Accuracy:  0.5586\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  0.989855  Accuracy:  0.5546\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:  0.939351  Accuracy:  0.5766\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:  0.934627  Accuracy:  0.5704\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:  0.904429  Accuracy:  0.5668\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:  0.917182  Accuracy:  0.5724\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  0.963625  Accuracy:  0.5572\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:  0.942819  Accuracy:  0.5742\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:  0.908722  Accuracy:  0.5744\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:  0.943357  Accuracy:  0.567\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:  0.970382  Accuracy:  0.5632\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  0.973138  Accuracy:  0.5686\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:  0.965961  Accuracy:  0.5704\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:  0.9155  Accuracy:  0.5748\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:  0.912357  Accuracy:  0.573\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:  0.919027  Accuracy:  0.58\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  0.902468  Accuracy:  0.579\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:  0.908227  Accuracy:  0.5762\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:  0.86912  Accuracy:  0.5754\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:  0.907702  Accuracy:  0.575\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:  0.907238  Accuracy:  0.5698\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  0.886788  Accuracy:  0.585\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:  0.908055  Accuracy:  0.5814\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:  0.858023  Accuracy:  0.5926\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:  0.890628  Accuracy:  0.5704\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:  0.886474  Accuracy:  0.5788\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  0.901031  Accuracy:  0.5848\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:  0.861686  Accuracy:  0.592\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:  0.882843  Accuracy:  0.5824\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:  0.856457  Accuracy:  0.5848\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:  0.864552  Accuracy:  0.5812\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  0.915197  Accuracy:  0.5724\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:  0.865073  Accuracy:  0.5704\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:  0.882281  Accuracy:  0.5706\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:  0.921811  Accuracy:  0.563\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:  0.908891  Accuracy:  0.5634\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  0.952371  Accuracy:  0.5762\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:  0.954883  Accuracy:  0.5576\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:  0.850385  Accuracy:  0.5792\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:  0.893997  Accuracy:  0.5786\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:  0.896244  Accuracy:  0.5748\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  0.863534  Accuracy:  0.5908\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:  0.86949  Accuracy:  0.5836\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:  0.834514  Accuracy:  0.5776\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:  0.850103  Accuracy:  0.5758\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:  0.864149  Accuracy:  0.5762\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  0.869221  Accuracy:  0.5852\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:  0.869882  Accuracy:  0.5754\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:  0.839695  Accuracy:  0.5766\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:  0.904324  Accuracy:  0.5584\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:  0.84365  Accuracy:  0.5672\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  0.839917  Accuracy:  0.5866\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:  0.83492  Accuracy:  0.5932\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:  0.820607  Accuracy:  0.5904\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:  0.854429  Accuracy:  0.5818\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:  0.86524  Accuracy:  0.5752\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  0.844146  Accuracy:  0.5936\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:  0.819495  Accuracy:  0.5898\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:  0.812181  Accuracy:  0.5844\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:  0.831207  Accuracy:  0.5862\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:  0.773825  Accuracy:  0.596\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  0.92023  Accuracy:  0.5542\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:  0.84707  Accuracy:  0.5916\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:  0.839253  Accuracy:  0.5726\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:  0.822177  Accuracy:  0.58\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:  0.807161  Accuracy:  0.588\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  0.790448  Accuracy:  0.5948\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:  0.805387  Accuracy:  0.5932\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:  0.796486  Accuracy:  0.5996\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:  0.839416  Accuracy:  0.5756\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:  0.805944  Accuracy:  0.5862\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  0.798186  Accuracy:  0.5874\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:  0.814275  Accuracy:  0.5918\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:  0.8008  Accuracy:  0.5928\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:  0.812106  Accuracy:  0.5894\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:  0.848107  Accuracy:  0.5756\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  0.861115  Accuracy:  0.5822\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:  0.81653  Accuracy:  0.584\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:  0.770521  Accuracy:  0.592\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:  0.803729  Accuracy:  0.5764\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:  0.756785  Accuracy:  0.5968\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  0.824595  Accuracy:  0.5872\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:  0.836287  Accuracy:  0.5902\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:  0.763618  Accuracy:  0.5974\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:  0.753531  Accuracy:  0.5922\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:  0.723485  Accuracy:  0.592\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  0.784167  Accuracy:  0.6024\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:  0.770351  Accuracy:  0.5958\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:  0.793512  Accuracy:  0.5972\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:  0.742811  Accuracy:  0.6008\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:  0.734988  Accuracy:  0.5928\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  0.810889  Accuracy:  0.5882\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:  0.821667  Accuracy:  0.5888\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:  0.778898  Accuracy:  0.6028\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:  0.709839  Accuracy:  0.5942\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:  0.726932  Accuracy:  0.58\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  0.760802  Accuracy:  0.5956\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:  0.7637  Accuracy:  0.5938\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:  0.728817  Accuracy:  0.5904\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:  0.754519  Accuracy:  0.5858\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:  0.686487  Accuracy:  0.5912\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  0.810908  Accuracy:  0.5916\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:  0.728587  Accuracy:  0.5952\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:  0.763094  Accuracy:  0.5984\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:  0.714532  Accuracy:  0.5942\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:  0.682143  Accuracy:  0.602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  0.752775  Accuracy:  0.599\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:  0.759013  Accuracy:  0.5976\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:  0.733061  Accuracy:  0.6022\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:  0.713647  Accuracy:  0.6012\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:  0.72091  Accuracy:  0.5844\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  0.784214  Accuracy:  0.5878\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:  0.753511  Accuracy:  0.5846\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:  0.724543  Accuracy:  0.5962\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:  0.68655  Accuracy:  0.597\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:  0.653004  Accuracy:  0.6018\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  0.717767  Accuracy:  0.6002\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:  0.690442  Accuracy:  0.6048\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:  0.720627  Accuracy:  0.5938\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:  0.73399  Accuracy:  0.5854\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:  0.694127  Accuracy:  0.5916\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  0.749029  Accuracy:  0.5936\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:  0.726605  Accuracy:  0.5952\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:  0.739173  Accuracy:  0.5824\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:  0.684813  Accuracy:  0.603\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:  0.641531  Accuracy:  0.613\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  0.721802  Accuracy:  0.598\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:  0.718008  Accuracy:  0.5998\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:  0.700022  Accuracy:  0.598\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:  0.68247  Accuracy:  0.6012\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:  0.667262  Accuracy:  0.6018\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  0.70619  Accuracy:  0.5898\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:  0.746777  Accuracy:  0.5966\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:  0.678838  Accuracy:  0.5986\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:  0.678711  Accuracy:  0.6006\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:  0.66538  Accuracy:  0.5934\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  0.683847  Accuracy:  0.6028\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:  0.740098  Accuracy:  0.592\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:  0.748319  Accuracy:  0.5926\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:  0.706417  Accuracy:  0.5928\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:  0.664498  Accuracy:  0.5992\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  0.68615  Accuracy:  0.6008\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:  0.655879  Accuracy:  0.6\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:  0.66399  Accuracy:  0.6118\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:  0.656482  Accuracy:  0.6134\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:  0.669897  Accuracy:  0.5858\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  0.690576  Accuracy:  0.6046\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:  0.67464  Accuracy:  0.6018\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:  0.728159  Accuracy:  0.5912\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:  0.686529  Accuracy:  0.5904\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:  0.696293  Accuracy:  0.5796\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  0.684338  Accuracy:  0.6034\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:  0.687102  Accuracy:  0.6022\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:  0.664134  Accuracy:  0.5968\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:  0.691289  Accuracy:  0.597\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:  0.644654  Accuracy:  0.5904\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  0.662368  Accuracy:  0.6152\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:  0.677432  Accuracy:  0.6122\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:  0.660335  Accuracy:  0.5946\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:  0.627572  Accuracy:  0.6072\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:  0.618347  Accuracy:  0.6018\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  0.650892  Accuracy:  0.6004\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:  0.606349  Accuracy:  0.6168\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:  0.635135  Accuracy:  0.5994\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:  0.625225  Accuracy:  0.606\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:  0.617482  Accuracy:  0.596\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  0.64812  Accuracy:  0.6\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:  0.720895  Accuracy:  0.5904\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:  0.695965  Accuracy:  0.5806\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:  0.628692  Accuracy:  0.59\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:  0.618806  Accuracy:  0.597\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  0.604489  Accuracy:  0.6056\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:  0.622258  Accuracy:  0.6002\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:  0.582515  Accuracy:  0.602\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:  0.607357  Accuracy:  0.598\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:  0.625363  Accuracy:  0.5982\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  0.599659  Accuracy:  0.6194\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:  0.614945  Accuracy:  0.6052\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:  0.619084  Accuracy:  0.6066\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:  0.601543  Accuracy:  0.6086\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:  0.557722  Accuracy:  0.5906\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  0.646983  Accuracy:  0.6\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:  0.629573  Accuracy:  0.611\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:  0.629072  Accuracy:  0.6022\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:  0.608465  Accuracy:  0.6026\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:  0.632333  Accuracy:  0.5904\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  0.604278  Accuracy:  0.6018\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:  0.612585  Accuracy:  0.6018\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:  0.646994  Accuracy:  0.5984\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:  0.615187  Accuracy:  0.5962\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:  0.566451  Accuracy:  0.6042\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  0.626258  Accuracy:  0.5978\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:  0.658358  Accuracy:  0.5966\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:  0.620655  Accuracy:  0.5898\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:  0.597505  Accuracy:  0.6042\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:  0.602644  Accuracy:  0.597\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  0.598345  Accuracy:  0.6156\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:  0.658557  Accuracy:  0.5938\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:  0.595397  Accuracy:  0.6006\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:  0.61454  Accuracy:  0.615\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:  0.552787  Accuracy:  0.5978\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  0.621922  Accuracy:  0.6096\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:  0.648593  Accuracy:  0.6038\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:  0.545885  Accuracy:  0.609\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:  0.614955  Accuracy:  0.6104\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:  0.541973  Accuracy:  0.6064\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  0.59803  Accuracy:  0.6044\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:  0.600605  Accuracy:  0.6134\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:  0.556043  Accuracy:  0.6242\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:  0.575231  Accuracy:  0.6018\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:  0.516195  Accuracy:  0.617\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  0.61314  Accuracy:  0.6032\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:  0.564918  Accuracy:  0.6164\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:  0.560686  Accuracy:  0.6182\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:  0.525278  Accuracy:  0.6152\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:  0.510584  Accuracy:  0.6076\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  0.626142  Accuracy:  0.601\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:  0.567924  Accuracy:  0.615\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:  0.527658  Accuracy:  0.621\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:  0.502677  Accuracy:  0.6186\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:  0.471686  Accuracy:  0.6174\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  0.602872  Accuracy:  0.5984\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:  0.566954  Accuracy:  0.6128\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:  0.549007  Accuracy:  0.6142\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:  0.542439  Accuracy:  0.6196\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:  0.462048  Accuracy:  0.618\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  0.567956  Accuracy:  0.607\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:  0.539527  Accuracy:  0.619\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:  0.562187  Accuracy:  0.6038\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:  0.529441  Accuracy:  0.6046\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:  0.495213  Accuracy:  0.6078\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  0.552612  Accuracy:  0.6162\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:  0.56614  Accuracy:  0.5992\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:  0.51669  Accuracy:  0.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104, CIFAR-10 Batch 4:  Loss:  0.519641  Accuracy:  0.606\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:  0.514455  Accuracy:  0.6104\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  0.512338  Accuracy:  0.6222\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:  0.535123  Accuracy:  0.5998\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:  0.530707  Accuracy:  0.6102\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:  0.515408  Accuracy:  0.6086\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:  0.494709  Accuracy:  0.6104\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  0.545362  Accuracy:  0.6112\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:  0.525205  Accuracy:  0.6072\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:  0.518011  Accuracy:  0.604\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:  0.533115  Accuracy:  0.5934\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:  0.52595  Accuracy:  0.603\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  0.5238  Accuracy:  0.6062\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:  0.573539  Accuracy:  0.592\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:  0.513597  Accuracy:  0.609\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:  0.559195  Accuracy:  0.5978\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:  0.451082  Accuracy:  0.6212\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  0.532807  Accuracy:  0.6056\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:  0.545669  Accuracy:  0.611\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:  0.509409  Accuracy:  0.6184\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:  0.468545  Accuracy:  0.6122\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:  0.463523  Accuracy:  0.6202\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  0.522359  Accuracy:  0.6126\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:  0.487982  Accuracy:  0.6128\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:  0.476718  Accuracy:  0.6144\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:  0.461189  Accuracy:  0.6156\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:  0.43165  Accuracy:  0.6222\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  0.491274  Accuracy:  0.6188\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:  0.483414  Accuracy:  0.6124\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:  0.44371  Accuracy:  0.6208\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:  0.479398  Accuracy:  0.6098\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:  0.449174  Accuracy:  0.6134\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  0.49323  Accuracy:  0.616\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:  0.507009  Accuracy:  0.609\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:  0.477575  Accuracy:  0.6174\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:  0.466521  Accuracy:  0.614\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:  0.442099  Accuracy:  0.6194\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  0.521746  Accuracy:  0.6174\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:  0.5256  Accuracy:  0.616\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:  0.465136  Accuracy:  0.606\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:  0.449764  Accuracy:  0.62\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:  0.41925  Accuracy:  0.6184\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  0.429897  Accuracy:  0.6266\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:  0.477233  Accuracy:  0.6146\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:  0.401766  Accuracy:  0.6246\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:  0.472783  Accuracy:  0.6152\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:  0.404001  Accuracy:  0.619\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  0.478896  Accuracy:  0.619\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:  0.462302  Accuracy:  0.6112\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:  0.422164  Accuracy:  0.6184\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:  0.448099  Accuracy:  0.6134\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:  0.404889  Accuracy:  0.6204\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  0.446128  Accuracy:  0.6096\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:  0.453115  Accuracy:  0.6156\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:  0.440368  Accuracy:  0.6154\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:  0.434411  Accuracy:  0.6204\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:  0.403419  Accuracy:  0.6236\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  0.438964  Accuracy:  0.6136\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:  0.460883  Accuracy:  0.6104\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:  0.424057  Accuracy:  0.6152\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:  0.455103  Accuracy:  0.6154\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:  0.400484  Accuracy:  0.619\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  0.423005  Accuracy:  0.62\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:  0.436171  Accuracy:  0.6108\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:  0.387633  Accuracy:  0.6212\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:  0.422754  Accuracy:  0.6102\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:  0.381721  Accuracy:  0.6234\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  0.416702  Accuracy:  0.6144\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:  0.438237  Accuracy:  0.6136\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:  0.422115  Accuracy:  0.6148\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:  0.410389  Accuracy:  0.6266\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:  0.36126  Accuracy:  0.6266\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  0.455903  Accuracy:  0.6042\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:  0.424784  Accuracy:  0.6106\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:  0.428467  Accuracy:  0.6188\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:  0.409837  Accuracy:  0.6184\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:  0.367543  Accuracy:  0.6364\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  0.419056  Accuracy:  0.6224\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:  0.453129  Accuracy:  0.6164\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:  0.389942  Accuracy:  0.6252\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:  0.41386  Accuracy:  0.6232\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:  0.358763  Accuracy:  0.6218\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  0.415774  Accuracy:  0.6222\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:  0.425446  Accuracy:  0.6068\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:  0.407475  Accuracy:  0.614\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:  0.42846  Accuracy:  0.61\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:  0.33875  Accuracy:  0.617\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  0.378146  Accuracy:  0.6228\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:  0.393769  Accuracy:  0.6196\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:  0.390272  Accuracy:  0.6198\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:  0.404669  Accuracy:  0.6242\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:  0.325076  Accuracy:  0.6288\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  0.364824  Accuracy:  0.62\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:  0.382032  Accuracy:  0.6216\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:  0.350721  Accuracy:  0.6166\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:  0.342146  Accuracy:  0.6274\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:  0.291412  Accuracy:  0.6274\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  0.3528  Accuracy:  0.6218\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:  0.380974  Accuracy:  0.615\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:  0.4047  Accuracy:  0.6058\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:  0.376966  Accuracy:  0.6234\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:  0.301157  Accuracy:  0.6208\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  0.372141  Accuracy:  0.6218\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:  0.356963  Accuracy:  0.6196\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:  0.410793  Accuracy:  0.6078\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:  0.391074  Accuracy:  0.6108\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:  0.33642  Accuracy:  0.6166\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  0.344128  Accuracy:  0.6164\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:  0.351718  Accuracy:  0.6154\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:  0.372687  Accuracy:  0.6198\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:  0.389264  Accuracy:  0.612\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:  0.317858  Accuracy:  0.6178\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  0.327445  Accuracy:  0.6258\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:  0.356899  Accuracy:  0.618\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:  0.377844  Accuracy:  0.6126\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:  0.362748  Accuracy:  0.6254\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:  0.293237  Accuracy:  0.6316\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  0.320666  Accuracy:  0.6158\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:  0.349214  Accuracy:  0.6188\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:  0.344239  Accuracy:  0.6264\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:  0.338815  Accuracy:  0.6266\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:  0.344263  Accuracy:  0.615\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  0.449069  Accuracy:  0.611\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:  0.389964  Accuracy:  0.6086\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:  0.348918  Accuracy:  0.6244\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:  0.323221  Accuracy:  0.636\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:  0.3123  Accuracy:  0.6214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  0.348155  Accuracy:  0.62\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:  0.326187  Accuracy:  0.6216\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:  0.316236  Accuracy:  0.6248\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:  0.355509  Accuracy:  0.615\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:  0.281186  Accuracy:  0.6284\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  0.318562  Accuracy:  0.6318\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:  0.330042  Accuracy:  0.6208\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:  0.320264  Accuracy:  0.6232\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:  0.310372  Accuracy:  0.624\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:  0.275932  Accuracy:  0.6238\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  0.312852  Accuracy:  0.615\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:  0.337826  Accuracy:  0.6108\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:  0.302724  Accuracy:  0.6268\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:  0.327664  Accuracy:  0.623\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:  0.317479  Accuracy:  0.6294\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  0.319025  Accuracy:  0.6256\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:  0.315901  Accuracy:  0.6202\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:  0.334677  Accuracy:  0.614\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:  0.319278  Accuracy:  0.6162\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:  0.272279  Accuracy:  0.6316\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  0.286782  Accuracy:  0.6216\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:  0.303797  Accuracy:  0.625\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:  0.313092  Accuracy:  0.624\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:  0.325259  Accuracy:  0.6192\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:  0.290018  Accuracy:  0.6172\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  0.308983  Accuracy:  0.6224\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:  0.311102  Accuracy:  0.6236\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:  0.331937  Accuracy:  0.6182\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:  0.328916  Accuracy:  0.618\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:  0.292452  Accuracy:  0.624\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  0.325155  Accuracy:  0.6246\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:  0.349791  Accuracy:  0.6254\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:  0.395915  Accuracy:  0.5994\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:  0.350607  Accuracy:  0.6136\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:  0.344098  Accuracy:  0.6006\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  0.371995  Accuracy:  0.6158\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:  0.381837  Accuracy:  0.6068\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:  0.4295  Accuracy:  0.599\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:  0.389477  Accuracy:  0.5922\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:  0.319966  Accuracy:  0.6122\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  0.38555  Accuracy:  0.6152\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:  0.365065  Accuracy:  0.6192\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:  0.309034  Accuracy:  0.6268\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:  0.289221  Accuracy:  0.616\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:  0.278781  Accuracy:  0.6258\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  0.285892  Accuracy:  0.6286\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:  0.311634  Accuracy:  0.6208\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:  0.270453  Accuracy:  0.6298\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:  0.260589  Accuracy:  0.6198\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:  0.294637  Accuracy:  0.6172\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  0.269186  Accuracy:  0.6214\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:  0.30032  Accuracy:  0.6122\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:  0.3126  Accuracy:  0.6142\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:  0.283852  Accuracy:  0.6146\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:  0.300706  Accuracy:  0.6164\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  0.279988  Accuracy:  0.6208\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:  0.29315  Accuracy:  0.6166\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:  0.298801  Accuracy:  0.6108\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:  0.286282  Accuracy:  0.617\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:  0.309804  Accuracy:  0.6188\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  0.327203  Accuracy:  0.6128\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:  0.35037  Accuracy:  0.6146\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:  0.340978  Accuracy:  0.603\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:  0.313205  Accuracy:  0.6066\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:  0.289141  Accuracy:  0.6102\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  0.290967  Accuracy:  0.6142\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:  0.305645  Accuracy:  0.6156\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:  0.348058  Accuracy:  0.5984\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:  0.341252  Accuracy:  0.612\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:  0.284516  Accuracy:  0.6088\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  0.282952  Accuracy:  0.62\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:  0.308949  Accuracy:  0.624\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:  0.309039  Accuracy:  0.6038\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:  0.30315  Accuracy:  0.599\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:  0.316086  Accuracy:  0.6056\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  0.318201  Accuracy:  0.622\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:  0.288493  Accuracy:  0.6274\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:  0.303178  Accuracy:  0.6118\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:  0.259773  Accuracy:  0.6166\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:  0.261078  Accuracy:  0.6176\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  0.289069  Accuracy:  0.6244\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:  0.271839  Accuracy:  0.6224\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:  0.276416  Accuracy:  0.6096\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:  0.261171  Accuracy:  0.6162\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:  0.26603  Accuracy:  0.618\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  0.285376  Accuracy:  0.612\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:  0.295815  Accuracy:  0.61\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:  0.299944  Accuracy:  0.6104\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:  0.271558  Accuracy:  0.6218\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:  0.265182  Accuracy:  0.6256\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  0.243531  Accuracy:  0.6276\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:  0.26859  Accuracy:  0.6228\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:  0.307285  Accuracy:  0.6156\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:  0.295372  Accuracy:  0.6166\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:  0.277589  Accuracy:  0.608\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  0.27117  Accuracy:  0.6132\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:  0.352591  Accuracy:  0.6018\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:  0.303858  Accuracy:  0.6098\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:  0.359256  Accuracy:  0.5944\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:  0.29423  Accuracy:  0.617\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  0.319964  Accuracy:  0.606\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:  0.314002  Accuracy:  0.6234\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:  0.349126  Accuracy:  0.6078\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:  0.302397  Accuracy:  0.6098\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:  0.279667  Accuracy:  0.6006\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  0.258895  Accuracy:  0.6148\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:  0.262028  Accuracy:  0.612\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:  0.287357  Accuracy:  0.6132\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:  0.274868  Accuracy:  0.6082\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:  0.24293  Accuracy:  0.6132\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  0.279096  Accuracy:  0.612\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:  0.253053  Accuracy:  0.6212\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:  0.225675  Accuracy:  0.62\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:  0.26307  Accuracy:  0.6178\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:  0.241965  Accuracy:  0.619\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  0.251266  Accuracy:  0.609\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:  0.287908  Accuracy:  0.6144\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:  0.263416  Accuracy:  0.6116\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:  0.264867  Accuracy:  0.6148\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:  0.225276  Accuracy:  0.6164\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  0.257047  Accuracy:  0.6088\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:  0.28839  Accuracy:  0.6162\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:  0.286638  Accuracy:  0.6048\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:  0.253367  Accuracy:  0.62\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:  0.240372  Accuracy:  0.6264\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  0.249497  Accuracy:  0.6248\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:  0.261928  Accuracy:  0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155, CIFAR-10 Batch 3:  Loss:  0.268004  Accuracy:  0.6096\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:  0.295615  Accuracy:  0.6002\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:  0.239854  Accuracy:  0.616\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  0.266758  Accuracy:  0.6204\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:  0.284241  Accuracy:  0.6308\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:  0.210035  Accuracy:  0.6204\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:  0.223821  Accuracy:  0.619\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:  0.195856  Accuracy:  0.6286\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  0.216006  Accuracy:  0.6178\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:  0.229613  Accuracy:  0.6126\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:  0.185248  Accuracy:  0.6244\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:  0.237681  Accuracy:  0.627\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:  0.204949  Accuracy:  0.6258\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  0.228451  Accuracy:  0.6216\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:  0.209496  Accuracy:  0.6208\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:  0.22693  Accuracy:  0.609\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:  0.236453  Accuracy:  0.613\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:  0.213608  Accuracy:  0.6136\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  0.223018  Accuracy:  0.6246\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:  0.222615  Accuracy:  0.6244\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:  0.218589  Accuracy:  0.625\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:  0.224036  Accuracy:  0.614\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:  0.214934  Accuracy:  0.6204\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  0.234989  Accuracy:  0.6146\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:  0.18667  Accuracy:  0.6236\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:  0.25277  Accuracy:  0.6092\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:  0.244923  Accuracy:  0.5962\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:  0.199604  Accuracy:  0.621\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  0.211463  Accuracy:  0.6198\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:  0.25708  Accuracy:  0.6246\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:  0.228853  Accuracy:  0.6168\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:  0.258315  Accuracy:  0.597\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:  0.241466  Accuracy:  0.6152\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  0.263717  Accuracy:  0.6018\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:  0.262424  Accuracy:  0.6082\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:  0.257166  Accuracy:  0.6196\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:  0.251117  Accuracy:  0.6134\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:  0.215597  Accuracy:  0.6138\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  0.26122  Accuracy:  0.6146\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:  0.229973  Accuracy:  0.6128\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:  0.299029  Accuracy:  0.602\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:  0.249077  Accuracy:  0.6044\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:  0.244513  Accuracy:  0.5984\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.21627  Accuracy:  0.6186\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:  0.255145  Accuracy:  0.6156\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:  0.210376  Accuracy:  0.6198\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:  0.255484  Accuracy:  0.6118\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:  0.197624  Accuracy:  0.6218\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.239807  Accuracy:  0.6166\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:  0.204289  Accuracy:  0.6252\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:  0.200543  Accuracy:  0.6196\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:  0.223355  Accuracy:  0.6228\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:  0.165004  Accuracy:  0.6166\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  0.210771  Accuracy:  0.6128\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:  0.191983  Accuracy:  0.6282\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:  0.187728  Accuracy:  0.6222\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:  0.197838  Accuracy:  0.6236\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:  0.174109  Accuracy:  0.6146\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.203864  Accuracy:  0.6084\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:  0.211398  Accuracy:  0.6094\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:  0.212709  Accuracy:  0.6084\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:  0.193489  Accuracy:  0.6254\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:  0.187576  Accuracy:  0.6148\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.187326  Accuracy:  0.6078\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:  0.188544  Accuracy:  0.6104\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:  0.205357  Accuracy:  0.6144\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:  0.237269  Accuracy:  0.611\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:  0.183683  Accuracy:  0.6202\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.181465  Accuracy:  0.6152\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:  0.21463  Accuracy:  0.6124\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:  0.243922  Accuracy:  0.6002\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:  0.254126  Accuracy:  0.62\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:  0.206555  Accuracy:  0.6196\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.175622  Accuracy:  0.6312\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:  0.182765  Accuracy:  0.6222\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:  0.183745  Accuracy:  0.616\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:  0.220621  Accuracy:  0.6212\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:  0.174547  Accuracy:  0.6172\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.206636  Accuracy:  0.6118\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:  0.19946  Accuracy:  0.6166\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:  0.169162  Accuracy:  0.6222\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:  0.192862  Accuracy:  0.624\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:  0.169933  Accuracy:  0.608\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.19022  Accuracy:  0.6172\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:  0.194948  Accuracy:  0.627\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:  0.189702  Accuracy:  0.6138\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:  0.225061  Accuracy:  0.6202\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:  0.198645  Accuracy:  0.6114\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.182186  Accuracy:  0.6222\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:  0.175453  Accuracy:  0.6326\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:  0.213431  Accuracy:  0.6094\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:  0.207009  Accuracy:  0.6168\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:  0.164603  Accuracy:  0.6222\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.189589  Accuracy:  0.614\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:  0.147117  Accuracy:  0.6246\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:  0.21084  Accuracy:  0.6096\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:  0.214775  Accuracy:  0.6138\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:  0.177964  Accuracy:  0.62\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.224004  Accuracy:  0.615\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:  0.189772  Accuracy:  0.6252\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:  0.173052  Accuracy:  0.6082\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:  0.159843  Accuracy:  0.6194\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:  0.204224  Accuracy:  0.6072\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.158396  Accuracy:  0.626\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:  0.20636  Accuracy:  0.633\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:  0.179451  Accuracy:  0.6152\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:  0.181271  Accuracy:  0.6184\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:  0.127225  Accuracy:  0.618\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:  0.176532  Accuracy:  0.6186\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:  0.179326  Accuracy:  0.6202\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:  0.140491  Accuracy:  0.6176\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:  0.16533  Accuracy:  0.619\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:  0.166583  Accuracy:  0.6122\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.189725  Accuracy:  0.6146\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:  0.169375  Accuracy:  0.6222\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:  0.166519  Accuracy:  0.604\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:  0.147404  Accuracy:  0.6228\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:  0.174927  Accuracy:  0.6178\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.160209  Accuracy:  0.6178\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:  0.135993  Accuracy:  0.6344\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:  0.205636  Accuracy:  0.6132\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:  0.191893  Accuracy:  0.622\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:  0.132584  Accuracy:  0.6376\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.162102  Accuracy:  0.6296\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:  0.187448  Accuracy:  0.6212\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:  0.16339  Accuracy:  0.6216\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:  0.181503  Accuracy:  0.6134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180, CIFAR-10 Batch 5:  Loss:  0.185916  Accuracy:  0.6138\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.190443  Accuracy:  0.6032\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:  0.158862  Accuracy:  0.6208\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:  0.20338  Accuracy:  0.6072\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:  0.178843  Accuracy:  0.6088\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:  0.170339  Accuracy:  0.6126\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.162485  Accuracy:  0.6246\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:  0.196936  Accuracy:  0.6226\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:  0.190626  Accuracy:  0.6086\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:  0.156453  Accuracy:  0.6226\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:  0.186287  Accuracy:  0.6078\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.200591  Accuracy:  0.6006\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:  0.194374  Accuracy:  0.6166\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:  0.220234  Accuracy:  0.5942\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:  0.188949  Accuracy:  0.6064\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:  0.156505  Accuracy:  0.6206\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.188317  Accuracy:  0.6086\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:  0.165129  Accuracy:  0.6074\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:  0.187759  Accuracy:  0.6146\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:  0.167295  Accuracy:  0.6174\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:  0.151908  Accuracy:  0.6218\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.173573  Accuracy:  0.6156\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:  0.164235  Accuracy:  0.6228\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:  0.188736  Accuracy:  0.6142\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:  0.168196  Accuracy:  0.6148\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:  0.142095  Accuracy:  0.6166\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.173733  Accuracy:  0.6082\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:  0.151047  Accuracy:  0.6192\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:  0.152621  Accuracy:  0.6152\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:  0.146747  Accuracy:  0.6136\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:  0.121769  Accuracy:  0.6224\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.154047  Accuracy:  0.614\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:  0.165061  Accuracy:  0.611\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:  0.1563  Accuracy:  0.618\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:  0.1328  Accuracy:  0.6284\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:  0.109899  Accuracy:  0.6254\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.167048  Accuracy:  0.6182\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:  0.144713  Accuracy:  0.6214\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:  0.156303  Accuracy:  0.6138\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:  0.126478  Accuracy:  0.6198\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:  0.15484  Accuracy:  0.6256\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.126183  Accuracy:  0.6206\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:  0.151638  Accuracy:  0.6182\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:  0.152171  Accuracy:  0.614\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:  0.131823  Accuracy:  0.6158\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:  0.118415  Accuracy:  0.6152\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.146807  Accuracy:  0.616\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:  0.185147  Accuracy:  0.6118\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:  0.120378  Accuracy:  0.6188\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:  0.161115  Accuracy:  0.6084\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:  0.139725  Accuracy:  0.6208\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.158714  Accuracy:  0.6082\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:  0.158289  Accuracy:  0.628\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:  0.138907  Accuracy:  0.6202\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:  0.167126  Accuracy:  0.6108\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:  0.144726  Accuracy:  0.6128\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.140266  Accuracy:  0.6166\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:  0.16138  Accuracy:  0.6258\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:  0.123274  Accuracy:  0.6238\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:  0.152852  Accuracy:  0.6238\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:  0.173613  Accuracy:  0.6158\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  0.136589  Accuracy:  0.6226\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:  0.123913  Accuracy:  0.6188\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:  0.138429  Accuracy:  0.6158\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:  0.160185  Accuracy:  0.6088\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:  0.130887  Accuracy:  0.6194\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  0.111515  Accuracy:  0.633\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:  0.116951  Accuracy:  0.6366\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:  0.137819  Accuracy:  0.617\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:  0.136757  Accuracy:  0.6132\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:  0.151817  Accuracy:  0.6104\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.12357  Accuracy:  0.6216\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:  0.144812  Accuracy:  0.6208\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:  0.130935  Accuracy:  0.6248\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:  0.136234  Accuracy:  0.6118\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:  0.159804  Accuracy:  0.6052\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.143482  Accuracy:  0.6282\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:  0.125886  Accuracy:  0.6322\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:  0.132411  Accuracy:  0.6202\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:  0.157316  Accuracy:  0.605\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:  0.145142  Accuracy:  0.6172\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.132265  Accuracy:  0.6328\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:  0.117911  Accuracy:  0.6218\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:  0.147062  Accuracy:  0.6144\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:  0.165789  Accuracy:  0.6202\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:  0.128098  Accuracy:  0.6196\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.12125  Accuracy:  0.6226\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:  0.14163  Accuracy:  0.6162\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:  0.122107  Accuracy:  0.622\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:  0.125636  Accuracy:  0.6258\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:  0.14015  Accuracy:  0.62\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.12364  Accuracy:  0.6334\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:  0.147833  Accuracy:  0.621\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:  0.110066  Accuracy:  0.6222\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:  0.128093  Accuracy:  0.627\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:  0.0938929  Accuracy:  0.626\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.102046  Accuracy:  0.626\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:  0.11521  Accuracy:  0.624\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:  0.139492  Accuracy:  0.6198\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:  0.113538  Accuracy:  0.6176\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:  0.106553  Accuracy:  0.6202\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.110705  Accuracy:  0.6206\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:  0.104924  Accuracy:  0.624\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:  0.111126  Accuracy:  0.6284\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:  0.150962  Accuracy:  0.6136\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss:  0.136448  Accuracy:  0.6226\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.109945  Accuracy:  0.6216\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss:  0.115689  Accuracy:  0.6278\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss:  0.101081  Accuracy:  0.6268\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss:  0.140541  Accuracy:  0.6146\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss:  0.112898  Accuracy:  0.622\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.14  Accuracy:  0.622\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss:  0.111391  Accuracy:  0.6276\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss:  0.0915314  Accuracy:  0.6306\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss:  0.0878569  Accuracy:  0.6238\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss:  0.0949446  Accuracy:  0.6152\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.0919846  Accuracy:  0.623\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss:  0.112953  Accuracy:  0.6276\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss:  0.103304  Accuracy:  0.6278\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss:  0.104166  Accuracy:  0.6306\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss:  0.0903711  Accuracy:  0.6304\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.119501  Accuracy:  0.6272\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss:  0.121351  Accuracy:  0.6168\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss:  0.105991  Accuracy:  0.6232\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss:  0.105898  Accuracy:  0.612\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss:  0.0997189  Accuracy:  0.6252\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.113248  Accuracy:  0.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206, CIFAR-10 Batch 2:  Loss:  0.0940145  Accuracy:  0.6292\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss:  0.0920674  Accuracy:  0.6332\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss:  0.111864  Accuracy:  0.6308\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss:  0.118031  Accuracy:  0.623\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.110053  Accuracy:  0.6254\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss:  0.11124  Accuracy:  0.6238\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss:  0.0934415  Accuracy:  0.626\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss:  0.103235  Accuracy:  0.6232\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss:  0.121006  Accuracy:  0.6152\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.0987952  Accuracy:  0.6308\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss:  0.102926  Accuracy:  0.6206\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss:  0.0991686  Accuracy:  0.6212\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss:  0.111895  Accuracy:  0.6204\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss:  0.0880018  Accuracy:  0.6272\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.101739  Accuracy:  0.623\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss:  0.090157  Accuracy:  0.6218\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss:  0.0852202  Accuracy:  0.6264\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss:  0.124889  Accuracy:  0.6106\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss:  0.0826498  Accuracy:  0.6198\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.0765644  Accuracy:  0.6226\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss:  0.0948099  Accuracy:  0.6236\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss:  0.0829589  Accuracy:  0.6294\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss:  0.107199  Accuracy:  0.6186\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss:  0.0969653  Accuracy:  0.6254\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.0865521  Accuracy:  0.6364\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss:  0.121504  Accuracy:  0.6254\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss:  0.0867558  Accuracy:  0.6266\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss:  0.110831  Accuracy:  0.6206\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss:  0.089445  Accuracy:  0.6334\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.106489  Accuracy:  0.6312\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss:  0.115154  Accuracy:  0.617\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss:  0.0943992  Accuracy:  0.626\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss:  0.145092  Accuracy:  0.6268\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss:  0.105109  Accuracy:  0.6248\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.0805991  Accuracy:  0.6278\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss:  0.0902708  Accuracy:  0.6176\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss:  0.0939164  Accuracy:  0.6228\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss:  0.145823  Accuracy:  0.6058\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss:  0.104552  Accuracy:  0.6164\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.11422  Accuracy:  0.628\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss:  0.112799  Accuracy:  0.626\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss:  0.11246  Accuracy:  0.6284\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss:  0.109895  Accuracy:  0.6222\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss:  0.104556  Accuracy:  0.6214\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.125618  Accuracy:  0.624\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss:  0.0922087  Accuracy:  0.627\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss:  0.106203  Accuracy:  0.6322\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss:  0.10549  Accuracy:  0.6164\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss:  0.0749423  Accuracy:  0.6254\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.131316  Accuracy:  0.6166\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss:  0.111518  Accuracy:  0.6188\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss:  0.0926224  Accuracy:  0.6218\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss:  0.132395  Accuracy:  0.6108\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss:  0.126154  Accuracy:  0.6238\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.104344  Accuracy:  0.6168\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss:  0.103009  Accuracy:  0.6206\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss:  0.141325  Accuracy:  0.6148\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss:  0.114084  Accuracy:  0.6182\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss:  0.09361  Accuracy:  0.6214\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.0937243  Accuracy:  0.6242\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss:  0.125569  Accuracy:  0.6038\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss:  0.107417  Accuracy:  0.613\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss:  0.143342  Accuracy:  0.6074\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss:  0.131293  Accuracy:  0.6156\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.116342  Accuracy:  0.6144\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss:  0.154531  Accuracy:  0.6136\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss:  0.148714  Accuracy:  0.613\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss:  0.146525  Accuracy:  0.6096\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss:  0.129732  Accuracy:  0.6082\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.120609  Accuracy:  0.6294\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss:  0.121703  Accuracy:  0.6164\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss:  0.125471  Accuracy:  0.6156\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss:  0.112389  Accuracy:  0.6162\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss:  0.090538  Accuracy:  0.617\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.109621  Accuracy:  0.6232\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss:  0.134184  Accuracy:  0.614\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss:  0.14751  Accuracy:  0.6054\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss:  0.126101  Accuracy:  0.6058\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss:  0.116886  Accuracy:  0.6136\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.107669  Accuracy:  0.623\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss:  0.138806  Accuracy:  0.6202\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss:  0.0970065  Accuracy:  0.6208\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss:  0.116718  Accuracy:  0.6164\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss:  0.100157  Accuracy:  0.6238\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.12075  Accuracy:  0.6332\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss:  0.148201  Accuracy:  0.6018\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss:  0.123072  Accuracy:  0.6212\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss:  0.100831  Accuracy:  0.6156\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss:  0.104295  Accuracy:  0.6282\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.149811  Accuracy:  0.6178\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss:  0.14036  Accuracy:  0.6082\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss:  0.14324  Accuracy:  0.6206\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss:  0.116423  Accuracy:  0.6194\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss:  0.096377  Accuracy:  0.6182\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.0856891  Accuracy:  0.6298\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss:  0.127478  Accuracy:  0.6224\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss:  0.136635  Accuracy:  0.6112\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss:  0.134088  Accuracy:  0.6064\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss:  0.100903  Accuracy:  0.6174\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.116564  Accuracy:  0.6142\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss:  0.144912  Accuracy:  0.6108\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss:  0.102382  Accuracy:  0.629\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss:  0.0820947  Accuracy:  0.6122\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss:  0.0896181  Accuracy:  0.6244\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.0987615  Accuracy:  0.6172\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss:  0.131119  Accuracy:  0.6082\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss:  0.0935266  Accuracy:  0.6256\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss:  0.0998327  Accuracy:  0.6198\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss:  0.096521  Accuracy:  0.6134\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.118775  Accuracy:  0.6098\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss:  0.123942  Accuracy:  0.6108\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss:  0.140443  Accuracy:  0.6202\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss:  0.113142  Accuracy:  0.6168\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss:  0.108984  Accuracy:  0.6188\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.130741  Accuracy:  0.6192\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss:  0.109828  Accuracy:  0.6156\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss:  0.137433  Accuracy:  0.6074\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss:  0.100377  Accuracy:  0.6194\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss:  0.114319  Accuracy:  0.6158\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.105708  Accuracy:  0.619\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss:  0.117386  Accuracy:  0.62\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss:  0.145563  Accuracy:  0.6126\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss:  0.126931  Accuracy:  0.6056\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss:  0.109699  Accuracy:  0.6134\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.137409  Accuracy:  0.6152\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss:  0.127264  Accuracy:  0.6208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231, CIFAR-10 Batch 3:  Loss:  0.165009  Accuracy:  0.6132\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss:  0.158097  Accuracy:  0.5896\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss:  0.128376  Accuracy:  0.614\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.135676  Accuracy:  0.6148\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss:  0.123821  Accuracy:  0.6232\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss:  0.105402  Accuracy:  0.6222\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss:  0.161644  Accuracy:  0.6146\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss:  0.168829  Accuracy:  0.6158\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.132669  Accuracy:  0.6086\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss:  0.188246  Accuracy:  0.604\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss:  0.116721  Accuracy:  0.6214\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss:  0.119702  Accuracy:  0.6124\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss:  0.128196  Accuracy:  0.6146\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.102939  Accuracy:  0.6128\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss:  0.169062  Accuracy:  0.6078\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss:  0.146557  Accuracy:  0.6144\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss:  0.122842  Accuracy:  0.6236\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss:  0.10414  Accuracy:  0.6122\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.101954  Accuracy:  0.6154\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss:  0.089035  Accuracy:  0.6162\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss:  0.108845  Accuracy:  0.6352\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss:  0.114145  Accuracy:  0.621\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss:  0.0696568  Accuracy:  0.6134\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.0963798  Accuracy:  0.6264\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss:  0.134599  Accuracy:  0.6228\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss:  0.116813  Accuracy:  0.6176\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss:  0.102478  Accuracy:  0.6184\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss:  0.103936  Accuracy:  0.62\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.0886406  Accuracy:  0.6372\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss:  0.109374  Accuracy:  0.6098\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss:  0.124068  Accuracy:  0.6126\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss:  0.119749  Accuracy:  0.6094\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss:  0.124004  Accuracy:  0.6054\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.121642  Accuracy:  0.6154\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss:  0.109207  Accuracy:  0.6316\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss:  0.0928502  Accuracy:  0.6244\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss:  0.0984997  Accuracy:  0.6204\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss:  0.107987  Accuracy:  0.614\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.13393  Accuracy:  0.6166\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss:  0.126957  Accuracy:  0.6188\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss:  0.0875984  Accuracy:  0.6228\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss:  0.0908085  Accuracy:  0.614\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss:  0.084632  Accuracy:  0.6276\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.0883764  Accuracy:  0.626\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss:  0.101961  Accuracy:  0.6238\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss:  0.12087  Accuracy:  0.6256\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss:  0.0989963  Accuracy:  0.6122\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss:  0.082473  Accuracy:  0.62\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.0872612  Accuracy:  0.6266\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss:  0.0801901  Accuracy:  0.625\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss:  0.0872707  Accuracy:  0.6268\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss:  0.0755425  Accuracy:  0.6222\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss:  0.0753758  Accuracy:  0.622\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.0828885  Accuracy:  0.6236\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss:  0.077193  Accuracy:  0.6236\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss:  0.070353  Accuracy:  0.6234\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss:  0.0632311  Accuracy:  0.6366\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss:  0.0718134  Accuracy:  0.6208\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.0950706  Accuracy:  0.6176\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss:  0.0614283  Accuracy:  0.6294\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss:  0.0558209  Accuracy:  0.6256\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss:  0.0776782  Accuracy:  0.6302\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss:  0.0892362  Accuracy:  0.6194\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.0893901  Accuracy:  0.6132\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss:  0.0871754  Accuracy:  0.6136\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss:  0.0823358  Accuracy:  0.631\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss:  0.112353  Accuracy:  0.6268\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss:  0.0745323  Accuracy:  0.6194\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.0600104  Accuracy:  0.6246\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss:  0.0671344  Accuracy:  0.6296\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss:  0.0864326  Accuracy:  0.6334\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss:  0.082799  Accuracy:  0.6166\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss:  0.0897665  Accuracy:  0.6212\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.0873493  Accuracy:  0.6232\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss:  0.0920584  Accuracy:  0.6256\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss:  0.0766752  Accuracy:  0.6334\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss:  0.0826663  Accuracy:  0.6302\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss:  0.0805714  Accuracy:  0.6306\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.0587528  Accuracy:  0.6218\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss:  0.076355  Accuracy:  0.6186\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss:  0.111838  Accuracy:  0.63\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss:  0.0661541  Accuracy:  0.6232\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss:  0.0643192  Accuracy:  0.6186\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.0856909  Accuracy:  0.6218\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss:  0.127934  Accuracy:  0.6142\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss:  0.0988165  Accuracy:  0.6234\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss:  0.100123  Accuracy:  0.61\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss:  0.0897701  Accuracy:  0.6288\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.0770646  Accuracy:  0.6238\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss:  0.085453  Accuracy:  0.6236\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss:  0.0843631  Accuracy:  0.6298\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss:  0.0802497  Accuracy:  0.6102\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss:  0.0995367  Accuracy:  0.6138\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.119063  Accuracy:  0.6276\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss:  0.0867692  Accuracy:  0.6374\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss:  0.0715475  Accuracy:  0.6278\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss:  0.0850902  Accuracy:  0.6224\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss:  0.07279  Accuracy:  0.6204\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.0994505  Accuracy:  0.6176\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss:  0.0809221  Accuracy:  0.6388\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss:  0.0873945  Accuracy:  0.6238\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss:  0.120591  Accuracy:  0.6262\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss:  0.100585  Accuracy:  0.62\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.0833241  Accuracy:  0.6256\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss:  0.0929442  Accuracy:  0.626\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss:  0.0651885  Accuracy:  0.635\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss:  0.0894211  Accuracy:  0.6238\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss:  0.067352  Accuracy:  0.6244\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.0801293  Accuracy:  0.6286\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss:  0.0713388  Accuracy:  0.6358\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss:  0.0678749  Accuracy:  0.6324\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss:  0.0603941  Accuracy:  0.64\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss:  0.0827041  Accuracy:  0.6276\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.0537267  Accuracy:  0.6272\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss:  0.0773294  Accuracy:  0.6308\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss:  0.0677229  Accuracy:  0.6332\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss:  0.0558924  Accuracy:  0.6346\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss:  0.0497441  Accuracy:  0.6262\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.0583283  Accuracy:  0.6202\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss:  0.0802692  Accuracy:  0.6302\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss:  0.045871  Accuracy:  0.632\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss:  0.061797  Accuracy:  0.6264\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss:  0.0630268  Accuracy:  0.6282\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.0587597  Accuracy:  0.6266\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss:  0.0621847  Accuracy:  0.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256, CIFAR-10 Batch 3:  Loss:  0.0911319  Accuracy:  0.6294\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss:  0.0585086  Accuracy:  0.6302\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss:  0.0491734  Accuracy:  0.6366\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:  0.0523579  Accuracy:  0.621\n",
      "Epoch 257, CIFAR-10 Batch 2:  Loss:  0.0632464  Accuracy:  0.6342\n",
      "Epoch 257, CIFAR-10 Batch 3:  Loss:  0.0371064  Accuracy:  0.6276\n",
      "Epoch 257, CIFAR-10 Batch 4:  Loss:  0.0444548  Accuracy:  0.6306\n",
      "Epoch 257, CIFAR-10 Batch 5:  Loss:  0.046455  Accuracy:  0.6282\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:  0.0692625  Accuracy:  0.6284\n",
      "Epoch 258, CIFAR-10 Batch 2:  Loss:  0.0511811  Accuracy:  0.6384\n",
      "Epoch 258, CIFAR-10 Batch 3:  Loss:  0.0587788  Accuracy:  0.6336\n",
      "Epoch 258, CIFAR-10 Batch 4:  Loss:  0.064857  Accuracy:  0.6306\n",
      "Epoch 258, CIFAR-10 Batch 5:  Loss:  0.0310593  Accuracy:  0.618\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:  0.0595751  Accuracy:  0.628\n",
      "Epoch 259, CIFAR-10 Batch 2:  Loss:  0.0503789  Accuracy:  0.6372\n",
      "Epoch 259, CIFAR-10 Batch 3:  Loss:  0.050646  Accuracy:  0.632\n",
      "Epoch 259, CIFAR-10 Batch 4:  Loss:  0.0578772  Accuracy:  0.6258\n",
      "Epoch 259, CIFAR-10 Batch 5:  Loss:  0.0640716  Accuracy:  0.6316\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:  0.0561831  Accuracy:  0.6228\n",
      "Epoch 260, CIFAR-10 Batch 2:  Loss:  0.0446038  Accuracy:  0.6382\n",
      "Epoch 260, CIFAR-10 Batch 3:  Loss:  0.0402842  Accuracy:  0.6306\n",
      "Epoch 260, CIFAR-10 Batch 4:  Loss:  0.0643387  Accuracy:  0.6274\n",
      "Epoch 260, CIFAR-10 Batch 5:  Loss:  0.0668824  Accuracy:  0.6246\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:  0.0907401  Accuracy:  0.624\n",
      "Epoch 261, CIFAR-10 Batch 2:  Loss:  0.0518444  Accuracy:  0.6304\n",
      "Epoch 261, CIFAR-10 Batch 3:  Loss:  0.0741501  Accuracy:  0.6306\n",
      "Epoch 261, CIFAR-10 Batch 4:  Loss:  0.0403305  Accuracy:  0.6308\n",
      "Epoch 261, CIFAR-10 Batch 5:  Loss:  0.0679982  Accuracy:  0.6252\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:  0.0534552  Accuracy:  0.6246\n",
      "Epoch 262, CIFAR-10 Batch 2:  Loss:  0.0607809  Accuracy:  0.6258\n",
      "Epoch 262, CIFAR-10 Batch 3:  Loss:  0.0528999  Accuracy:  0.6278\n",
      "Epoch 262, CIFAR-10 Batch 4:  Loss:  0.0521229  Accuracy:  0.6304\n",
      "Epoch 262, CIFAR-10 Batch 5:  Loss:  0.0355941  Accuracy:  0.6326\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:  0.0506161  Accuracy:  0.6306\n",
      "Epoch 263, CIFAR-10 Batch 2:  Loss:  0.0438699  Accuracy:  0.6214\n",
      "Epoch 263, CIFAR-10 Batch 3:  Loss:  0.0741661  Accuracy:  0.6286\n",
      "Epoch 263, CIFAR-10 Batch 4:  Loss:  0.0507559  Accuracy:  0.6328\n",
      "Epoch 263, CIFAR-10 Batch 5:  Loss:  0.0525492  Accuracy:  0.6338\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:  0.0778127  Accuracy:  0.626\n",
      "Epoch 264, CIFAR-10 Batch 2:  Loss:  0.0998222  Accuracy:  0.623\n",
      "Epoch 264, CIFAR-10 Batch 3:  Loss:  0.0579274  Accuracy:  0.6198\n",
      "Epoch 264, CIFAR-10 Batch 4:  Loss:  0.0545172  Accuracy:  0.6266\n",
      "Epoch 264, CIFAR-10 Batch 5:  Loss:  0.0361077  Accuracy:  0.6206\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:  0.0543094  Accuracy:  0.626\n",
      "Epoch 265, CIFAR-10 Batch 2:  Loss:  0.0891583  Accuracy:  0.6194\n",
      "Epoch 265, CIFAR-10 Batch 3:  Loss:  0.0706189  Accuracy:  0.6276\n",
      "Epoch 265, CIFAR-10 Batch 4:  Loss:  0.0464588  Accuracy:  0.6324\n",
      "Epoch 265, CIFAR-10 Batch 5:  Loss:  0.0572551  Accuracy:  0.6314\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:  0.0422263  Accuracy:  0.6248\n",
      "Epoch 266, CIFAR-10 Batch 2:  Loss:  0.039189  Accuracy:  0.6352\n",
      "Epoch 266, CIFAR-10 Batch 3:  Loss:  0.0545835  Accuracy:  0.6316\n",
      "Epoch 266, CIFAR-10 Batch 4:  Loss:  0.0695756  Accuracy:  0.6292\n",
      "Epoch 266, CIFAR-10 Batch 5:  Loss:  0.0521393  Accuracy:  0.6282\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:  0.0513704  Accuracy:  0.6226\n",
      "Epoch 267, CIFAR-10 Batch 2:  Loss:  0.0548303  Accuracy:  0.6298\n",
      "Epoch 267, CIFAR-10 Batch 3:  Loss:  0.0434894  Accuracy:  0.632\n",
      "Epoch 267, CIFAR-10 Batch 4:  Loss:  0.068888  Accuracy:  0.629\n",
      "Epoch 267, CIFAR-10 Batch 5:  Loss:  0.0652125  Accuracy:  0.6202\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:  0.0702569  Accuracy:  0.6266\n",
      "Epoch 268, CIFAR-10 Batch 2:  Loss:  0.0502996  Accuracy:  0.6264\n",
      "Epoch 268, CIFAR-10 Batch 3:  Loss:  0.0570209  Accuracy:  0.6278\n",
      "Epoch 268, CIFAR-10 Batch 4:  Loss:  0.0670146  Accuracy:  0.6272\n",
      "Epoch 268, CIFAR-10 Batch 5:  Loss:  0.0502221  Accuracy:  0.6202\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:  0.069714  Accuracy:  0.6246\n",
      "Epoch 269, CIFAR-10 Batch 2:  Loss:  0.0866225  Accuracy:  0.6284\n",
      "Epoch 269, CIFAR-10 Batch 3:  Loss:  0.081445  Accuracy:  0.625\n",
      "Epoch 269, CIFAR-10 Batch 4:  Loss:  0.0589058  Accuracy:  0.6338\n",
      "Epoch 269, CIFAR-10 Batch 5:  Loss:  0.0453731  Accuracy:  0.627\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:  0.0427237  Accuracy:  0.63\n",
      "Epoch 270, CIFAR-10 Batch 2:  Loss:  0.0674087  Accuracy:  0.6222\n",
      "Epoch 270, CIFAR-10 Batch 3:  Loss:  0.0491086  Accuracy:  0.633\n",
      "Epoch 270, CIFAR-10 Batch 4:  Loss:  0.0573708  Accuracy:  0.6368\n",
      "Epoch 270, CIFAR-10 Batch 5:  Loss:  0.0774461  Accuracy:  0.6212\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:  0.0566033  Accuracy:  0.6248\n",
      "Epoch 271, CIFAR-10 Batch 2:  Loss:  0.0467775  Accuracy:  0.6268\n",
      "Epoch 271, CIFAR-10 Batch 3:  Loss:  0.053246  Accuracy:  0.631\n",
      "Epoch 271, CIFAR-10 Batch 4:  Loss:  0.0489169  Accuracy:  0.6308\n",
      "Epoch 271, CIFAR-10 Batch 5:  Loss:  0.0400336  Accuracy:  0.622\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:  0.046804  Accuracy:  0.6294\n",
      "Epoch 272, CIFAR-10 Batch 2:  Loss:  0.0619865  Accuracy:  0.637\n",
      "Epoch 272, CIFAR-10 Batch 3:  Loss:  0.0697727  Accuracy:  0.632\n",
      "Epoch 272, CIFAR-10 Batch 4:  Loss:  0.043521  Accuracy:  0.6318\n",
      "Epoch 272, CIFAR-10 Batch 5:  Loss:  0.0370281  Accuracy:  0.6292\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:  0.0425682  Accuracy:  0.636\n",
      "Epoch 273, CIFAR-10 Batch 2:  Loss:  0.0737472  Accuracy:  0.6202\n",
      "Epoch 273, CIFAR-10 Batch 3:  Loss:  0.0508831  Accuracy:  0.6292\n",
      "Epoch 273, CIFAR-10 Batch 4:  Loss:  0.0546684  Accuracy:  0.6338\n",
      "Epoch 273, CIFAR-10 Batch 5:  Loss:  0.0409717  Accuracy:  0.6272\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:  0.0444352  Accuracy:  0.6256\n",
      "Epoch 274, CIFAR-10 Batch 2:  Loss:  0.0655408  Accuracy:  0.6322\n",
      "Epoch 274, CIFAR-10 Batch 3:  Loss:  0.0639299  Accuracy:  0.6238\n",
      "Epoch 274, CIFAR-10 Batch 4:  Loss:  0.0418095  Accuracy:  0.6232\n",
      "Epoch 274, CIFAR-10 Batch 5:  Loss:  0.0548139  Accuracy:  0.6362\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:  0.0557197  Accuracy:  0.6284\n",
      "Epoch 275, CIFAR-10 Batch 2:  Loss:  0.0642082  Accuracy:  0.6292\n",
      "Epoch 275, CIFAR-10 Batch 3:  Loss:  0.0613303  Accuracy:  0.6328\n",
      "Epoch 275, CIFAR-10 Batch 4:  Loss:  0.0737246  Accuracy:  0.6342\n",
      "Epoch 275, CIFAR-10 Batch 5:  Loss:  0.048844  Accuracy:  0.6324\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:  0.0479478  Accuracy:  0.6308\n",
      "Epoch 276, CIFAR-10 Batch 2:  Loss:  0.055431  Accuracy:  0.6188\n",
      "Epoch 276, CIFAR-10 Batch 3:  Loss:  0.0558801  Accuracy:  0.6326\n",
      "Epoch 276, CIFAR-10 Batch 4:  Loss:  0.0425526  Accuracy:  0.634\n",
      "Epoch 276, CIFAR-10 Batch 5:  Loss:  0.0465825  Accuracy:  0.6346\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:  0.0621334  Accuracy:  0.6316\n",
      "Epoch 277, CIFAR-10 Batch 2:  Loss:  0.0871416  Accuracy:  0.6234\n",
      "Epoch 277, CIFAR-10 Batch 3:  Loss:  0.0688614  Accuracy:  0.6242\n",
      "Epoch 277, CIFAR-10 Batch 4:  Loss:  0.0517482  Accuracy:  0.633\n",
      "Epoch 277, CIFAR-10 Batch 5:  Loss:  0.0535358  Accuracy:  0.6282\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:  0.0503266  Accuracy:  0.631\n",
      "Epoch 278, CIFAR-10 Batch 2:  Loss:  0.0609727  Accuracy:  0.6328\n",
      "Epoch 278, CIFAR-10 Batch 3:  Loss:  0.0487899  Accuracy:  0.6316\n",
      "Epoch 278, CIFAR-10 Batch 4:  Loss:  0.0828504  Accuracy:  0.6224\n",
      "Epoch 278, CIFAR-10 Batch 5:  Loss:  0.0514957  Accuracy:  0.6284\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:  0.0587043  Accuracy:  0.6282\n",
      "Epoch 279, CIFAR-10 Batch 2:  Loss:  0.057697  Accuracy:  0.6276\n",
      "Epoch 279, CIFAR-10 Batch 3:  Loss:  0.085769  Accuracy:  0.6232\n",
      "Epoch 279, CIFAR-10 Batch 4:  Loss:  0.0752297  Accuracy:  0.62\n",
      "Epoch 279, CIFAR-10 Batch 5:  Loss:  0.0654328  Accuracy:  0.622\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:  0.0663154  Accuracy:  0.6282\n",
      "Epoch 280, CIFAR-10 Batch 2:  Loss:  0.0551739  Accuracy:  0.6368\n",
      "Epoch 280, CIFAR-10 Batch 3:  Loss:  0.0921582  Accuracy:  0.6266\n",
      "Epoch 280, CIFAR-10 Batch 4:  Loss:  0.0761361  Accuracy:  0.614\n",
      "Epoch 280, CIFAR-10 Batch 5:  Loss:  0.0442809  Accuracy:  0.6326\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:  0.0499913  Accuracy:  0.627\n",
      "Epoch 281, CIFAR-10 Batch 2:  Loss:  0.0646569  Accuracy:  0.6202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281, CIFAR-10 Batch 3:  Loss:  0.0516736  Accuracy:  0.6302\n",
      "Epoch 281, CIFAR-10 Batch 4:  Loss:  0.0549551  Accuracy:  0.618\n",
      "Epoch 281, CIFAR-10 Batch 5:  Loss:  0.0645655  Accuracy:  0.6264\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:  0.0693379  Accuracy:  0.6092\n",
      "Epoch 282, CIFAR-10 Batch 2:  Loss:  0.0792156  Accuracy:  0.6246\n",
      "Epoch 282, CIFAR-10 Batch 3:  Loss:  0.0793459  Accuracy:  0.6242\n",
      "Epoch 282, CIFAR-10 Batch 4:  Loss:  0.0650482  Accuracy:  0.619\n",
      "Epoch 282, CIFAR-10 Batch 5:  Loss:  0.0958624  Accuracy:  0.6174\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:  0.0789826  Accuracy:  0.6174\n",
      "Epoch 283, CIFAR-10 Batch 2:  Loss:  0.0731559  Accuracy:  0.6242\n",
      "Epoch 283, CIFAR-10 Batch 3:  Loss:  0.0721503  Accuracy:  0.6306\n",
      "Epoch 283, CIFAR-10 Batch 4:  Loss:  0.0607382  Accuracy:  0.6296\n",
      "Epoch 283, CIFAR-10 Batch 5:  Loss:  0.0546058  Accuracy:  0.6158\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:  0.095153  Accuracy:  0.6288\n",
      "Epoch 284, CIFAR-10 Batch 2:  Loss:  0.0833169  Accuracy:  0.628\n",
      "Epoch 284, CIFAR-10 Batch 3:  Loss:  0.0712591  Accuracy:  0.6228\n",
      "Epoch 284, CIFAR-10 Batch 4:  Loss:  0.0672555  Accuracy:  0.6298\n",
      "Epoch 284, CIFAR-10 Batch 5:  Loss:  0.0473777  Accuracy:  0.6222\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:  0.0626708  Accuracy:  0.632\n",
      "Epoch 285, CIFAR-10 Batch 2:  Loss:  0.0566288  Accuracy:  0.6338\n",
      "Epoch 285, CIFAR-10 Batch 3:  Loss:  0.0627636  Accuracy:  0.6258\n",
      "Epoch 285, CIFAR-10 Batch 4:  Loss:  0.056636  Accuracy:  0.6292\n",
      "Epoch 285, CIFAR-10 Batch 5:  Loss:  0.0683726  Accuracy:  0.6242\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:  0.0578216  Accuracy:  0.6206\n",
      "Epoch 286, CIFAR-10 Batch 2:  Loss:  0.0654432  Accuracy:  0.6318\n",
      "Epoch 286, CIFAR-10 Batch 3:  Loss:  0.0633393  Accuracy:  0.622\n",
      "Epoch 286, CIFAR-10 Batch 4:  Loss:  0.0530812  Accuracy:  0.6312\n",
      "Epoch 286, CIFAR-10 Batch 5:  Loss:  0.053075  Accuracy:  0.625\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:  0.0754422  Accuracy:  0.626\n",
      "Epoch 287, CIFAR-10 Batch 2:  Loss:  0.0750335  Accuracy:  0.624\n",
      "Epoch 287, CIFAR-10 Batch 3:  Loss:  0.0490579  Accuracy:  0.6386\n",
      "Epoch 287, CIFAR-10 Batch 4:  Loss:  0.0385133  Accuracy:  0.6314\n",
      "Epoch 287, CIFAR-10 Batch 5:  Loss:  0.0483068  Accuracy:  0.6226\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:  0.0394852  Accuracy:  0.6356\n",
      "Epoch 288, CIFAR-10 Batch 2:  Loss:  0.0527877  Accuracy:  0.6356\n",
      "Epoch 288, CIFAR-10 Batch 3:  Loss:  0.0388759  Accuracy:  0.633\n",
      "Epoch 288, CIFAR-10 Batch 4:  Loss:  0.0555438  Accuracy:  0.6236\n",
      "Epoch 288, CIFAR-10 Batch 5:  Loss:  0.09077  Accuracy:  0.6238\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:  0.0556057  Accuracy:  0.625\n",
      "Epoch 289, CIFAR-10 Batch 2:  Loss:  0.0672942  Accuracy:  0.629\n",
      "Epoch 289, CIFAR-10 Batch 3:  Loss:  0.0406066  Accuracy:  0.6382\n",
      "Epoch 289, CIFAR-10 Batch 4:  Loss:  0.0569157  Accuracy:  0.6174\n",
      "Epoch 289, CIFAR-10 Batch 5:  Loss:  0.0474349  Accuracy:  0.6338\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:  0.0480266  Accuracy:  0.6288\n",
      "Epoch 290, CIFAR-10 Batch 2:  Loss:  0.0485007  Accuracy:  0.6362\n",
      "Epoch 290, CIFAR-10 Batch 3:  Loss:  0.0696984  Accuracy:  0.6304\n",
      "Epoch 290, CIFAR-10 Batch 4:  Loss:  0.0489607  Accuracy:  0.6308\n",
      "Epoch 290, CIFAR-10 Batch 5:  Loss:  0.0488525  Accuracy:  0.6266\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:  0.0595508  Accuracy:  0.6274\n",
      "Epoch 291, CIFAR-10 Batch 2:  Loss:  0.0649589  Accuracy:  0.623\n",
      "Epoch 291, CIFAR-10 Batch 3:  Loss:  0.0489409  Accuracy:  0.6304\n",
      "Epoch 291, CIFAR-10 Batch 4:  Loss:  0.054407  Accuracy:  0.6264\n",
      "Epoch 291, CIFAR-10 Batch 5:  Loss:  0.0461596  Accuracy:  0.6234\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:  0.050837  Accuracy:  0.6188\n",
      "Epoch 292, CIFAR-10 Batch 2:  Loss:  0.0551529  Accuracy:  0.6338\n",
      "Epoch 292, CIFAR-10 Batch 3:  Loss:  0.0424253  Accuracy:  0.634\n",
      "Epoch 292, CIFAR-10 Batch 4:  Loss:  0.0650484  Accuracy:  0.6246\n",
      "Epoch 292, CIFAR-10 Batch 5:  Loss:  0.0737874  Accuracy:  0.6206\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:  0.0561129  Accuracy:  0.6254\n",
      "Epoch 293, CIFAR-10 Batch 2:  Loss:  0.0684936  Accuracy:  0.6328\n",
      "Epoch 293, CIFAR-10 Batch 3:  Loss:  0.0813235  Accuracy:  0.6222\n",
      "Epoch 293, CIFAR-10 Batch 4:  Loss:  0.0799935  Accuracy:  0.6224\n",
      "Epoch 293, CIFAR-10 Batch 5:  Loss:  0.0663249  Accuracy:  0.6162\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:  0.0728594  Accuracy:  0.62\n",
      "Epoch 294, CIFAR-10 Batch 2:  Loss:  0.0690134  Accuracy:  0.6308\n",
      "Epoch 294, CIFAR-10 Batch 3:  Loss:  0.0561998  Accuracy:  0.6312\n",
      "Epoch 294, CIFAR-10 Batch 4:  Loss:  0.0524488  Accuracy:  0.6358\n",
      "Epoch 294, CIFAR-10 Batch 5:  Loss:  0.0538839  Accuracy:  0.6226\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:  0.0651158  Accuracy:  0.6236\n",
      "Epoch 295, CIFAR-10 Batch 2:  Loss:  0.0521798  Accuracy:  0.6226\n",
      "Epoch 295, CIFAR-10 Batch 3:  Loss:  0.0541643  Accuracy:  0.6272\n",
      "Epoch 295, CIFAR-10 Batch 4:  Loss:  0.0539235  Accuracy:  0.628\n",
      "Epoch 295, CIFAR-10 Batch 5:  Loss:  0.0426912  Accuracy:  0.6226\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:  0.0492973  Accuracy:  0.6276\n",
      "Epoch 296, CIFAR-10 Batch 2:  Loss:  0.056959  Accuracy:  0.6328\n",
      "Epoch 296, CIFAR-10 Batch 3:  Loss:  0.0489372  Accuracy:  0.6334\n",
      "Epoch 296, CIFAR-10 Batch 4:  Loss:  0.0530044  Accuracy:  0.6252\n",
      "Epoch 296, CIFAR-10 Batch 5:  Loss:  0.0642107  Accuracy:  0.6298\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:  0.0699302  Accuracy:  0.6256\n",
      "Epoch 297, CIFAR-10 Batch 2:  Loss:  0.0694531  Accuracy:  0.6206\n",
      "Epoch 297, CIFAR-10 Batch 3:  Loss:  0.0615768  Accuracy:  0.6316\n",
      "Epoch 297, CIFAR-10 Batch 4:  Loss:  0.0414105  Accuracy:  0.6324\n",
      "Epoch 297, CIFAR-10 Batch 5:  Loss:  0.0438068  Accuracy:  0.6248\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:  0.0489457  Accuracy:  0.6236\n",
      "Epoch 298, CIFAR-10 Batch 2:  Loss:  0.0439331  Accuracy:  0.6232\n",
      "Epoch 298, CIFAR-10 Batch 3:  Loss:  0.0743813  Accuracy:  0.6286\n",
      "Epoch 298, CIFAR-10 Batch 4:  Loss:  0.0713141  Accuracy:  0.626\n",
      "Epoch 298, CIFAR-10 Batch 5:  Loss:  0.0636109  Accuracy:  0.6324\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:  0.0443445  Accuracy:  0.6316\n",
      "Epoch 299, CIFAR-10 Batch 2:  Loss:  0.0527542  Accuracy:  0.6236\n",
      "Epoch 299, CIFAR-10 Batch 3:  Loss:  0.0675593  Accuracy:  0.6326\n",
      "Epoch 299, CIFAR-10 Batch 4:  Loss:  0.0525167  Accuracy:  0.623\n",
      "Epoch 299, CIFAR-10 Batch 5:  Loss:  0.0645785  Accuracy:  0.6264\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:  0.0632382  Accuracy:  0.6364\n",
      "Epoch 300, CIFAR-10 Batch 2:  Loss:  0.0503806  Accuracy:  0.6218\n",
      "Epoch 300, CIFAR-10 Batch 3:  Loss:  0.0633764  Accuracy:  0.6338\n",
      "Epoch 300, CIFAR-10 Batch 4:  Loss:  0.0375152  Accuracy:  0.6294\n",
      "Epoch 300, CIFAR-10 Batch 5:  Loss:  0.0632453  Accuracy:  0.6284\n",
      "Epoch 301, CIFAR-10 Batch 1:  Loss:  0.0350863  Accuracy:  0.638\n",
      "Epoch 301, CIFAR-10 Batch 2:  Loss:  0.0506499  Accuracy:  0.6262\n",
      "Epoch 301, CIFAR-10 Batch 3:  Loss:  0.0574212  Accuracy:  0.6332\n",
      "Epoch 301, CIFAR-10 Batch 4:  Loss:  0.0697331  Accuracy:  0.625\n",
      "Epoch 301, CIFAR-10 Batch 5:  Loss:  0.0562935  Accuracy:  0.6128\n",
      "Epoch 302, CIFAR-10 Batch 1:  Loss:  0.034759  Accuracy:  0.6326\n",
      "Epoch 302, CIFAR-10 Batch 2:  Loss:  0.04595  Accuracy:  0.6266\n",
      "Epoch 302, CIFAR-10 Batch 3:  Loss:  0.0393446  Accuracy:  0.6298\n",
      "Epoch 302, CIFAR-10 Batch 4:  Loss:  0.0369594  Accuracy:  0.6354\n",
      "Epoch 302, CIFAR-10 Batch 5:  Loss:  0.0611725  Accuracy:  0.6274\n",
      "Epoch 303, CIFAR-10 Batch 1:  Loss:  0.0447224  Accuracy:  0.6302\n",
      "Epoch 303, CIFAR-10 Batch 2:  Loss:  0.0600006  Accuracy:  0.626\n",
      "Epoch 303, CIFAR-10 Batch 3:  Loss:  0.06157  Accuracy:  0.6346\n",
      "Epoch 303, CIFAR-10 Batch 4:  Loss:  0.0514349  Accuracy:  0.6358\n",
      "Epoch 303, CIFAR-10 Batch 5:  Loss:  0.0430286  Accuracy:  0.6326\n",
      "Epoch 304, CIFAR-10 Batch 1:  Loss:  0.0538625  Accuracy:  0.629\n",
      "Epoch 304, CIFAR-10 Batch 2:  Loss:  0.0525035  Accuracy:  0.6362\n",
      "Epoch 304, CIFAR-10 Batch 3:  Loss:  0.0456661  Accuracy:  0.6284\n",
      "Epoch 304, CIFAR-10 Batch 4:  Loss:  0.0305604  Accuracy:  0.6352\n",
      "Epoch 304, CIFAR-10 Batch 5:  Loss:  0.071589  Accuracy:  0.6186\n",
      "Epoch 305, CIFAR-10 Batch 1:  Loss:  0.0726846  Accuracy:  0.6228\n",
      "Epoch 305, CIFAR-10 Batch 2:  Loss:  0.0720409  Accuracy:  0.6216\n",
      "Epoch 305, CIFAR-10 Batch 3:  Loss:  0.0575939  Accuracy:  0.626\n",
      "Epoch 305, CIFAR-10 Batch 4:  Loss:  0.0460117  Accuracy:  0.6178\n",
      "Epoch 305, CIFAR-10 Batch 5:  Loss:  0.0649619  Accuracy:  0.6262\n",
      "Epoch 306, CIFAR-10 Batch 1:  Loss:  0.0536738  Accuracy:  0.6236\n",
      "Epoch 306, CIFAR-10 Batch 2:  Loss:  0.0633181  Accuracy:  0.6288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306, CIFAR-10 Batch 3:  Loss:  0.0534556  Accuracy:  0.632\n",
      "Epoch 306, CIFAR-10 Batch 4:  Loss:  0.0454174  Accuracy:  0.6332\n",
      "Epoch 306, CIFAR-10 Batch 5:  Loss:  0.0477893  Accuracy:  0.6304\n",
      "Epoch 307, CIFAR-10 Batch 1:  Loss:  0.0476074  Accuracy:  0.6202\n",
      "Epoch 307, CIFAR-10 Batch 2:  Loss:  0.0650721  Accuracy:  0.6244\n",
      "Epoch 307, CIFAR-10 Batch 3:  Loss:  0.032479  Accuracy:  0.6308\n",
      "Epoch 307, CIFAR-10 Batch 4:  Loss:  0.0527328  Accuracy:  0.6332\n",
      "Epoch 307, CIFAR-10 Batch 5:  Loss:  0.0415349  Accuracy:  0.6282\n",
      "Epoch 308, CIFAR-10 Batch 1:  Loss:  0.0904404  Accuracy:  0.6202\n",
      "Epoch 308, CIFAR-10 Batch 2:  Loss:  0.0644987  Accuracy:  0.6142\n",
      "Epoch 308, CIFAR-10 Batch 3:  Loss:  0.0512814  Accuracy:  0.6252\n",
      "Epoch 308, CIFAR-10 Batch 4:  Loss:  0.0666597  Accuracy:  0.6288\n",
      "Epoch 308, CIFAR-10 Batch 5:  Loss:  0.0387181  Accuracy:  0.6332\n",
      "Epoch 309, CIFAR-10 Batch 1:  Loss:  0.0547622  Accuracy:  0.621\n",
      "Epoch 309, CIFAR-10 Batch 2:  Loss:  0.0559733  Accuracy:  0.6244\n",
      "Epoch 309, CIFAR-10 Batch 3:  Loss:  0.0568588  Accuracy:  0.6136\n",
      "Epoch 309, CIFAR-10 Batch 4:  Loss:  0.0544562  Accuracy:  0.6226\n",
      "Epoch 309, CIFAR-10 Batch 5:  Loss:  0.0705164  Accuracy:  0.616\n",
      "Epoch 310, CIFAR-10 Batch 1:  Loss:  0.066318  Accuracy:  0.6252\n",
      "Epoch 310, CIFAR-10 Batch 2:  Loss:  0.0551564  Accuracy:  0.6288\n",
      "Epoch 310, CIFAR-10 Batch 3:  Loss:  0.0623754  Accuracy:  0.619\n",
      "Epoch 310, CIFAR-10 Batch 4:  Loss:  0.0554083  Accuracy:  0.6272\n",
      "Epoch 310, CIFAR-10 Batch 5:  Loss:  0.078096  Accuracy:  0.6272\n",
      "Epoch 311, CIFAR-10 Batch 1:  Loss:  0.0661222  Accuracy:  0.624\n",
      "Epoch 311, CIFAR-10 Batch 2:  Loss:  0.041948  Accuracy:  0.6194\n",
      "Epoch 311, CIFAR-10 Batch 3:  Loss:  0.065329  Accuracy:  0.6338\n",
      "Epoch 311, CIFAR-10 Batch 4:  Loss:  0.0400868  Accuracy:  0.6348\n",
      "Epoch 311, CIFAR-10 Batch 5:  Loss:  0.0441629  Accuracy:  0.6274\n",
      "Epoch 312, CIFAR-10 Batch 1:  Loss:  0.0780109  Accuracy:  0.6264\n",
      "Epoch 312, CIFAR-10 Batch 2:  Loss:  0.0568991  Accuracy:  0.6228\n",
      "Epoch 312, CIFAR-10 Batch 3:  Loss:  0.0721883  Accuracy:  0.6278\n",
      "Epoch 312, CIFAR-10 Batch 4:  Loss:  0.0727822  Accuracy:  0.6194\n",
      "Epoch 312, CIFAR-10 Batch 5:  Loss:  0.0502044  Accuracy:  0.6278\n",
      "Epoch 313, CIFAR-10 Batch 1:  Loss:  0.0703826  Accuracy:  0.6192\n",
      "Epoch 313, CIFAR-10 Batch 2:  Loss:  0.101648  Accuracy:  0.6074\n",
      "Epoch 313, CIFAR-10 Batch 3:  Loss:  0.0654553  Accuracy:  0.6206\n",
      "Epoch 313, CIFAR-10 Batch 4:  Loss:  0.0793086  Accuracy:  0.6196\n",
      "Epoch 313, CIFAR-10 Batch 5:  Loss:  0.0539781  Accuracy:  0.624\n",
      "Epoch 314, CIFAR-10 Batch 1:  Loss:  0.0431509  Accuracy:  0.637\n",
      "Epoch 314, CIFAR-10 Batch 2:  Loss:  0.0927538  Accuracy:  0.615\n",
      "Epoch 314, CIFAR-10 Batch 3:  Loss:  0.106873  Accuracy:  0.6212\n",
      "Epoch 314, CIFAR-10 Batch 4:  Loss:  0.0697444  Accuracy:  0.622\n",
      "Epoch 314, CIFAR-10 Batch 5:  Loss:  0.0666344  Accuracy:  0.6164\n",
      "Epoch 315, CIFAR-10 Batch 1:  Loss:  0.0515323  Accuracy:  0.6252\n",
      "Epoch 315, CIFAR-10 Batch 2:  Loss:  0.0709358  Accuracy:  0.6208\n",
      "Epoch 315, CIFAR-10 Batch 3:  Loss:  0.0674606  Accuracy:  0.6254\n",
      "Epoch 315, CIFAR-10 Batch 4:  Loss:  0.0486632  Accuracy:  0.6206\n",
      "Epoch 315, CIFAR-10 Batch 5:  Loss:  0.0737076  Accuracy:  0.612\n",
      "Epoch 316, CIFAR-10 Batch 1:  Loss:  0.0884493  Accuracy:  0.6212\n",
      "Epoch 316, CIFAR-10 Batch 2:  Loss:  0.0597182  Accuracy:  0.6252\n",
      "Epoch 316, CIFAR-10 Batch 3:  Loss:  0.0461634  Accuracy:  0.6318\n",
      "Epoch 316, CIFAR-10 Batch 4:  Loss:  0.0468613  Accuracy:  0.6308\n",
      "Epoch 316, CIFAR-10 Batch 5:  Loss:  0.0780884  Accuracy:  0.6228\n",
      "Epoch 317, CIFAR-10 Batch 1:  Loss:  0.0695131  Accuracy:  0.617\n",
      "Epoch 317, CIFAR-10 Batch 2:  Loss:  0.0681463  Accuracy:  0.6334\n",
      "Epoch 317, CIFAR-10 Batch 3:  Loss:  0.0518575  Accuracy:  0.6298\n",
      "Epoch 317, CIFAR-10 Batch 4:  Loss:  0.0577643  Accuracy:  0.6326\n",
      "Epoch 317, CIFAR-10 Batch 5:  Loss:  0.0542283  Accuracy:  0.6282\n",
      "Epoch 318, CIFAR-10 Batch 1:  Loss:  0.055764  Accuracy:  0.6256\n",
      "Epoch 318, CIFAR-10 Batch 2:  Loss:  0.0761791  Accuracy:  0.6172\n",
      "Epoch 318, CIFAR-10 Batch 3:  Loss:  0.0573263  Accuracy:  0.6306\n",
      "Epoch 318, CIFAR-10 Batch 4:  Loss:  0.055795  Accuracy:  0.6248\n",
      "Epoch 318, CIFAR-10 Batch 5:  Loss:  0.0536877  Accuracy:  0.6284\n",
      "Epoch 319, CIFAR-10 Batch 1:  Loss:  0.0534605  Accuracy:  0.6254\n",
      "Epoch 319, CIFAR-10 Batch 2:  Loss:  0.0497783  Accuracy:  0.6306\n",
      "Epoch 319, CIFAR-10 Batch 3:  Loss:  0.0512821  Accuracy:  0.6296\n",
      "Epoch 319, CIFAR-10 Batch 4:  Loss:  0.054364  Accuracy:  0.6326\n",
      "Epoch 319, CIFAR-10 Batch 5:  Loss:  0.035077  Accuracy:  0.629\n",
      "Epoch 320, CIFAR-10 Batch 1:  Loss:  0.0618744  Accuracy:  0.6194\n",
      "Epoch 320, CIFAR-10 Batch 2:  Loss:  0.0589439  Accuracy:  0.6218\n",
      "Epoch 320, CIFAR-10 Batch 3:  Loss:  0.0318687  Accuracy:  0.6232\n",
      "Epoch 320, CIFAR-10 Batch 4:  Loss:  0.0503918  Accuracy:  0.6298\n",
      "Epoch 320, CIFAR-10 Batch 5:  Loss:  0.0457763  Accuracy:  0.6398\n",
      "Epoch 321, CIFAR-10 Batch 1:  Loss:  0.0494788  Accuracy:  0.6206\n",
      "Epoch 321, CIFAR-10 Batch 2:  Loss:  0.0407965  Accuracy:  0.6248\n",
      "Epoch 321, CIFAR-10 Batch 3:  Loss:  0.0455698  Accuracy:  0.6172\n",
      "Epoch 321, CIFAR-10 Batch 4:  Loss:  0.0400375  Accuracy:  0.6296\n",
      "Epoch 321, CIFAR-10 Batch 5:  Loss:  0.0463306  Accuracy:  0.6276\n",
      "Epoch 322, CIFAR-10 Batch 1:  Loss:  0.0410623  Accuracy:  0.6284\n",
      "Epoch 322, CIFAR-10 Batch 2:  Loss:  0.0630783  Accuracy:  0.6312\n",
      "Epoch 322, CIFAR-10 Batch 3:  Loss:  0.0363113  Accuracy:  0.6238\n",
      "Epoch 322, CIFAR-10 Batch 4:  Loss:  0.0457622  Accuracy:  0.6254\n",
      "Epoch 322, CIFAR-10 Batch 5:  Loss:  0.0458214  Accuracy:  0.6238\n",
      "Epoch 323, CIFAR-10 Batch 1:  Loss:  0.0460332  Accuracy:  0.6348\n",
      "Epoch 323, CIFAR-10 Batch 2:  Loss:  0.0509928  Accuracy:  0.6252\n",
      "Epoch 323, CIFAR-10 Batch 3:  Loss:  0.0434877  Accuracy:  0.622\n",
      "Epoch 323, CIFAR-10 Batch 4:  Loss:  0.037013  Accuracy:  0.6232\n",
      "Epoch 323, CIFAR-10 Batch 5:  Loss:  0.0538824  Accuracy:  0.627\n",
      "Epoch 324, CIFAR-10 Batch 1:  Loss:  0.0402081  Accuracy:  0.6276\n",
      "Epoch 324, CIFAR-10 Batch 2:  Loss:  0.0579209  Accuracy:  0.6236\n",
      "Epoch 324, CIFAR-10 Batch 3:  Loss:  0.0501498  Accuracy:  0.6334\n",
      "Epoch 324, CIFAR-10 Batch 4:  Loss:  0.0516882  Accuracy:  0.6172\n",
      "Epoch 324, CIFAR-10 Batch 5:  Loss:  0.055274  Accuracy:  0.6328\n",
      "Epoch 325, CIFAR-10 Batch 1:  Loss:  0.0387889  Accuracy:  0.6278\n",
      "Epoch 325, CIFAR-10 Batch 2:  Loss:  0.0498278  Accuracy:  0.624\n",
      "Epoch 325, CIFAR-10 Batch 3:  Loss:  0.0437593  Accuracy:  0.6278\n",
      "Epoch 325, CIFAR-10 Batch 4:  Loss:  0.038703  Accuracy:  0.6224\n",
      "Epoch 325, CIFAR-10 Batch 5:  Loss:  0.0569171  Accuracy:  0.6144\n",
      "Epoch 326, CIFAR-10 Batch 1:  Loss:  0.0534327  Accuracy:  0.625\n",
      "Epoch 326, CIFAR-10 Batch 2:  Loss:  0.0558968  Accuracy:  0.6292\n",
      "Epoch 326, CIFAR-10 Batch 3:  Loss:  0.0317641  Accuracy:  0.6292\n",
      "Epoch 326, CIFAR-10 Batch 4:  Loss:  0.0391324  Accuracy:  0.621\n",
      "Epoch 326, CIFAR-10 Batch 5:  Loss:  0.0548663  Accuracy:  0.6234\n",
      "Epoch 327, CIFAR-10 Batch 1:  Loss:  0.0412518  Accuracy:  0.6242\n",
      "Epoch 327, CIFAR-10 Batch 2:  Loss:  0.0708696  Accuracy:  0.6284\n",
      "Epoch 327, CIFAR-10 Batch 3:  Loss:  0.0583162  Accuracy:  0.6216\n",
      "Epoch 327, CIFAR-10 Batch 4:  Loss:  0.0518861  Accuracy:  0.6158\n",
      "Epoch 327, CIFAR-10 Batch 5:  Loss:  0.0493586  Accuracy:  0.6306\n",
      "Epoch 328, CIFAR-10 Batch 1:  Loss:  0.0412519  Accuracy:  0.6276\n",
      "Epoch 328, CIFAR-10 Batch 2:  Loss:  0.0320377  Accuracy:  0.6288\n",
      "Epoch 328, CIFAR-10 Batch 3:  Loss:  0.0493717  Accuracy:  0.629\n",
      "Epoch 328, CIFAR-10 Batch 4:  Loss:  0.0453473  Accuracy:  0.633\n",
      "Epoch 328, CIFAR-10 Batch 5:  Loss:  0.0621746  Accuracy:  0.6218\n",
      "Epoch 329, CIFAR-10 Batch 1:  Loss:  0.0593045  Accuracy:  0.6302\n",
      "Epoch 329, CIFAR-10 Batch 2:  Loss:  0.053196  Accuracy:  0.622\n",
      "Epoch 329, CIFAR-10 Batch 3:  Loss:  0.0625997  Accuracy:  0.6232\n",
      "Epoch 329, CIFAR-10 Batch 4:  Loss:  0.0656545  Accuracy:  0.6112\n",
      "Epoch 329, CIFAR-10 Batch 5:  Loss:  0.0911077  Accuracy:  0.6252\n",
      "Epoch 330, CIFAR-10 Batch 1:  Loss:  0.050334  Accuracy:  0.6224\n",
      "Epoch 330, CIFAR-10 Batch 2:  Loss:  0.0643648  Accuracy:  0.6334\n",
      "Epoch 330, CIFAR-10 Batch 3:  Loss:  0.0628215  Accuracy:  0.6222\n",
      "Epoch 330, CIFAR-10 Batch 4:  Loss:  0.0784087  Accuracy:  0.62\n",
      "Epoch 330, CIFAR-10 Batch 5:  Loss:  0.0642591  Accuracy:  0.6282\n",
      "Epoch 331, CIFAR-10 Batch 1:  Loss:  0.0596333  Accuracy:  0.6138\n",
      "Epoch 331, CIFAR-10 Batch 2:  Loss:  0.0823797  Accuracy:  0.6392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331, CIFAR-10 Batch 3:  Loss:  0.0550433  Accuracy:  0.6298\n",
      "Epoch 331, CIFAR-10 Batch 4:  Loss:  0.044062  Accuracy:  0.632\n",
      "Epoch 331, CIFAR-10 Batch 5:  Loss:  0.0463564  Accuracy:  0.632\n",
      "Epoch 332, CIFAR-10 Batch 1:  Loss:  0.0837423  Accuracy:  0.6144\n",
      "Epoch 332, CIFAR-10 Batch 2:  Loss:  0.0452804  Accuracy:  0.6258\n",
      "Epoch 332, CIFAR-10 Batch 3:  Loss:  0.0574166  Accuracy:  0.6216\n",
      "Epoch 332, CIFAR-10 Batch 4:  Loss:  0.0627718  Accuracy:  0.635\n",
      "Epoch 332, CIFAR-10 Batch 5:  Loss:  0.0442619  Accuracy:  0.6254\n",
      "Epoch 333, CIFAR-10 Batch 1:  Loss:  0.0469891  Accuracy:  0.6312\n",
      "Epoch 333, CIFAR-10 Batch 2:  Loss:  0.040109  Accuracy:  0.6236\n",
      "Epoch 333, CIFAR-10 Batch 3:  Loss:  0.0451266  Accuracy:  0.6276\n",
      "Epoch 333, CIFAR-10 Batch 4:  Loss:  0.0570923  Accuracy:  0.6332\n",
      "Epoch 333, CIFAR-10 Batch 5:  Loss:  0.035204  Accuracy:  0.6226\n",
      "Epoch 334, CIFAR-10 Batch 1:  Loss:  0.05371  Accuracy:  0.6278\n",
      "Epoch 334, CIFAR-10 Batch 2:  Loss:  0.0383866  Accuracy:  0.6362\n",
      "Epoch 334, CIFAR-10 Batch 3:  Loss:  0.0352707  Accuracy:  0.6334\n",
      "Epoch 334, CIFAR-10 Batch 4:  Loss:  0.0674153  Accuracy:  0.6246\n",
      "Epoch 334, CIFAR-10 Batch 5:  Loss:  0.0608975  Accuracy:  0.6312\n",
      "Epoch 335, CIFAR-10 Batch 1:  Loss:  0.0573657  Accuracy:  0.63\n",
      "Epoch 335, CIFAR-10 Batch 2:  Loss:  0.0555528  Accuracy:  0.6282\n",
      "Epoch 335, CIFAR-10 Batch 3:  Loss:  0.0497699  Accuracy:  0.6314\n",
      "Epoch 335, CIFAR-10 Batch 4:  Loss:  0.0480889  Accuracy:  0.6232\n",
      "Epoch 335, CIFAR-10 Batch 5:  Loss:  0.0495699  Accuracy:  0.632\n",
      "Epoch 336, CIFAR-10 Batch 1:  Loss:  0.0486355  Accuracy:  0.6314\n",
      "Epoch 336, CIFAR-10 Batch 2:  Loss:  0.0427598  Accuracy:  0.6338\n",
      "Epoch 336, CIFAR-10 Batch 3:  Loss:  0.0543263  Accuracy:  0.633\n",
      "Epoch 336, CIFAR-10 Batch 4:  Loss:  0.0444253  Accuracy:  0.6234\n",
      "Epoch 336, CIFAR-10 Batch 5:  Loss:  0.0478019  Accuracy:  0.6232\n",
      "Epoch 337, CIFAR-10 Batch 1:  Loss:  0.0677828  Accuracy:  0.6238\n",
      "Epoch 337, CIFAR-10 Batch 2:  Loss:  0.0540161  Accuracy:  0.6298\n",
      "Epoch 337, CIFAR-10 Batch 3:  Loss:  0.0435564  Accuracy:  0.6296\n",
      "Epoch 337, CIFAR-10 Batch 4:  Loss:  0.0370201  Accuracy:  0.6288\n",
      "Epoch 337, CIFAR-10 Batch 5:  Loss:  0.030348  Accuracy:  0.6314\n",
      "Epoch 338, CIFAR-10 Batch 1:  Loss:  0.0388294  Accuracy:  0.6282\n",
      "Epoch 338, CIFAR-10 Batch 2:  Loss:  0.0360685  Accuracy:  0.6322\n",
      "Epoch 338, CIFAR-10 Batch 3:  Loss:  0.0449058  Accuracy:  0.6288\n",
      "Epoch 338, CIFAR-10 Batch 4:  Loss:  0.0416858  Accuracy:  0.6158\n",
      "Epoch 338, CIFAR-10 Batch 5:  Loss:  0.0428902  Accuracy:  0.6308\n",
      "Epoch 339, CIFAR-10 Batch 1:  Loss:  0.0620714  Accuracy:  0.633\n",
      "Epoch 339, CIFAR-10 Batch 2:  Loss:  0.0592935  Accuracy:  0.6306\n",
      "Epoch 339, CIFAR-10 Batch 3:  Loss:  0.0755804  Accuracy:  0.6304\n",
      "Epoch 339, CIFAR-10 Batch 4:  Loss:  0.0635837  Accuracy:  0.6148\n",
      "Epoch 339, CIFAR-10 Batch 5:  Loss:  0.0463806  Accuracy:  0.6404\n",
      "Epoch 340, CIFAR-10 Batch 1:  Loss:  0.0624033  Accuracy:  0.6168\n",
      "Epoch 340, CIFAR-10 Batch 2:  Loss:  0.0718379  Accuracy:  0.634\n",
      "Epoch 340, CIFAR-10 Batch 3:  Loss:  0.0480167  Accuracy:  0.6252\n",
      "Epoch 340, CIFAR-10 Batch 4:  Loss:  0.0366391  Accuracy:  0.6334\n",
      "Epoch 340, CIFAR-10 Batch 5:  Loss:  0.0365906  Accuracy:  0.6326\n",
      "Epoch 341, CIFAR-10 Batch 1:  Loss:  0.051153  Accuracy:  0.6348\n",
      "Epoch 341, CIFAR-10 Batch 2:  Loss:  0.0642635  Accuracy:  0.6284\n",
      "Epoch 341, CIFAR-10 Batch 3:  Loss:  0.0506903  Accuracy:  0.63\n",
      "Epoch 341, CIFAR-10 Batch 4:  Loss:  0.0577235  Accuracy:  0.632\n",
      "Epoch 341, CIFAR-10 Batch 5:  Loss:  0.0314321  Accuracy:  0.626\n",
      "Epoch 342, CIFAR-10 Batch 1:  Loss:  0.0592082  Accuracy:  0.6248\n",
      "Epoch 342, CIFAR-10 Batch 2:  Loss:  0.0430423  Accuracy:  0.6338\n",
      "Epoch 342, CIFAR-10 Batch 3:  Loss:  0.0471351  Accuracy:  0.6108\n",
      "Epoch 342, CIFAR-10 Batch 4:  Loss:  0.039291  Accuracy:  0.6276\n",
      "Epoch 342, CIFAR-10 Batch 5:  Loss:  0.0535536  Accuracy:  0.6342\n",
      "Epoch 343, CIFAR-10 Batch 1:  Loss:  0.0708629  Accuracy:  0.618\n",
      "Epoch 343, CIFAR-10 Batch 2:  Loss:  0.0377236  Accuracy:  0.6286\n",
      "Epoch 343, CIFAR-10 Batch 3:  Loss:  0.0532682  Accuracy:  0.6292\n",
      "Epoch 343, CIFAR-10 Batch 4:  Loss:  0.0502217  Accuracy:  0.6268\n",
      "Epoch 343, CIFAR-10 Batch 5:  Loss:  0.0221408  Accuracy:  0.6298\n",
      "Epoch 344, CIFAR-10 Batch 1:  Loss:  0.0594862  Accuracy:  0.6282\n",
      "Epoch 344, CIFAR-10 Batch 2:  Loss:  0.0378896  Accuracy:  0.628\n",
      "Epoch 344, CIFAR-10 Batch 3:  Loss:  0.0595649  Accuracy:  0.6248\n",
      "Epoch 344, CIFAR-10 Batch 4:  Loss:  0.0681597  Accuracy:  0.6252\n",
      "Epoch 344, CIFAR-10 Batch 5:  Loss:  0.0262701  Accuracy:  0.63\n",
      "Epoch 345, CIFAR-10 Batch 1:  Loss:  0.0363048  Accuracy:  0.628\n",
      "Epoch 345, CIFAR-10 Batch 2:  Loss:  0.0528152  Accuracy:  0.6326\n",
      "Epoch 345, CIFAR-10 Batch 3:  Loss:  0.0509514  Accuracy:  0.6334\n",
      "Epoch 345, CIFAR-10 Batch 4:  Loss:  0.0467253  Accuracy:  0.6252\n",
      "Epoch 345, CIFAR-10 Batch 5:  Loss:  0.0542878  Accuracy:  0.6242\n",
      "Epoch 346, CIFAR-10 Batch 1:  Loss:  0.0490415  Accuracy:  0.6182\n",
      "Epoch 346, CIFAR-10 Batch 2:  Loss:  0.067542  Accuracy:  0.6258\n",
      "Epoch 346, CIFAR-10 Batch 3:  Loss:  0.046037  Accuracy:  0.6182\n",
      "Epoch 346, CIFAR-10 Batch 4:  Loss:  0.0410223  Accuracy:  0.6242\n",
      "Epoch 346, CIFAR-10 Batch 5:  Loss:  0.0558598  Accuracy:  0.6268\n",
      "Epoch 347, CIFAR-10 Batch 1:  Loss:  0.0774296  Accuracy:  0.6268\n",
      "Epoch 347, CIFAR-10 Batch 2:  Loss:  0.0511587  Accuracy:  0.6424\n",
      "Epoch 347, CIFAR-10 Batch 3:  Loss:  0.0575278  Accuracy:  0.6262\n",
      "Epoch 347, CIFAR-10 Batch 4:  Loss:  0.0501491  Accuracy:  0.624\n",
      "Epoch 347, CIFAR-10 Batch 5:  Loss:  0.0427051  Accuracy:  0.6366\n",
      "Epoch 348, CIFAR-10 Batch 1:  Loss:  0.0428717  Accuracy:  0.63\n",
      "Epoch 348, CIFAR-10 Batch 2:  Loss:  0.0856589  Accuracy:  0.6328\n",
      "Epoch 348, CIFAR-10 Batch 3:  Loss:  0.0684891  Accuracy:  0.627\n",
      "Epoch 348, CIFAR-10 Batch 4:  Loss:  0.0498667  Accuracy:  0.6326\n",
      "Epoch 348, CIFAR-10 Batch 5:  Loss:  0.0500768  Accuracy:  0.638\n",
      "Epoch 349, CIFAR-10 Batch 1:  Loss:  0.0685393  Accuracy:  0.6272\n",
      "Epoch 349, CIFAR-10 Batch 2:  Loss:  0.0687205  Accuracy:  0.6364\n",
      "Epoch 349, CIFAR-10 Batch 3:  Loss:  0.0411528  Accuracy:  0.631\n",
      "Epoch 349, CIFAR-10 Batch 4:  Loss:  0.0427927  Accuracy:  0.6226\n",
      "Epoch 349, CIFAR-10 Batch 5:  Loss:  0.0408981  Accuracy:  0.6432\n",
      "Epoch 350, CIFAR-10 Batch 1:  Loss:  0.0463574  Accuracy:  0.6334\n",
      "Epoch 350, CIFAR-10 Batch 2:  Loss:  0.0497295  Accuracy:  0.6356\n",
      "Epoch 350, CIFAR-10 Batch 3:  Loss:  0.045377  Accuracy:  0.6462\n",
      "Epoch 350, CIFAR-10 Batch 4:  Loss:  0.0386508  Accuracy:  0.626\n",
      "Epoch 350, CIFAR-10 Batch 5:  Loss:  0.0454216  Accuracy:  0.6368\n",
      "Epoch 351, CIFAR-10 Batch 1:  Loss:  0.0548622  Accuracy:  0.6302\n",
      "Epoch 351, CIFAR-10 Batch 2:  Loss:  0.0710401  Accuracy:  0.6264\n",
      "Epoch 351, CIFAR-10 Batch 3:  Loss:  0.0405832  Accuracy:  0.6338\n",
      "Epoch 351, CIFAR-10 Batch 4:  Loss:  0.0414068  Accuracy:  0.6242\n",
      "Epoch 351, CIFAR-10 Batch 5:  Loss:  0.0528032  Accuracy:  0.6328\n",
      "Epoch 352, CIFAR-10 Batch 1:  Loss:  0.0673101  Accuracy:  0.6246\n",
      "Epoch 352, CIFAR-10 Batch 2:  Loss:  0.0574382  Accuracy:  0.619\n",
      "Epoch 352, CIFAR-10 Batch 3:  Loss:  0.0463886  Accuracy:  0.63\n",
      "Epoch 352, CIFAR-10 Batch 4:  Loss:  0.0458902  Accuracy:  0.6312\n",
      "Epoch 352, CIFAR-10 Batch 5:  Loss:  0.0343393  Accuracy:  0.6274\n",
      "Epoch 353, CIFAR-10 Batch 1:  Loss:  0.0328406  Accuracy:  0.6394\n",
      "Epoch 353, CIFAR-10 Batch 2:  Loss:  0.0393038  Accuracy:  0.635\n",
      "Epoch 353, CIFAR-10 Batch 3:  Loss:  0.0513765  Accuracy:  0.6328\n",
      "Epoch 353, CIFAR-10 Batch 4:  Loss:  0.0529518  Accuracy:  0.622\n",
      "Epoch 353, CIFAR-10 Batch 5:  Loss:  0.0397024  Accuracy:  0.6312\n",
      "Epoch 354, CIFAR-10 Batch 1:  Loss:  0.0641164  Accuracy:  0.6206\n",
      "Epoch 354, CIFAR-10 Batch 2:  Loss:  0.040962  Accuracy:  0.6316\n",
      "Epoch 354, CIFAR-10 Batch 3:  Loss:  0.0461617  Accuracy:  0.6236\n",
      "Epoch 354, CIFAR-10 Batch 4:  Loss:  0.0494773  Accuracy:  0.6254\n",
      "Epoch 354, CIFAR-10 Batch 5:  Loss:  0.0492726  Accuracy:  0.6374\n",
      "Epoch 355, CIFAR-10 Batch 1:  Loss:  0.047664  Accuracy:  0.6304\n",
      "Epoch 355, CIFAR-10 Batch 2:  Loss:  0.0428945  Accuracy:  0.6408\n",
      "Epoch 355, CIFAR-10 Batch 3:  Loss:  0.0397859  Accuracy:  0.6254\n",
      "Epoch 355, CIFAR-10 Batch 4:  Loss:  0.0338743  Accuracy:  0.6274\n",
      "Epoch 355, CIFAR-10 Batch 5:  Loss:  0.0364218  Accuracy:  0.6292\n",
      "Epoch 356, CIFAR-10 Batch 1:  Loss:  0.0484058  Accuracy:  0.6218\n",
      "Epoch 356, CIFAR-10 Batch 2:  Loss:  0.0634666  Accuracy:  0.6244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356, CIFAR-10 Batch 3:  Loss:  0.0535722  Accuracy:  0.6242\n",
      "Epoch 356, CIFAR-10 Batch 4:  Loss:  0.064413  Accuracy:  0.625\n",
      "Epoch 356, CIFAR-10 Batch 5:  Loss:  0.0387995  Accuracy:  0.6312\n",
      "Epoch 357, CIFAR-10 Batch 1:  Loss:  0.069895  Accuracy:  0.613\n",
      "Epoch 357, CIFAR-10 Batch 2:  Loss:  0.0837876  Accuracy:  0.6262\n",
      "Epoch 357, CIFAR-10 Batch 3:  Loss:  0.0576763  Accuracy:  0.625\n",
      "Epoch 357, CIFAR-10 Batch 4:  Loss:  0.0576221  Accuracy:  0.6152\n",
      "Epoch 357, CIFAR-10 Batch 5:  Loss:  0.0432483  Accuracy:  0.6382\n",
      "Epoch 358, CIFAR-10 Batch 1:  Loss:  0.0578514  Accuracy:  0.6334\n",
      "Epoch 358, CIFAR-10 Batch 2:  Loss:  0.048561  Accuracy:  0.629\n",
      "Epoch 358, CIFAR-10 Batch 3:  Loss:  0.0610107  Accuracy:  0.6286\n",
      "Epoch 358, CIFAR-10 Batch 4:  Loss:  0.0422161  Accuracy:  0.6332\n",
      "Epoch 358, CIFAR-10 Batch 5:  Loss:  0.0530391  Accuracy:  0.6326\n",
      "Epoch 359, CIFAR-10 Batch 1:  Loss:  0.0443532  Accuracy:  0.6374\n",
      "Epoch 359, CIFAR-10 Batch 2:  Loss:  0.0670033  Accuracy:  0.6304\n",
      "Epoch 359, CIFAR-10 Batch 3:  Loss:  0.0427489  Accuracy:  0.626\n",
      "Epoch 359, CIFAR-10 Batch 4:  Loss:  0.0471148  Accuracy:  0.6182\n",
      "Epoch 359, CIFAR-10 Batch 5:  Loss:  0.0288451  Accuracy:  0.641\n",
      "Epoch 360, CIFAR-10 Batch 1:  Loss:  0.046286  Accuracy:  0.6394\n",
      "Epoch 360, CIFAR-10 Batch 2:  Loss:  0.0572446  Accuracy:  0.6362\n",
      "Epoch 360, CIFAR-10 Batch 3:  Loss:  0.0328628  Accuracy:  0.6284\n",
      "Epoch 360, CIFAR-10 Batch 4:  Loss:  0.0448412  Accuracy:  0.6232\n",
      "Epoch 360, CIFAR-10 Batch 5:  Loss:  0.0541363  Accuracy:  0.6316\n",
      "Epoch 361, CIFAR-10 Batch 1:  Loss:  0.0456556  Accuracy:  0.6298\n",
      "Epoch 361, CIFAR-10 Batch 2:  Loss:  0.0440908  Accuracy:  0.627\n",
      "Epoch 361, CIFAR-10 Batch 3:  Loss:  0.0588684  Accuracy:  0.6308\n",
      "Epoch 361, CIFAR-10 Batch 4:  Loss:  0.0392905  Accuracy:  0.617\n",
      "Epoch 361, CIFAR-10 Batch 5:  Loss:  0.0624248  Accuracy:  0.6298\n",
      "Epoch 362, CIFAR-10 Batch 1:  Loss:  0.0755247  Accuracy:  0.6322\n",
      "Epoch 362, CIFAR-10 Batch 2:  Loss:  0.0431565  Accuracy:  0.6272\n",
      "Epoch 362, CIFAR-10 Batch 3:  Loss:  0.058876  Accuracy:  0.6284\n",
      "Epoch 362, CIFAR-10 Batch 4:  Loss:  0.0676516  Accuracy:  0.6178\n",
      "Epoch 362, CIFAR-10 Batch 5:  Loss:  0.0380045  Accuracy:  0.6278\n",
      "Epoch 363, CIFAR-10 Batch 1:  Loss:  0.0521801  Accuracy:  0.628\n",
      "Epoch 363, CIFAR-10 Batch 2:  Loss:  0.030655  Accuracy:  0.6404\n",
      "Epoch 363, CIFAR-10 Batch 3:  Loss:  0.043155  Accuracy:  0.6368\n",
      "Epoch 363, CIFAR-10 Batch 4:  Loss:  0.0445123  Accuracy:  0.6254\n",
      "Epoch 363, CIFAR-10 Batch 5:  Loss:  0.0454628  Accuracy:  0.6366\n",
      "Epoch 364, CIFAR-10 Batch 1:  Loss:  0.0678212  Accuracy:  0.6238\n",
      "Epoch 364, CIFAR-10 Batch 2:  Loss:  0.0479825  Accuracy:  0.6364\n",
      "Epoch 364, CIFAR-10 Batch 3:  Loss:  0.0512821  Accuracy:  0.639\n",
      "Epoch 364, CIFAR-10 Batch 4:  Loss:  0.0552269  Accuracy:  0.6288\n",
      "Epoch 364, CIFAR-10 Batch 5:  Loss:  0.030078  Accuracy:  0.6294\n",
      "Epoch 365, CIFAR-10 Batch 1:  Loss:  0.0538397  Accuracy:  0.6278\n",
      "Epoch 365, CIFAR-10 Batch 2:  Loss:  0.0537218  Accuracy:  0.6374\n",
      "Epoch 365, CIFAR-10 Batch 3:  Loss:  0.0396452  Accuracy:  0.6422\n",
      "Epoch 365, CIFAR-10 Batch 4:  Loss:  0.0452211  Accuracy:  0.6238\n",
      "Epoch 365, CIFAR-10 Batch 5:  Loss:  0.0286056  Accuracy:  0.6296\n",
      "Epoch 366, CIFAR-10 Batch 1:  Loss:  0.0399916  Accuracy:  0.6312\n",
      "Epoch 366, CIFAR-10 Batch 2:  Loss:  0.0422986  Accuracy:  0.6346\n",
      "Epoch 366, CIFAR-10 Batch 3:  Loss:  0.0329933  Accuracy:  0.6306\n",
      "Epoch 366, CIFAR-10 Batch 4:  Loss:  0.0538217  Accuracy:  0.6274\n",
      "Epoch 366, CIFAR-10 Batch 5:  Loss:  0.0358868  Accuracy:  0.6284\n",
      "Epoch 367, CIFAR-10 Batch 1:  Loss:  0.0603401  Accuracy:  0.6316\n",
      "Epoch 367, CIFAR-10 Batch 2:  Loss:  0.0459023  Accuracy:  0.6352\n",
      "Epoch 367, CIFAR-10 Batch 3:  Loss:  0.0467912  Accuracy:  0.6254\n",
      "Epoch 367, CIFAR-10 Batch 4:  Loss:  0.0425334  Accuracy:  0.6292\n",
      "Epoch 367, CIFAR-10 Batch 5:  Loss:  0.048236  Accuracy:  0.6322\n",
      "Epoch 368, CIFAR-10 Batch 1:  Loss:  0.0482219  Accuracy:  0.6294\n",
      "Epoch 368, CIFAR-10 Batch 2:  Loss:  0.0376374  Accuracy:  0.6386\n",
      "Epoch 368, CIFAR-10 Batch 3:  Loss:  0.0521428  Accuracy:  0.6388\n",
      "Epoch 368, CIFAR-10 Batch 4:  Loss:  0.0511419  Accuracy:  0.6346\n",
      "Epoch 368, CIFAR-10 Batch 5:  Loss:  0.0281177  Accuracy:  0.6286\n",
      "Epoch 369, CIFAR-10 Batch 1:  Loss:  0.0415002  Accuracy:  0.638\n",
      "Epoch 369, CIFAR-10 Batch 2:  Loss:  0.0328703  Accuracy:  0.636\n",
      "Epoch 369, CIFAR-10 Batch 3:  Loss:  0.036701  Accuracy:  0.6354\n",
      "Epoch 369, CIFAR-10 Batch 4:  Loss:  0.0672235  Accuracy:  0.6398\n",
      "Epoch 369, CIFAR-10 Batch 5:  Loss:  0.0448651  Accuracy:  0.621\n",
      "Epoch 370, CIFAR-10 Batch 1:  Loss:  0.0424061  Accuracy:  0.6474\n",
      "Epoch 370, CIFAR-10 Batch 2:  Loss:  0.0397852  Accuracy:  0.6362\n",
      "Epoch 370, CIFAR-10 Batch 3:  Loss:  0.049544  Accuracy:  0.6306\n",
      "Epoch 370, CIFAR-10 Batch 4:  Loss:  0.0509211  Accuracy:  0.6396\n",
      "Epoch 370, CIFAR-10 Batch 5:  Loss:  0.0375577  Accuracy:  0.6298\n",
      "Epoch 371, CIFAR-10 Batch 1:  Loss:  0.0434185  Accuracy:  0.6276\n",
      "Epoch 371, CIFAR-10 Batch 2:  Loss:  0.0395445  Accuracy:  0.646\n",
      "Epoch 371, CIFAR-10 Batch 3:  Loss:  0.0507627  Accuracy:  0.6458\n",
      "Epoch 371, CIFAR-10 Batch 4:  Loss:  0.0553672  Accuracy:  0.6274\n",
      "Epoch 371, CIFAR-10 Batch 5:  Loss:  0.0378571  Accuracy:  0.6298\n",
      "Epoch 372, CIFAR-10 Batch 1:  Loss:  0.0700473  Accuracy:  0.6288\n",
      "Epoch 372, CIFAR-10 Batch 2:  Loss:  0.0320007  Accuracy:  0.6364\n",
      "Epoch 372, CIFAR-10 Batch 3:  Loss:  0.0265131  Accuracy:  0.6426\n",
      "Epoch 372, CIFAR-10 Batch 4:  Loss:  0.0588996  Accuracy:  0.6274\n",
      "Epoch 372, CIFAR-10 Batch 5:  Loss:  0.0479124  Accuracy:  0.6284\n",
      "Epoch 373, CIFAR-10 Batch 1:  Loss:  0.0672392  Accuracy:  0.6296\n",
      "Epoch 373, CIFAR-10 Batch 2:  Loss:  0.0504526  Accuracy:  0.6296\n",
      "Epoch 373, CIFAR-10 Batch 3:  Loss:  0.0631713  Accuracy:  0.6478\n",
      "Epoch 373, CIFAR-10 Batch 4:  Loss:  0.043992  Accuracy:  0.6252\n",
      "Epoch 373, CIFAR-10 Batch 5:  Loss:  0.0433308  Accuracy:  0.627\n",
      "Epoch 374, CIFAR-10 Batch 1:  Loss:  0.0333592  Accuracy:  0.6306\n",
      "Epoch 374, CIFAR-10 Batch 2:  Loss:  0.0502529  Accuracy:  0.631\n",
      "Epoch 374, CIFAR-10 Batch 3:  Loss:  0.031929  Accuracy:  0.6324\n",
      "Epoch 374, CIFAR-10 Batch 4:  Loss:  0.0484003  Accuracy:  0.6326\n",
      "Epoch 374, CIFAR-10 Batch 5:  Loss:  0.0471042  Accuracy:  0.6314\n",
      "Epoch 375, CIFAR-10 Batch 1:  Loss:  0.0558676  Accuracy:  0.622\n",
      "Epoch 375, CIFAR-10 Batch 2:  Loss:  0.0423191  Accuracy:  0.6372\n",
      "Epoch 375, CIFAR-10 Batch 3:  Loss:  0.0668933  Accuracy:  0.6288\n",
      "Epoch 375, CIFAR-10 Batch 4:  Loss:  0.0472185  Accuracy:  0.6138\n",
      "Epoch 375, CIFAR-10 Batch 5:  Loss:  0.0307794  Accuracy:  0.6294\n",
      "Epoch 376, CIFAR-10 Batch 1:  Loss:  0.0486904  Accuracy:  0.6304\n",
      "Epoch 376, CIFAR-10 Batch 2:  Loss:  0.03736  Accuracy:  0.6368\n",
      "Epoch 376, CIFAR-10 Batch 3:  Loss:  0.058305  Accuracy:  0.641\n",
      "Epoch 376, CIFAR-10 Batch 4:  Loss:  0.053705  Accuracy:  0.6274\n",
      "Epoch 376, CIFAR-10 Batch 5:  Loss:  0.0430369  Accuracy:  0.6256\n",
      "Epoch 377, CIFAR-10 Batch 1:  Loss:  0.0415689  Accuracy:  0.6372\n",
      "Epoch 377, CIFAR-10 Batch 2:  Loss:  0.0459399  Accuracy:  0.6278\n",
      "Epoch 377, CIFAR-10 Batch 3:  Loss:  0.0372749  Accuracy:  0.6436\n",
      "Epoch 377, CIFAR-10 Batch 4:  Loss:  0.0440502  Accuracy:  0.6424\n",
      "Epoch 377, CIFAR-10 Batch 5:  Loss:  0.0406212  Accuracy:  0.6302\n",
      "Epoch 378, CIFAR-10 Batch 1:  Loss:  0.0637617  Accuracy:  0.6252\n",
      "Epoch 378, CIFAR-10 Batch 2:  Loss:  0.0407779  Accuracy:  0.6362\n",
      "Epoch 378, CIFAR-10 Batch 3:  Loss:  0.0385468  Accuracy:  0.6456\n",
      "Epoch 378, CIFAR-10 Batch 4:  Loss:  0.0341623  Accuracy:  0.6346\n",
      "Epoch 378, CIFAR-10 Batch 5:  Loss:  0.029768  Accuracy:  0.6354\n",
      "Epoch 379, CIFAR-10 Batch 1:  Loss:  0.0460275  Accuracy:  0.6392\n",
      "Epoch 379, CIFAR-10 Batch 2:  Loss:  0.0363418  Accuracy:  0.635\n",
      "Epoch 379, CIFAR-10 Batch 3:  Loss:  0.0317801  Accuracy:  0.6394\n",
      "Epoch 379, CIFAR-10 Batch 4:  Loss:  0.0449686  Accuracy:  0.6178\n",
      "Epoch 379, CIFAR-10 Batch 5:  Loss:  0.0272626  Accuracy:  0.6248\n",
      "Epoch 380, CIFAR-10 Batch 1:  Loss:  0.0327761  Accuracy:  0.633\n",
      "Epoch 380, CIFAR-10 Batch 2:  Loss:  0.033046  Accuracy:  0.6384\n",
      "Epoch 380, CIFAR-10 Batch 3:  Loss:  0.0490469  Accuracy:  0.6368\n",
      "Epoch 380, CIFAR-10 Batch 4:  Loss:  0.0581877  Accuracy:  0.6234\n",
      "Epoch 380, CIFAR-10 Batch 5:  Loss:  0.0333935  Accuracy:  0.6324\n",
      "Epoch 381, CIFAR-10 Batch 1:  Loss:  0.0333498  Accuracy:  0.6334\n",
      "Epoch 381, CIFAR-10 Batch 2:  Loss:  0.0369129  Accuracy:  0.6372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381, CIFAR-10 Batch 3:  Loss:  0.0467289  Accuracy:  0.642\n",
      "Epoch 381, CIFAR-10 Batch 4:  Loss:  0.0598805  Accuracy:  0.6296\n",
      "Epoch 381, CIFAR-10 Batch 5:  Loss:  0.0406443  Accuracy:  0.6322\n",
      "Epoch 382, CIFAR-10 Batch 1:  Loss:  0.0849867  Accuracy:  0.6226\n",
      "Epoch 382, CIFAR-10 Batch 2:  Loss:  0.029396  Accuracy:  0.6432\n",
      "Epoch 382, CIFAR-10 Batch 3:  Loss:  0.0365508  Accuracy:  0.6412\n",
      "Epoch 382, CIFAR-10 Batch 4:  Loss:  0.0286349  Accuracy:  0.6374\n",
      "Epoch 382, CIFAR-10 Batch 5:  Loss:  0.0263529  Accuracy:  0.6298\n",
      "Epoch 383, CIFAR-10 Batch 1:  Loss:  0.0306481  Accuracy:  0.6338\n",
      "Epoch 383, CIFAR-10 Batch 2:  Loss:  0.0282818  Accuracy:  0.6338\n",
      "Epoch 383, CIFAR-10 Batch 3:  Loss:  0.0548809  Accuracy:  0.635\n",
      "Epoch 383, CIFAR-10 Batch 4:  Loss:  0.0463047  Accuracy:  0.6296\n",
      "Epoch 383, CIFAR-10 Batch 5:  Loss:  0.0515916  Accuracy:  0.6264\n",
      "Epoch 384, CIFAR-10 Batch 1:  Loss:  0.0585077  Accuracy:  0.6354\n",
      "Epoch 384, CIFAR-10 Batch 2:  Loss:  0.0523416  Accuracy:  0.6346\n",
      "Epoch 384, CIFAR-10 Batch 3:  Loss:  0.0424044  Accuracy:  0.6372\n",
      "Epoch 384, CIFAR-10 Batch 4:  Loss:  0.0629703  Accuracy:  0.6314\n",
      "Epoch 384, CIFAR-10 Batch 5:  Loss:  0.0528985  Accuracy:  0.6282\n",
      "Epoch 385, CIFAR-10 Batch 1:  Loss:  0.026287  Accuracy:  0.6254\n",
      "Epoch 385, CIFAR-10 Batch 2:  Loss:  0.0349911  Accuracy:  0.632\n",
      "Epoch 385, CIFAR-10 Batch 3:  Loss:  0.0272868  Accuracy:  0.6404\n",
      "Epoch 385, CIFAR-10 Batch 4:  Loss:  0.0325075  Accuracy:  0.6226\n",
      "Epoch 385, CIFAR-10 Batch 5:  Loss:  0.0285451  Accuracy:  0.6256\n",
      "Epoch 386, CIFAR-10 Batch 1:  Loss:  0.0466013  Accuracy:  0.6278\n",
      "Epoch 386, CIFAR-10 Batch 2:  Loss:  0.0434705  Accuracy:  0.6294\n",
      "Epoch 386, CIFAR-10 Batch 3:  Loss:  0.0467808  Accuracy:  0.639\n",
      "Epoch 386, CIFAR-10 Batch 4:  Loss:  0.0569216  Accuracy:  0.6242\n",
      "Epoch 386, CIFAR-10 Batch 5:  Loss:  0.0494316  Accuracy:  0.6306\n",
      "Epoch 387, CIFAR-10 Batch 1:  Loss:  0.0545983  Accuracy:  0.6258\n",
      "Epoch 387, CIFAR-10 Batch 2:  Loss:  0.0381001  Accuracy:  0.635\n",
      "Epoch 387, CIFAR-10 Batch 3:  Loss:  0.041809  Accuracy:  0.6398\n",
      "Epoch 387, CIFAR-10 Batch 4:  Loss:  0.0636069  Accuracy:  0.6276\n",
      "Epoch 387, CIFAR-10 Batch 5:  Loss:  0.0504864  Accuracy:  0.6234\n",
      "Epoch 388, CIFAR-10 Batch 1:  Loss:  0.0319922  Accuracy:  0.6268\n",
      "Epoch 388, CIFAR-10 Batch 2:  Loss:  0.0349606  Accuracy:  0.6326\n",
      "Epoch 388, CIFAR-10 Batch 3:  Loss:  0.0337214  Accuracy:  0.6322\n",
      "Epoch 388, CIFAR-10 Batch 4:  Loss:  0.0382288  Accuracy:  0.6296\n",
      "Epoch 388, CIFAR-10 Batch 5:  Loss:  0.0409971  Accuracy:  0.6346\n",
      "Epoch 389, CIFAR-10 Batch 1:  Loss:  0.0391948  Accuracy:  0.632\n",
      "Epoch 389, CIFAR-10 Batch 2:  Loss:  0.0425222  Accuracy:  0.628\n",
      "Epoch 389, CIFAR-10 Batch 3:  Loss:  0.0614339  Accuracy:  0.635\n",
      "Epoch 389, CIFAR-10 Batch 4:  Loss:  0.0398328  Accuracy:  0.6324\n",
      "Epoch 389, CIFAR-10 Batch 5:  Loss:  0.040494  Accuracy:  0.6288\n",
      "Epoch 390, CIFAR-10 Batch 1:  Loss:  0.0405335  Accuracy:  0.62\n",
      "Epoch 390, CIFAR-10 Batch 2:  Loss:  0.0621427  Accuracy:  0.6244\n",
      "Epoch 390, CIFAR-10 Batch 3:  Loss:  0.0399119  Accuracy:  0.6444\n",
      "Epoch 390, CIFAR-10 Batch 4:  Loss:  0.0412126  Accuracy:  0.6344\n",
      "Epoch 390, CIFAR-10 Batch 5:  Loss:  0.0344749  Accuracy:  0.6372\n",
      "Epoch 391, CIFAR-10 Batch 1:  Loss:  0.0514209  Accuracy:  0.6326\n",
      "Epoch 391, CIFAR-10 Batch 2:  Loss:  0.0547684  Accuracy:  0.6304\n",
      "Epoch 391, CIFAR-10 Batch 3:  Loss:  0.066478  Accuracy:  0.6154\n",
      "Epoch 391, CIFAR-10 Batch 4:  Loss:  0.0572456  Accuracy:  0.6232\n",
      "Epoch 391, CIFAR-10 Batch 5:  Loss:  0.0401048  Accuracy:  0.6212\n",
      "Epoch 392, CIFAR-10 Batch 1:  Loss:  0.0487829  Accuracy:  0.6242\n",
      "Epoch 392, CIFAR-10 Batch 2:  Loss:  0.0333014  Accuracy:  0.631\n",
      "Epoch 392, CIFAR-10 Batch 3:  Loss:  0.0532602  Accuracy:  0.63\n",
      "Epoch 392, CIFAR-10 Batch 4:  Loss:  0.0617951  Accuracy:  0.627\n",
      "Epoch 392, CIFAR-10 Batch 5:  Loss:  0.0499439  Accuracy:  0.635\n",
      "Epoch 393, CIFAR-10 Batch 1:  Loss:  0.0686229  Accuracy:  0.6272\n",
      "Epoch 393, CIFAR-10 Batch 2:  Loss:  0.0450013  Accuracy:  0.6296\n",
      "Epoch 393, CIFAR-10 Batch 3:  Loss:  0.0365504  Accuracy:  0.6348\n",
      "Epoch 393, CIFAR-10 Batch 4:  Loss:  0.0499009  Accuracy:  0.6278\n",
      "Epoch 393, CIFAR-10 Batch 5:  Loss:  0.0343615  Accuracy:  0.6342\n",
      "Epoch 394, CIFAR-10 Batch 1:  Loss:  0.0506839  Accuracy:  0.6332\n",
      "Epoch 394, CIFAR-10 Batch 2:  Loss:  0.0569241  Accuracy:  0.633\n",
      "Epoch 394, CIFAR-10 Batch 3:  Loss:  0.0362121  Accuracy:  0.638\n",
      "Epoch 394, CIFAR-10 Batch 4:  Loss:  0.0690388  Accuracy:  0.6244\n",
      "Epoch 394, CIFAR-10 Batch 5:  Loss:  0.0466566  Accuracy:  0.633\n",
      "Epoch 395, CIFAR-10 Batch 1:  Loss:  0.0426444  Accuracy:  0.6294\n",
      "Epoch 395, CIFAR-10 Batch 2:  Loss:  0.0495836  Accuracy:  0.6328\n",
      "Epoch 395, CIFAR-10 Batch 3:  Loss:  0.0335308  Accuracy:  0.6364\n",
      "Epoch 395, CIFAR-10 Batch 4:  Loss:  0.0326116  Accuracy:  0.6272\n",
      "Epoch 395, CIFAR-10 Batch 5:  Loss:  0.0394588  Accuracy:  0.6302\n",
      "Epoch 396, CIFAR-10 Batch 1:  Loss:  0.0382536  Accuracy:  0.6316\n",
      "Epoch 396, CIFAR-10 Batch 2:  Loss:  0.0321793  Accuracy:  0.633\n",
      "Epoch 396, CIFAR-10 Batch 3:  Loss:  0.0692952  Accuracy:  0.6378\n",
      "Epoch 396, CIFAR-10 Batch 4:  Loss:  0.0444294  Accuracy:  0.633\n",
      "Epoch 396, CIFAR-10 Batch 5:  Loss:  0.0526401  Accuracy:  0.6312\n",
      "Epoch 397, CIFAR-10 Batch 1:  Loss:  0.0426133  Accuracy:  0.6374\n",
      "Epoch 397, CIFAR-10 Batch 2:  Loss:  0.0262669  Accuracy:  0.6428\n",
      "Epoch 397, CIFAR-10 Batch 3:  Loss:  0.0388403  Accuracy:  0.634\n",
      "Epoch 397, CIFAR-10 Batch 4:  Loss:  0.0521249  Accuracy:  0.6198\n",
      "Epoch 397, CIFAR-10 Batch 5:  Loss:  0.0573126  Accuracy:  0.6278\n",
      "Epoch 398, CIFAR-10 Batch 1:  Loss:  0.0412249  Accuracy:  0.625\n",
      "Epoch 398, CIFAR-10 Batch 2:  Loss:  0.0426413  Accuracy:  0.6352\n",
      "Epoch 398, CIFAR-10 Batch 3:  Loss:  0.0457629  Accuracy:  0.6396\n",
      "Epoch 398, CIFAR-10 Batch 4:  Loss:  0.0406012  Accuracy:  0.635\n",
      "Epoch 398, CIFAR-10 Batch 5:  Loss:  0.0273129  Accuracy:  0.6348\n",
      "Epoch 399, CIFAR-10 Batch 1:  Loss:  0.0373812  Accuracy:  0.6362\n",
      "Epoch 399, CIFAR-10 Batch 2:  Loss:  0.0333805  Accuracy:  0.6366\n",
      "Epoch 399, CIFAR-10 Batch 3:  Loss:  0.0471778  Accuracy:  0.6374\n",
      "Epoch 399, CIFAR-10 Batch 4:  Loss:  0.0589474  Accuracy:  0.6308\n",
      "Epoch 399, CIFAR-10 Batch 5:  Loss:  0.0301553  Accuracy:  0.6336\n",
      "Epoch 400, CIFAR-10 Batch 1:  Loss:  0.025183  Accuracy:  0.6376\n",
      "Epoch 400, CIFAR-10 Batch 2:  Loss:  0.059117  Accuracy:  0.6248\n",
      "Epoch 400, CIFAR-10 Batch 3:  Loss:  0.0385946  Accuracy:  0.636\n",
      "Epoch 400, CIFAR-10 Batch 4:  Loss:  0.0298966  Accuracy:  0.6296\n",
      "Epoch 400, CIFAR-10 Batch 5:  Loss:  0.0248315  Accuracy:  0.6386\n",
      "Epoch 401, CIFAR-10 Batch 1:  Loss:  0.0378022  Accuracy:  0.6336\n",
      "Epoch 401, CIFAR-10 Batch 2:  Loss:  0.0491551  Accuracy:  0.631\n",
      "Epoch 401, CIFAR-10 Batch 3:  Loss:  0.0437001  Accuracy:  0.6398\n",
      "Epoch 401, CIFAR-10 Batch 4:  Loss:  0.0433243  Accuracy:  0.6338\n",
      "Epoch 401, CIFAR-10 Batch 5:  Loss:  0.0313967  Accuracy:  0.627\n",
      "Epoch 402, CIFAR-10 Batch 1:  Loss:  0.0547883  Accuracy:  0.6362\n",
      "Epoch 402, CIFAR-10 Batch 2:  Loss:  0.0523868  Accuracy:  0.6322\n",
      "Epoch 402, CIFAR-10 Batch 3:  Loss:  0.0544116  Accuracy:  0.6358\n",
      "Epoch 402, CIFAR-10 Batch 4:  Loss:  0.0372966  Accuracy:  0.6368\n",
      "Epoch 402, CIFAR-10 Batch 5:  Loss:  0.0445405  Accuracy:  0.6248\n",
      "Epoch 403, CIFAR-10 Batch 1:  Loss:  0.0250534  Accuracy:  0.633\n",
      "Epoch 403, CIFAR-10 Batch 2:  Loss:  0.0453265  Accuracy:  0.6144\n",
      "Epoch 403, CIFAR-10 Batch 3:  Loss:  0.0448302  Accuracy:  0.6284\n",
      "Epoch 403, CIFAR-10 Batch 4:  Loss:  0.0824878  Accuracy:  0.6174\n",
      "Epoch 403, CIFAR-10 Batch 5:  Loss:  0.0439286  Accuracy:  0.626\n",
      "Epoch 404, CIFAR-10 Batch 1:  Loss:  0.0526515  Accuracy:  0.6308\n",
      "Epoch 404, CIFAR-10 Batch 2:  Loss:  0.053905  Accuracy:  0.6322\n",
      "Epoch 404, CIFAR-10 Batch 3:  Loss:  0.0366887  Accuracy:  0.6358\n",
      "Epoch 404, CIFAR-10 Batch 4:  Loss:  0.0528659  Accuracy:  0.6228\n",
      "Epoch 404, CIFAR-10 Batch 5:  Loss:  0.0436763  Accuracy:  0.6302\n",
      "Epoch 405, CIFAR-10 Batch 1:  Loss:  0.0357514  Accuracy:  0.63\n",
      "Epoch 405, CIFAR-10 Batch 2:  Loss:  0.0281078  Accuracy:  0.6236\n",
      "Epoch 405, CIFAR-10 Batch 3:  Loss:  0.0428912  Accuracy:  0.6368\n",
      "Epoch 405, CIFAR-10 Batch 4:  Loss:  0.0522285  Accuracy:  0.6218\n",
      "Epoch 405, CIFAR-10 Batch 5:  Loss:  0.0487485  Accuracy:  0.6262\n",
      "Epoch 406, CIFAR-10 Batch 1:  Loss:  0.0505711  Accuracy:  0.6346\n",
      "Epoch 406, CIFAR-10 Batch 2:  Loss:  0.040609  Accuracy:  0.635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406, CIFAR-10 Batch 3:  Loss:  0.0470388  Accuracy:  0.6278\n",
      "Epoch 406, CIFAR-10 Batch 4:  Loss:  0.0251788  Accuracy:  0.6298\n",
      "Epoch 406, CIFAR-10 Batch 5:  Loss:  0.0463264  Accuracy:  0.6312\n",
      "Epoch 407, CIFAR-10 Batch 1:  Loss:  0.0362186  Accuracy:  0.642\n",
      "Epoch 407, CIFAR-10 Batch 2:  Loss:  0.0625179  Accuracy:  0.6232\n",
      "Epoch 407, CIFAR-10 Batch 3:  Loss:  0.0277453  Accuracy:  0.6376\n",
      "Epoch 407, CIFAR-10 Batch 4:  Loss:  0.0251431  Accuracy:  0.6318\n",
      "Epoch 407, CIFAR-10 Batch 5:  Loss:  0.0445301  Accuracy:  0.6372\n",
      "Epoch 408, CIFAR-10 Batch 1:  Loss:  0.0421509  Accuracy:  0.6366\n",
      "Epoch 408, CIFAR-10 Batch 2:  Loss:  0.0461178  Accuracy:  0.6176\n",
      "Epoch 408, CIFAR-10 Batch 3:  Loss:  0.0804909  Accuracy:  0.6232\n",
      "Epoch 408, CIFAR-10 Batch 4:  Loss:  0.0619022  Accuracy:  0.6168\n",
      "Epoch 408, CIFAR-10 Batch 5:  Loss:  0.040309  Accuracy:  0.6356\n",
      "Epoch 409, CIFAR-10 Batch 1:  Loss:  0.0517784  Accuracy:  0.626\n",
      "Epoch 409, CIFAR-10 Batch 2:  Loss:  0.0287657  Accuracy:  0.6282\n",
      "Epoch 409, CIFAR-10 Batch 3:  Loss:  0.0724244  Accuracy:  0.6332\n",
      "Epoch 409, CIFAR-10 Batch 4:  Loss:  0.041263  Accuracy:  0.6238\n",
      "Epoch 409, CIFAR-10 Batch 5:  Loss:  0.0272058  Accuracy:  0.6272\n",
      "Epoch 410, CIFAR-10 Batch 1:  Loss:  0.0629442  Accuracy:  0.6312\n",
      "Epoch 410, CIFAR-10 Batch 2:  Loss:  0.0421563  Accuracy:  0.6308\n",
      "Epoch 410, CIFAR-10 Batch 3:  Loss:  0.0384478  Accuracy:  0.6338\n",
      "Epoch 410, CIFAR-10 Batch 4:  Loss:  0.0508859  Accuracy:  0.6302\n",
      "Epoch 410, CIFAR-10 Batch 5:  Loss:  0.027439  Accuracy:  0.6322\n",
      "Epoch 411, CIFAR-10 Batch 1:  Loss:  0.037353  Accuracy:  0.6326\n",
      "Epoch 411, CIFAR-10 Batch 2:  Loss:  0.0240224  Accuracy:  0.6248\n",
      "Epoch 411, CIFAR-10 Batch 3:  Loss:  0.026614  Accuracy:  0.6298\n",
      "Epoch 411, CIFAR-10 Batch 4:  Loss:  0.0417684  Accuracy:  0.6274\n",
      "Epoch 411, CIFAR-10 Batch 5:  Loss:  0.0272471  Accuracy:  0.6318\n",
      "Epoch 412, CIFAR-10 Batch 1:  Loss:  0.0290836  Accuracy:  0.6314\n",
      "Epoch 412, CIFAR-10 Batch 2:  Loss:  0.0358949  Accuracy:  0.6264\n",
      "Epoch 412, CIFAR-10 Batch 3:  Loss:  0.0354687  Accuracy:  0.6428\n",
      "Epoch 412, CIFAR-10 Batch 4:  Loss:  0.0497819  Accuracy:  0.6334\n",
      "Epoch 412, CIFAR-10 Batch 5:  Loss:  0.033601  Accuracy:  0.6354\n",
      "Epoch 413, CIFAR-10 Batch 1:  Loss:  0.0369814  Accuracy:  0.6356\n",
      "Epoch 413, CIFAR-10 Batch 2:  Loss:  0.0582663  Accuracy:  0.6406\n",
      "Epoch 413, CIFAR-10 Batch 3:  Loss:  0.0272486  Accuracy:  0.6386\n",
      "Epoch 413, CIFAR-10 Batch 4:  Loss:  0.031836  Accuracy:  0.6308\n",
      "Epoch 413, CIFAR-10 Batch 5:  Loss:  0.031785  Accuracy:  0.6282\n",
      "Epoch 414, CIFAR-10 Batch 1:  Loss:  0.0312894  Accuracy:  0.6364\n",
      "Epoch 414, CIFAR-10 Batch 2:  Loss:  0.0357503  Accuracy:  0.6264\n",
      "Epoch 414, CIFAR-10 Batch 3:  Loss:  0.0388879  Accuracy:  0.6322\n",
      "Epoch 414, CIFAR-10 Batch 4:  Loss:  0.0499502  Accuracy:  0.6308\n",
      "Epoch 414, CIFAR-10 Batch 5:  Loss:  0.0341794  Accuracy:  0.6384\n",
      "Epoch 415, CIFAR-10 Batch 1:  Loss:  0.0427505  Accuracy:  0.6332\n",
      "Epoch 415, CIFAR-10 Batch 2:  Loss:  0.0468099  Accuracy:  0.6224\n",
      "Epoch 415, CIFAR-10 Batch 3:  Loss:  0.0286536  Accuracy:  0.6416\n",
      "Epoch 415, CIFAR-10 Batch 4:  Loss:  0.0509595  Accuracy:  0.63\n",
      "Epoch 415, CIFAR-10 Batch 5:  Loss:  0.0372565  Accuracy:  0.634\n",
      "Epoch 416, CIFAR-10 Batch 1:  Loss:  0.0407587  Accuracy:  0.6342\n",
      "Epoch 416, CIFAR-10 Batch 2:  Loss:  0.043755  Accuracy:  0.6226\n",
      "Epoch 416, CIFAR-10 Batch 3:  Loss:  0.0682813  Accuracy:  0.6284\n",
      "Epoch 416, CIFAR-10 Batch 4:  Loss:  0.0362162  Accuracy:  0.636\n",
      "Epoch 416, CIFAR-10 Batch 5:  Loss:  0.0338572  Accuracy:  0.632\n",
      "Epoch 417, CIFAR-10 Batch 1:  Loss:  0.0469882  Accuracy:  0.6398\n",
      "Epoch 417, CIFAR-10 Batch 2:  Loss:  0.0533185  Accuracy:  0.6194\n",
      "Epoch 417, CIFAR-10 Batch 3:  Loss:  0.031192  Accuracy:  0.6348\n",
      "Epoch 417, CIFAR-10 Batch 4:  Loss:  0.0403208  Accuracy:  0.639\n",
      "Epoch 417, CIFAR-10 Batch 5:  Loss:  0.0423484  Accuracy:  0.627\n",
      "Epoch 418, CIFAR-10 Batch 1:  Loss:  0.0483118  Accuracy:  0.6444\n",
      "Epoch 418, CIFAR-10 Batch 2:  Loss:  0.0302089  Accuracy:  0.6366\n",
      "Epoch 418, CIFAR-10 Batch 3:  Loss:  0.0318305  Accuracy:  0.632\n",
      "Epoch 418, CIFAR-10 Batch 4:  Loss:  0.0293639  Accuracy:  0.6422\n",
      "Epoch 418, CIFAR-10 Batch 5:  Loss:  0.040722  Accuracy:  0.6274\n",
      "Epoch 419, CIFAR-10 Batch 1:  Loss:  0.0561192  Accuracy:  0.6366\n",
      "Epoch 419, CIFAR-10 Batch 2:  Loss:  0.0526746  Accuracy:  0.6376\n",
      "Epoch 419, CIFAR-10 Batch 3:  Loss:  0.0434164  Accuracy:  0.6286\n",
      "Epoch 419, CIFAR-10 Batch 4:  Loss:  0.0500922  Accuracy:  0.6356\n",
      "Epoch 419, CIFAR-10 Batch 5:  Loss:  0.0398382  Accuracy:  0.6358\n",
      "Epoch 420, CIFAR-10 Batch 1:  Loss:  0.0379745  Accuracy:  0.6376\n",
      "Epoch 420, CIFAR-10 Batch 2:  Loss:  0.0507879  Accuracy:  0.6218\n",
      "Epoch 420, CIFAR-10 Batch 3:  Loss:  0.0459721  Accuracy:  0.6274\n",
      "Epoch 420, CIFAR-10 Batch 4:  Loss:  0.0613317  Accuracy:  0.6286\n",
      "Epoch 420, CIFAR-10 Batch 5:  Loss:  0.057792  Accuracy:  0.631\n",
      "Epoch 421, CIFAR-10 Batch 1:  Loss:  0.0486493  Accuracy:  0.636\n",
      "Epoch 421, CIFAR-10 Batch 2:  Loss:  0.0362954  Accuracy:  0.6344\n",
      "Epoch 421, CIFAR-10 Batch 3:  Loss:  0.0267231  Accuracy:  0.6298\n",
      "Epoch 421, CIFAR-10 Batch 4:  Loss:  0.0383162  Accuracy:  0.6356\n",
      "Epoch 421, CIFAR-10 Batch 5:  Loss:  0.0611644  Accuracy:  0.6296\n",
      "Epoch 422, CIFAR-10 Batch 1:  Loss:  0.0411235  Accuracy:  0.6446\n",
      "Epoch 422, CIFAR-10 Batch 2:  Loss:  0.0507211  Accuracy:  0.6382\n",
      "Epoch 422, CIFAR-10 Batch 3:  Loss:  0.0455976  Accuracy:  0.6416\n",
      "Epoch 422, CIFAR-10 Batch 4:  Loss:  0.0254884  Accuracy:  0.632\n",
      "Epoch 422, CIFAR-10 Batch 5:  Loss:  0.0505205  Accuracy:  0.6316\n",
      "Epoch 423, CIFAR-10 Batch 1:  Loss:  0.0917843  Accuracy:  0.636\n",
      "Epoch 423, CIFAR-10 Batch 2:  Loss:  0.0380596  Accuracy:  0.6358\n",
      "Epoch 423, CIFAR-10 Batch 3:  Loss:  0.0316256  Accuracy:  0.6366\n",
      "Epoch 423, CIFAR-10 Batch 4:  Loss:  0.0847611  Accuracy:  0.6204\n",
      "Epoch 423, CIFAR-10 Batch 5:  Loss:  0.0369291  Accuracy:  0.6294\n",
      "Epoch 424, CIFAR-10 Batch 1:  Loss:  0.057104  Accuracy:  0.6346\n",
      "Epoch 424, CIFAR-10 Batch 2:  Loss:  0.024792  Accuracy:  0.6394\n",
      "Epoch 424, CIFAR-10 Batch 3:  Loss:  0.0368767  Accuracy:  0.6406\n",
      "Epoch 424, CIFAR-10 Batch 4:  Loss:  0.0504171  Accuracy:  0.6326\n",
      "Epoch 424, CIFAR-10 Batch 5:  Loss:  0.0384905  Accuracy:  0.6388\n",
      "Epoch 425, CIFAR-10 Batch 1:  Loss:  0.0315225  Accuracy:  0.6406\n",
      "Epoch 425, CIFAR-10 Batch 2:  Loss:  0.0380213  Accuracy:  0.6376\n",
      "Epoch 425, CIFAR-10 Batch 3:  Loss:  0.0543279  Accuracy:  0.6388\n",
      "Epoch 425, CIFAR-10 Batch 4:  Loss:  0.0536538  Accuracy:  0.6304\n",
      "Epoch 425, CIFAR-10 Batch 5:  Loss:  0.0505051  Accuracy:  0.6292\n",
      "Epoch 426, CIFAR-10 Batch 1:  Loss:  0.0319553  Accuracy:  0.642\n",
      "Epoch 426, CIFAR-10 Batch 2:  Loss:  0.061061  Accuracy:  0.6256\n",
      "Epoch 426, CIFAR-10 Batch 3:  Loss:  0.0760014  Accuracy:  0.6402\n",
      "Epoch 426, CIFAR-10 Batch 4:  Loss:  0.0525053  Accuracy:  0.6284\n",
      "Epoch 426, CIFAR-10 Batch 5:  Loss:  0.0421261  Accuracy:  0.6186\n",
      "Epoch 427, CIFAR-10 Batch 1:  Loss:  0.0510021  Accuracy:  0.636\n",
      "Epoch 427, CIFAR-10 Batch 2:  Loss:  0.0285975  Accuracy:  0.626\n",
      "Epoch 427, CIFAR-10 Batch 3:  Loss:  0.0454885  Accuracy:  0.6372\n",
      "Epoch 427, CIFAR-10 Batch 4:  Loss:  0.0479516  Accuracy:  0.6356\n",
      "Epoch 427, CIFAR-10 Batch 5:  Loss:  0.0348673  Accuracy:  0.62\n",
      "Epoch 428, CIFAR-10 Batch 1:  Loss:  0.0414777  Accuracy:  0.63\n",
      "Epoch 428, CIFAR-10 Batch 2:  Loss:  0.0439658  Accuracy:  0.6368\n",
      "Epoch 428, CIFAR-10 Batch 3:  Loss:  0.0330912  Accuracy:  0.6364\n",
      "Epoch 428, CIFAR-10 Batch 4:  Loss:  0.0374786  Accuracy:  0.6352\n",
      "Epoch 428, CIFAR-10 Batch 5:  Loss:  0.05732  Accuracy:  0.6234\n",
      "Epoch 429, CIFAR-10 Batch 1:  Loss:  0.0431317  Accuracy:  0.6342\n",
      "Epoch 429, CIFAR-10 Batch 2:  Loss:  0.0660903  Accuracy:  0.6364\n",
      "Epoch 429, CIFAR-10 Batch 3:  Loss:  0.027987  Accuracy:  0.6322\n",
      "Epoch 429, CIFAR-10 Batch 4:  Loss:  0.0508891  Accuracy:  0.6338\n",
      "Epoch 429, CIFAR-10 Batch 5:  Loss:  0.0688932  Accuracy:  0.6262\n",
      "Epoch 430, CIFAR-10 Batch 1:  Loss:  0.0352112  Accuracy:  0.631\n",
      "Epoch 430, CIFAR-10 Batch 2:  Loss:  0.0319393  Accuracy:  0.6386\n",
      "Epoch 430, CIFAR-10 Batch 3:  Loss:  0.0439383  Accuracy:  0.64\n",
      "Epoch 430, CIFAR-10 Batch 4:  Loss:  0.0395584  Accuracy:  0.6396\n",
      "Epoch 430, CIFAR-10 Batch 5:  Loss:  0.0333446  Accuracy:  0.6406\n",
      "Epoch 431, CIFAR-10 Batch 1:  Loss:  0.0331001  Accuracy:  0.636\n",
      "Epoch 431, CIFAR-10 Batch 2:  Loss:  0.0374927  Accuracy:  0.6316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431, CIFAR-10 Batch 3:  Loss:  0.043639  Accuracy:  0.6296\n",
      "Epoch 431, CIFAR-10 Batch 4:  Loss:  0.0319559  Accuracy:  0.635\n",
      "Epoch 431, CIFAR-10 Batch 5:  Loss:  0.0288153  Accuracy:  0.6298\n",
      "Epoch 432, CIFAR-10 Batch 1:  Loss:  0.0316803  Accuracy:  0.6312\n",
      "Epoch 432, CIFAR-10 Batch 2:  Loss:  0.0446116  Accuracy:  0.629\n",
      "Epoch 432, CIFAR-10 Batch 3:  Loss:  0.0327989  Accuracy:  0.6326\n",
      "Epoch 432, CIFAR-10 Batch 4:  Loss:  0.020216  Accuracy:  0.6376\n",
      "Epoch 432, CIFAR-10 Batch 5:  Loss:  0.0372842  Accuracy:  0.6284\n",
      "Epoch 433, CIFAR-10 Batch 1:  Loss:  0.0329176  Accuracy:  0.6454\n",
      "Epoch 433, CIFAR-10 Batch 2:  Loss:  0.0537337  Accuracy:  0.6292\n",
      "Epoch 433, CIFAR-10 Batch 3:  Loss:  0.0509129  Accuracy:  0.6326\n",
      "Epoch 433, CIFAR-10 Batch 4:  Loss:  0.0374561  Accuracy:  0.6354\n",
      "Epoch 433, CIFAR-10 Batch 5:  Loss:  0.048744  Accuracy:  0.6324\n",
      "Epoch 434, CIFAR-10 Batch 1:  Loss:  0.0516183  Accuracy:  0.6366\n",
      "Epoch 434, CIFAR-10 Batch 2:  Loss:  0.0405978  Accuracy:  0.6324\n",
      "Epoch 434, CIFAR-10 Batch 3:  Loss:  0.0421525  Accuracy:  0.6354\n",
      "Epoch 434, CIFAR-10 Batch 4:  Loss:  0.0304257  Accuracy:  0.6384\n",
      "Epoch 434, CIFAR-10 Batch 5:  Loss:  0.0428545  Accuracy:  0.6314\n",
      "Epoch 435, CIFAR-10 Batch 1:  Loss:  0.0230327  Accuracy:  0.6306\n",
      "Epoch 435, CIFAR-10 Batch 2:  Loss:  0.0217349  Accuracy:  0.647\n",
      "Epoch 435, CIFAR-10 Batch 3:  Loss:  0.0295399  Accuracy:  0.6306\n",
      "Epoch 435, CIFAR-10 Batch 4:  Loss:  0.0382299  Accuracy:  0.642\n",
      "Epoch 435, CIFAR-10 Batch 5:  Loss:  0.0448271  Accuracy:  0.6322\n",
      "Epoch 436, CIFAR-10 Batch 1:  Loss:  0.046228  Accuracy:  0.634\n",
      "Epoch 436, CIFAR-10 Batch 2:  Loss:  0.04895  Accuracy:  0.6456\n",
      "Epoch 436, CIFAR-10 Batch 3:  Loss:  0.0348894  Accuracy:  0.642\n",
      "Epoch 436, CIFAR-10 Batch 4:  Loss:  0.0449981  Accuracy:  0.6282\n",
      "Epoch 436, CIFAR-10 Batch 5:  Loss:  0.0341858  Accuracy:  0.6328\n",
      "Epoch 437, CIFAR-10 Batch 1:  Loss:  0.0520006  Accuracy:  0.638\n",
      "Epoch 437, CIFAR-10 Batch 2:  Loss:  0.0333858  Accuracy:  0.6356\n",
      "Epoch 437, CIFAR-10 Batch 3:  Loss:  0.0410638  Accuracy:  0.6356\n",
      "Epoch 437, CIFAR-10 Batch 4:  Loss:  0.0261339  Accuracy:  0.6436\n",
      "Epoch 437, CIFAR-10 Batch 5:  Loss:  0.0460152  Accuracy:  0.6214\n",
      "Epoch 438, CIFAR-10 Batch 1:  Loss:  0.0392159  Accuracy:  0.642\n",
      "Epoch 438, CIFAR-10 Batch 2:  Loss:  0.0283123  Accuracy:  0.643\n",
      "Epoch 438, CIFAR-10 Batch 3:  Loss:  0.0250812  Accuracy:  0.6342\n",
      "Epoch 438, CIFAR-10 Batch 4:  Loss:  0.0286777  Accuracy:  0.6438\n",
      "Epoch 438, CIFAR-10 Batch 5:  Loss:  0.0358417  Accuracy:  0.6194\n",
      "Epoch 439, CIFAR-10 Batch 1:  Loss:  0.0348723  Accuracy:  0.6434\n",
      "Epoch 439, CIFAR-10 Batch 2:  Loss:  0.0226716  Accuracy:  0.6424\n",
      "Epoch 439, CIFAR-10 Batch 3:  Loss:  0.0350113  Accuracy:  0.645\n",
      "Epoch 439, CIFAR-10 Batch 4:  Loss:  0.0326322  Accuracy:  0.6486\n",
      "Epoch 439, CIFAR-10 Batch 5:  Loss:  0.0255689  Accuracy:  0.639\n",
      "Epoch 440, CIFAR-10 Batch 1:  Loss:  0.029552  Accuracy:  0.635\n",
      "Epoch 440, CIFAR-10 Batch 2:  Loss:  0.0335944  Accuracy:  0.6424\n",
      "Epoch 440, CIFAR-10 Batch 3:  Loss:  0.0231569  Accuracy:  0.6438\n",
      "Epoch 440, CIFAR-10 Batch 4:  Loss:  0.0412919  Accuracy:  0.6392\n",
      "Epoch 440, CIFAR-10 Batch 5:  Loss:  0.0327803  Accuracy:  0.6276\n",
      "Epoch 441, CIFAR-10 Batch 1:  Loss:  0.0424788  Accuracy:  0.6394\n",
      "Epoch 441, CIFAR-10 Batch 2:  Loss:  0.0153749  Accuracy:  0.632\n",
      "Epoch 441, CIFAR-10 Batch 3:  Loss:  0.0268939  Accuracy:  0.6354\n",
      "Epoch 441, CIFAR-10 Batch 4:  Loss:  0.0337299  Accuracy:  0.6356\n",
      "Epoch 441, CIFAR-10 Batch 5:  Loss:  0.0197694  Accuracy:  0.6306\n",
      "Epoch 442, CIFAR-10 Batch 1:  Loss:  0.0392051  Accuracy:  0.6346\n",
      "Epoch 442, CIFAR-10 Batch 2:  Loss:  0.0253512  Accuracy:  0.6416\n",
      "Epoch 442, CIFAR-10 Batch 3:  Loss:  0.0384353  Accuracy:  0.6372\n",
      "Epoch 442, CIFAR-10 Batch 4:  Loss:  0.0267581  Accuracy:  0.635\n",
      "Epoch 442, CIFAR-10 Batch 5:  Loss:  0.0199017  Accuracy:  0.6386\n",
      "Epoch 443, CIFAR-10 Batch 1:  Loss:  0.0280869  Accuracy:  0.6466\n",
      "Epoch 443, CIFAR-10 Batch 2:  Loss:  0.0297735  Accuracy:  0.634\n",
      "Epoch 443, CIFAR-10 Batch 3:  Loss:  0.0172822  Accuracy:  0.6428\n",
      "Epoch 443, CIFAR-10 Batch 4:  Loss:  0.0585575  Accuracy:  0.6368\n",
      "Epoch 443, CIFAR-10 Batch 5:  Loss:  0.0282389  Accuracy:  0.6328\n",
      "Epoch 444, CIFAR-10 Batch 1:  Loss:  0.029835  Accuracy:  0.648\n",
      "Epoch 444, CIFAR-10 Batch 2:  Loss:  0.0283519  Accuracy:  0.641\n",
      "Epoch 444, CIFAR-10 Batch 3:  Loss:  0.0315258  Accuracy:  0.6538\n",
      "Epoch 444, CIFAR-10 Batch 4:  Loss:  0.0337648  Accuracy:  0.6372\n",
      "Epoch 444, CIFAR-10 Batch 5:  Loss:  0.0225153  Accuracy:  0.6458\n",
      "Epoch 445, CIFAR-10 Batch 1:  Loss:  0.0361878  Accuracy:  0.645\n",
      "Epoch 445, CIFAR-10 Batch 2:  Loss:  0.0249443  Accuracy:  0.642\n",
      "Epoch 445, CIFAR-10 Batch 3:  Loss:  0.0362283  Accuracy:  0.6474\n",
      "Epoch 445, CIFAR-10 Batch 4:  Loss:  0.0349563  Accuracy:  0.6398\n",
      "Epoch 445, CIFAR-10 Batch 5:  Loss:  0.0228977  Accuracy:  0.6382\n",
      "Epoch 446, CIFAR-10 Batch 1:  Loss:  0.0133881  Accuracy:  0.643\n",
      "Epoch 446, CIFAR-10 Batch 2:  Loss:  0.0431011  Accuracy:  0.6426\n",
      "Epoch 446, CIFAR-10 Batch 3:  Loss:  0.0307936  Accuracy:  0.6442\n",
      "Epoch 446, CIFAR-10 Batch 4:  Loss:  0.0374651  Accuracy:  0.6432\n",
      "Epoch 446, CIFAR-10 Batch 5:  Loss:  0.0412708  Accuracy:  0.633\n",
      "Epoch 447, CIFAR-10 Batch 1:  Loss:  0.0291873  Accuracy:  0.6388\n",
      "Epoch 447, CIFAR-10 Batch 2:  Loss:  0.0333446  Accuracy:  0.6368\n",
      "Epoch 447, CIFAR-10 Batch 3:  Loss:  0.0338023  Accuracy:  0.6398\n",
      "Epoch 447, CIFAR-10 Batch 4:  Loss:  0.0263939  Accuracy:  0.6376\n",
      "Epoch 447, CIFAR-10 Batch 5:  Loss:  0.0319186  Accuracy:  0.6402\n",
      "Epoch 448, CIFAR-10 Batch 1:  Loss:  0.021559  Accuracy:  0.6436\n",
      "Epoch 448, CIFAR-10 Batch 2:  Loss:  0.0421736  Accuracy:  0.6362\n",
      "Epoch 448, CIFAR-10 Batch 3:  Loss:  0.0249895  Accuracy:  0.6376\n",
      "Epoch 448, CIFAR-10 Batch 4:  Loss:  0.0386206  Accuracy:  0.6476\n",
      "Epoch 448, CIFAR-10 Batch 5:  Loss:  0.0303466  Accuracy:  0.635\n",
      "Epoch 449, CIFAR-10 Batch 1:  Loss:  0.0313635  Accuracy:  0.6454\n",
      "Epoch 449, CIFAR-10 Batch 2:  Loss:  0.0239887  Accuracy:  0.6408\n",
      "Epoch 449, CIFAR-10 Batch 3:  Loss:  0.0401169  Accuracy:  0.6404\n",
      "Epoch 449, CIFAR-10 Batch 4:  Loss:  0.0400418  Accuracy:  0.6398\n",
      "Epoch 449, CIFAR-10 Batch 5:  Loss:  0.0294914  Accuracy:  0.6334\n",
      "Epoch 450, CIFAR-10 Batch 1:  Loss:  0.0234143  Accuracy:  0.6442\n",
      "Epoch 450, CIFAR-10 Batch 2:  Loss:  0.038783  Accuracy:  0.6484\n",
      "Epoch 450, CIFAR-10 Batch 3:  Loss:  0.0327598  Accuracy:  0.6436\n",
      "Epoch 450, CIFAR-10 Batch 4:  Loss:  0.0367939  Accuracy:  0.6478\n",
      "Epoch 450, CIFAR-10 Batch 5:  Loss:  0.0315042  Accuracy:  0.6368\n",
      "Epoch 451, CIFAR-10 Batch 1:  Loss:  0.0227446  Accuracy:  0.6408\n",
      "Epoch 451, CIFAR-10 Batch 2:  Loss:  0.0274821  Accuracy:  0.6464\n",
      "Epoch 451, CIFAR-10 Batch 3:  Loss:  0.0304543  Accuracy:  0.6362\n",
      "Epoch 451, CIFAR-10 Batch 4:  Loss:  0.0280754  Accuracy:  0.6398\n",
      "Epoch 451, CIFAR-10 Batch 5:  Loss:  0.033479  Accuracy:  0.6408\n",
      "Epoch 452, CIFAR-10 Batch 1:  Loss:  0.0466361  Accuracy:  0.6384\n",
      "Epoch 452, CIFAR-10 Batch 2:  Loss:  0.0200805  Accuracy:  0.64\n",
      "Epoch 452, CIFAR-10 Batch 3:  Loss:  0.0278263  Accuracy:  0.6444\n",
      "Epoch 452, CIFAR-10 Batch 4:  Loss:  0.0243141  Accuracy:  0.6468\n",
      "Epoch 452, CIFAR-10 Batch 5:  Loss:  0.0288856  Accuracy:  0.6306\n",
      "Epoch 453, CIFAR-10 Batch 1:  Loss:  0.0337817  Accuracy:  0.6354\n",
      "Epoch 453, CIFAR-10 Batch 2:  Loss:  0.0373184  Accuracy:  0.6388\n",
      "Epoch 453, CIFAR-10 Batch 3:  Loss:  0.0192029  Accuracy:  0.6452\n",
      "Epoch 453, CIFAR-10 Batch 4:  Loss:  0.0199527  Accuracy:  0.6408\n",
      "Epoch 453, CIFAR-10 Batch 5:  Loss:  0.0310853  Accuracy:  0.6318\n",
      "Epoch 454, CIFAR-10 Batch 1:  Loss:  0.04488  Accuracy:  0.6424\n",
      "Epoch 454, CIFAR-10 Batch 2:  Loss:  0.0404709  Accuracy:  0.6336\n",
      "Epoch 454, CIFAR-10 Batch 3:  Loss:  0.0351433  Accuracy:  0.6384\n",
      "Epoch 454, CIFAR-10 Batch 4:  Loss:  0.0196856  Accuracy:  0.6502\n",
      "Epoch 454, CIFAR-10 Batch 5:  Loss:  0.0447489  Accuracy:  0.6444\n",
      "Epoch 455, CIFAR-10 Batch 1:  Loss:  0.0215695  Accuracy:  0.6434\n",
      "Epoch 455, CIFAR-10 Batch 2:  Loss:  0.0338881  Accuracy:  0.6422\n",
      "Epoch 455, CIFAR-10 Batch 3:  Loss:  0.0235854  Accuracy:  0.6468\n",
      "Epoch 455, CIFAR-10 Batch 4:  Loss:  0.0309888  Accuracy:  0.6452\n",
      "Epoch 455, CIFAR-10 Batch 5:  Loss:  0.0247519  Accuracy:  0.6398\n",
      "Epoch 456, CIFAR-10 Batch 1:  Loss:  0.0275355  Accuracy:  0.6386\n",
      "Epoch 456, CIFAR-10 Batch 2:  Loss:  0.0228112  Accuracy:  0.644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456, CIFAR-10 Batch 3:  Loss:  0.0519076  Accuracy:  0.6362\n",
      "Epoch 456, CIFAR-10 Batch 4:  Loss:  0.0252189  Accuracy:  0.6418\n",
      "Epoch 456, CIFAR-10 Batch 5:  Loss:  0.0351863  Accuracy:  0.6368\n",
      "Epoch 457, CIFAR-10 Batch 1:  Loss:  0.0255275  Accuracy:  0.6364\n",
      "Epoch 457, CIFAR-10 Batch 2:  Loss:  0.036021  Accuracy:  0.6356\n",
      "Epoch 457, CIFAR-10 Batch 3:  Loss:  0.0332378  Accuracy:  0.646\n",
      "Epoch 457, CIFAR-10 Batch 4:  Loss:  0.0200054  Accuracy:  0.6396\n",
      "Epoch 457, CIFAR-10 Batch 5:  Loss:  0.025999  Accuracy:  0.6334\n",
      "Epoch 458, CIFAR-10 Batch 1:  Loss:  0.0143552  Accuracy:  0.6354\n",
      "Epoch 458, CIFAR-10 Batch 2:  Loss:  0.0183957  Accuracy:  0.6404\n",
      "Epoch 458, CIFAR-10 Batch 3:  Loss:  0.0292505  Accuracy:  0.6444\n",
      "Epoch 458, CIFAR-10 Batch 4:  Loss:  0.0323318  Accuracy:  0.6378\n",
      "Epoch 458, CIFAR-10 Batch 5:  Loss:  0.0340106  Accuracy:  0.6364\n",
      "Epoch 459, CIFAR-10 Batch 1:  Loss:  0.0362707  Accuracy:  0.638\n",
      "Epoch 459, CIFAR-10 Batch 2:  Loss:  0.0440934  Accuracy:  0.631\n",
      "Epoch 459, CIFAR-10 Batch 3:  Loss:  0.0443715  Accuracy:  0.6426\n",
      "Epoch 459, CIFAR-10 Batch 4:  Loss:  0.0277942  Accuracy:  0.6486\n",
      "Epoch 459, CIFAR-10 Batch 5:  Loss:  0.0440122  Accuracy:  0.6334\n",
      "Epoch 460, CIFAR-10 Batch 1:  Loss:  0.0371996  Accuracy:  0.6414\n",
      "Epoch 460, CIFAR-10 Batch 2:  Loss:  0.0340918  Accuracy:  0.6386\n",
      "Epoch 460, CIFAR-10 Batch 3:  Loss:  0.0294997  Accuracy:  0.6438\n",
      "Epoch 460, CIFAR-10 Batch 4:  Loss:  0.0356391  Accuracy:  0.6316\n",
      "Epoch 460, CIFAR-10 Batch 5:  Loss:  0.0378118  Accuracy:  0.6338\n",
      "Epoch 461, CIFAR-10 Batch 1:  Loss:  0.0302426  Accuracy:  0.6362\n",
      "Epoch 461, CIFAR-10 Batch 2:  Loss:  0.0276184  Accuracy:  0.6376\n",
      "Epoch 461, CIFAR-10 Batch 3:  Loss:  0.0248147  Accuracy:  0.6416\n",
      "Epoch 461, CIFAR-10 Batch 4:  Loss:  0.0371928  Accuracy:  0.6442\n",
      "Epoch 461, CIFAR-10 Batch 5:  Loss:  0.0405407  Accuracy:  0.6366\n",
      "Epoch 462, CIFAR-10 Batch 1:  Loss:  0.018908  Accuracy:  0.6422\n",
      "Epoch 462, CIFAR-10 Batch 2:  Loss:  0.0253224  Accuracy:  0.6368\n",
      "Epoch 462, CIFAR-10 Batch 3:  Loss:  0.0511764  Accuracy:  0.6392\n",
      "Epoch 462, CIFAR-10 Batch 4:  Loss:  0.0450994  Accuracy:  0.6442\n",
      "Epoch 462, CIFAR-10 Batch 5:  Loss:  0.0250031  Accuracy:  0.6372\n",
      "Epoch 463, CIFAR-10 Batch 1:  Loss:  0.0309939  Accuracy:  0.6378\n",
      "Epoch 463, CIFAR-10 Batch 2:  Loss:  0.0336912  Accuracy:  0.6464\n",
      "Epoch 463, CIFAR-10 Batch 3:  Loss:  0.0144575  Accuracy:  0.6438\n",
      "Epoch 463, CIFAR-10 Batch 4:  Loss:  0.0275728  Accuracy:  0.6414\n",
      "Epoch 463, CIFAR-10 Batch 5:  Loss:  0.0299571  Accuracy:  0.6308\n",
      "Epoch 464, CIFAR-10 Batch 1:  Loss:  0.0323195  Accuracy:  0.6314\n",
      "Epoch 464, CIFAR-10 Batch 2:  Loss:  0.0375015  Accuracy:  0.6402\n",
      "Epoch 464, CIFAR-10 Batch 3:  Loss:  0.0289774  Accuracy:  0.6378\n",
      "Epoch 464, CIFAR-10 Batch 4:  Loss:  0.0246031  Accuracy:  0.6418\n",
      "Epoch 464, CIFAR-10 Batch 5:  Loss:  0.0280843  Accuracy:  0.631\n",
      "Epoch 465, CIFAR-10 Batch 1:  Loss:  0.0377794  Accuracy:  0.6348\n",
      "Epoch 465, CIFAR-10 Batch 2:  Loss:  0.0214987  Accuracy:  0.642\n",
      "Epoch 465, CIFAR-10 Batch 3:  Loss:  0.0343441  Accuracy:  0.6388\n",
      "Epoch 465, CIFAR-10 Batch 4:  Loss:  0.0246185  Accuracy:  0.6496\n",
      "Epoch 465, CIFAR-10 Batch 5:  Loss:  0.0344919  Accuracy:  0.6336\n",
      "Epoch 466, CIFAR-10 Batch 1:  Loss:  0.0271695  Accuracy:  0.6374\n",
      "Epoch 466, CIFAR-10 Batch 2:  Loss:  0.0407189  Accuracy:  0.636\n",
      "Epoch 466, CIFAR-10 Batch 3:  Loss:  0.0274667  Accuracy:  0.6412\n",
      "Epoch 466, CIFAR-10 Batch 4:  Loss:  0.0162265  Accuracy:  0.6392\n",
      "Epoch 466, CIFAR-10 Batch 5:  Loss:  0.0416251  Accuracy:  0.6318\n",
      "Epoch 467, CIFAR-10 Batch 1:  Loss:  0.0348793  Accuracy:  0.635\n",
      "Epoch 467, CIFAR-10 Batch 2:  Loss:  0.039431  Accuracy:  0.6452\n",
      "Epoch 467, CIFAR-10 Batch 3:  Loss:  0.021912  Accuracy:  0.6398\n",
      "Epoch 467, CIFAR-10 Batch 4:  Loss:  0.0465966  Accuracy:  0.6388\n",
      "Epoch 467, CIFAR-10 Batch 5:  Loss:  0.0364122  Accuracy:  0.6358\n",
      "Epoch 468, CIFAR-10 Batch 1:  Loss:  0.0402888  Accuracy:  0.6348\n",
      "Epoch 468, CIFAR-10 Batch 2:  Loss:  0.0247165  Accuracy:  0.6372\n",
      "Epoch 468, CIFAR-10 Batch 3:  Loss:  0.0251551  Accuracy:  0.6472\n",
      "Epoch 468, CIFAR-10 Batch 4:  Loss:  0.0278973  Accuracy:  0.6372\n",
      "Epoch 468, CIFAR-10 Batch 5:  Loss:  0.0243368  Accuracy:  0.6318\n",
      "Epoch 469, CIFAR-10 Batch 1:  Loss:  0.0371214  Accuracy:  0.6468\n",
      "Epoch 469, CIFAR-10 Batch 2:  Loss:  0.0295721  Accuracy:  0.6402\n",
      "Epoch 469, CIFAR-10 Batch 3:  Loss:  0.0311615  Accuracy:  0.6466\n",
      "Epoch 469, CIFAR-10 Batch 4:  Loss:  0.0250083  Accuracy:  0.6468\n",
      "Epoch 469, CIFAR-10 Batch 5:  Loss:  0.0294071  Accuracy:  0.6422\n",
      "Epoch 470, CIFAR-10 Batch 1:  Loss:  0.0369537  Accuracy:  0.6438\n",
      "Epoch 470, CIFAR-10 Batch 2:  Loss:  0.0520148  Accuracy:  0.6328\n",
      "Epoch 470, CIFAR-10 Batch 3:  Loss:  0.0287031  Accuracy:  0.6364\n",
      "Epoch 470, CIFAR-10 Batch 4:  Loss:  0.0259959  Accuracy:  0.641\n",
      "Epoch 470, CIFAR-10 Batch 5:  Loss:  0.0208459  Accuracy:  0.647\n",
      "Epoch 471, CIFAR-10 Batch 1:  Loss:  0.0354045  Accuracy:  0.6394\n",
      "Epoch 471, CIFAR-10 Batch 2:  Loss:  0.0295223  Accuracy:  0.6352\n",
      "Epoch 471, CIFAR-10 Batch 3:  Loss:  0.02627  Accuracy:  0.6388\n",
      "Epoch 471, CIFAR-10 Batch 4:  Loss:  0.0444183  Accuracy:  0.6424\n",
      "Epoch 471, CIFAR-10 Batch 5:  Loss:  0.0266228  Accuracy:  0.6466\n",
      "Epoch 472, CIFAR-10 Batch 1:  Loss:  0.0231499  Accuracy:  0.6374\n",
      "Epoch 472, CIFAR-10 Batch 2:  Loss:  0.0533842  Accuracy:  0.637\n",
      "Epoch 472, CIFAR-10 Batch 3:  Loss:  0.0229716  Accuracy:  0.6366\n",
      "Epoch 472, CIFAR-10 Batch 4:  Loss:  0.0429465  Accuracy:  0.638\n",
      "Epoch 472, CIFAR-10 Batch 5:  Loss:  0.0212149  Accuracy:  0.6444\n",
      "Epoch 473, CIFAR-10 Batch 1:  Loss:  0.0317025  Accuracy:  0.6388\n",
      "Epoch 473, CIFAR-10 Batch 2:  Loss:  0.0277072  Accuracy:  0.6456\n",
      "Epoch 473, CIFAR-10 Batch 3:  Loss:  0.0494664  Accuracy:  0.647\n",
      "Epoch 473, CIFAR-10 Batch 4:  Loss:  0.0285416  Accuracy:  0.6372\n",
      "Epoch 473, CIFAR-10 Batch 5:  Loss:  0.0485308  Accuracy:  0.6358\n",
      "Epoch 474, CIFAR-10 Batch 1:  Loss:  0.0649401  Accuracy:  0.6286\n",
      "Epoch 474, CIFAR-10 Batch 2:  Loss:  0.0501928  Accuracy:  0.639\n",
      "Epoch 474, CIFAR-10 Batch 3:  Loss:  0.0406102  Accuracy:  0.6376\n",
      "Epoch 474, CIFAR-10 Batch 4:  Loss:  0.0344844  Accuracy:  0.6362\n",
      "Epoch 474, CIFAR-10 Batch 5:  Loss:  0.0328181  Accuracy:  0.6404\n",
      "Epoch 475, CIFAR-10 Batch 1:  Loss:  0.0358205  Accuracy:  0.6328\n",
      "Epoch 475, CIFAR-10 Batch 2:  Loss:  0.0312374  Accuracy:  0.6398\n",
      "Epoch 475, CIFAR-10 Batch 3:  Loss:  0.0451816  Accuracy:  0.6368\n",
      "Epoch 475, CIFAR-10 Batch 4:  Loss:  0.0233083  Accuracy:  0.639\n",
      "Epoch 475, CIFAR-10 Batch 5:  Loss:  0.0271603  Accuracy:  0.642\n",
      "Epoch 476, CIFAR-10 Batch 1:  Loss:  0.0159247  Accuracy:  0.6334\n",
      "Epoch 476, CIFAR-10 Batch 2:  Loss:  0.016095  Accuracy:  0.6406\n",
      "Epoch 476, CIFAR-10 Batch 3:  Loss:  0.0427334  Accuracy:  0.6426\n",
      "Epoch 476, CIFAR-10 Batch 4:  Loss:  0.0218161  Accuracy:  0.6416\n",
      "Epoch 476, CIFAR-10 Batch 5:  Loss:  0.020925  Accuracy:  0.638\n",
      "Epoch 477, CIFAR-10 Batch 1:  Loss:  0.018032  Accuracy:  0.6418\n",
      "Epoch 477, CIFAR-10 Batch 2:  Loss:  0.036524  Accuracy:  0.6312\n",
      "Epoch 477, CIFAR-10 Batch 3:  Loss:  0.0483831  Accuracy:  0.6368\n",
      "Epoch 477, CIFAR-10 Batch 4:  Loss:  0.0220482  Accuracy:  0.634\n",
      "Epoch 477, CIFAR-10 Batch 5:  Loss:  0.0347417  Accuracy:  0.6322\n",
      "Epoch 478, CIFAR-10 Batch 1:  Loss:  0.0231462  Accuracy:  0.6354\n",
      "Epoch 478, CIFAR-10 Batch 2:  Loss:  0.0495007  Accuracy:  0.64\n",
      "Epoch 478, CIFAR-10 Batch 3:  Loss:  0.0202728  Accuracy:  0.6482\n",
      "Epoch 478, CIFAR-10 Batch 4:  Loss:  0.0534768  Accuracy:  0.6384\n",
      "Epoch 478, CIFAR-10 Batch 5:  Loss:  0.0418837  Accuracy:  0.6454\n",
      "Epoch 479, CIFAR-10 Batch 1:  Loss:  0.0400971  Accuracy:  0.6406\n",
      "Epoch 479, CIFAR-10 Batch 2:  Loss:  0.0262521  Accuracy:  0.6366\n",
      "Epoch 479, CIFAR-10 Batch 3:  Loss:  0.0468294  Accuracy:  0.6338\n",
      "Epoch 479, CIFAR-10 Batch 4:  Loss:  0.0337964  Accuracy:  0.6358\n",
      "Epoch 479, CIFAR-10 Batch 5:  Loss:  0.0187404  Accuracy:  0.6406\n",
      "Epoch 480, CIFAR-10 Batch 1:  Loss:  0.0421571  Accuracy:  0.6344\n",
      "Epoch 480, CIFAR-10 Batch 2:  Loss:  0.0258981  Accuracy:  0.6372\n",
      "Epoch 480, CIFAR-10 Batch 3:  Loss:  0.0274399  Accuracy:  0.6398\n",
      "Epoch 480, CIFAR-10 Batch 4:  Loss:  0.0257456  Accuracy:  0.647\n",
      "Epoch 480, CIFAR-10 Batch 5:  Loss:  0.0416069  Accuracy:  0.6298\n",
      "Epoch 481, CIFAR-10 Batch 1:  Loss:  0.0345574  Accuracy:  0.6424\n",
      "Epoch 481, CIFAR-10 Batch 2:  Loss:  0.0254764  Accuracy:  0.6368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481, CIFAR-10 Batch 3:  Loss:  0.0391354  Accuracy:  0.6384\n",
      "Epoch 481, CIFAR-10 Batch 4:  Loss:  0.0290736  Accuracy:  0.6374\n",
      "Epoch 481, CIFAR-10 Batch 5:  Loss:  0.0208123  Accuracy:  0.636\n",
      "Epoch 482, CIFAR-10 Batch 1:  Loss:  0.0246428  Accuracy:  0.6526\n",
      "Epoch 482, CIFAR-10 Batch 2:  Loss:  0.0310495  Accuracy:  0.6412\n",
      "Epoch 482, CIFAR-10 Batch 3:  Loss:  0.0481992  Accuracy:  0.6262\n",
      "Epoch 482, CIFAR-10 Batch 4:  Loss:  0.0267115  Accuracy:  0.6376\n",
      "Epoch 482, CIFAR-10 Batch 5:  Loss:  0.0350457  Accuracy:  0.644\n",
      "Epoch 483, CIFAR-10 Batch 1:  Loss:  0.0380132  Accuracy:  0.6394\n",
      "Epoch 483, CIFAR-10 Batch 2:  Loss:  0.0314678  Accuracy:  0.6382\n",
      "Epoch 483, CIFAR-10 Batch 3:  Loss:  0.0112879  Accuracy:  0.6444\n",
      "Epoch 483, CIFAR-10 Batch 4:  Loss:  0.0399361  Accuracy:  0.638\n",
      "Epoch 483, CIFAR-10 Batch 5:  Loss:  0.0297425  Accuracy:  0.6394\n",
      "Epoch 484, CIFAR-10 Batch 1:  Loss:  0.058192  Accuracy:  0.632\n",
      "Epoch 484, CIFAR-10 Batch 2:  Loss:  0.0190748  Accuracy:  0.6362\n",
      "Epoch 484, CIFAR-10 Batch 3:  Loss:  0.0159817  Accuracy:  0.6472\n",
      "Epoch 484, CIFAR-10 Batch 4:  Loss:  0.0168164  Accuracy:  0.6364\n",
      "Epoch 484, CIFAR-10 Batch 5:  Loss:  0.0457273  Accuracy:  0.627\n",
      "Epoch 485, CIFAR-10 Batch 1:  Loss:  0.0371936  Accuracy:  0.638\n",
      "Epoch 485, CIFAR-10 Batch 2:  Loss:  0.0409632  Accuracy:  0.6346\n",
      "Epoch 485, CIFAR-10 Batch 3:  Loss:  0.0395061  Accuracy:  0.6332\n",
      "Epoch 485, CIFAR-10 Batch 4:  Loss:  0.0253309  Accuracy:  0.6238\n",
      "Epoch 485, CIFAR-10 Batch 5:  Loss:  0.0328168  Accuracy:  0.626\n",
      "Epoch 486, CIFAR-10 Batch 1:  Loss:  0.0289547  Accuracy:  0.6316\n",
      "Epoch 486, CIFAR-10 Batch 2:  Loss:  0.0346481  Accuracy:  0.6338\n",
      "Epoch 486, CIFAR-10 Batch 3:  Loss:  0.0434525  Accuracy:  0.6386\n",
      "Epoch 486, CIFAR-10 Batch 4:  Loss:  0.0279197  Accuracy:  0.6378\n",
      "Epoch 486, CIFAR-10 Batch 5:  Loss:  0.0286515  Accuracy:  0.6356\n",
      "Epoch 487, CIFAR-10 Batch 1:  Loss:  0.0230624  Accuracy:  0.6374\n",
      "Epoch 487, CIFAR-10 Batch 2:  Loss:  0.0257667  Accuracy:  0.6364\n",
      "Epoch 487, CIFAR-10 Batch 3:  Loss:  0.0371911  Accuracy:  0.6326\n",
      "Epoch 487, CIFAR-10 Batch 4:  Loss:  0.036986  Accuracy:  0.637\n",
      "Epoch 487, CIFAR-10 Batch 5:  Loss:  0.0294723  Accuracy:  0.642\n",
      "Epoch 488, CIFAR-10 Batch 1:  Loss:  0.0278467  Accuracy:  0.6416\n",
      "Epoch 488, CIFAR-10 Batch 2:  Loss:  0.0390491  Accuracy:  0.6442\n",
      "Epoch 488, CIFAR-10 Batch 3:  Loss:  0.0416817  Accuracy:  0.6368\n",
      "Epoch 488, CIFAR-10 Batch 4:  Loss:  0.0333456  Accuracy:  0.6392\n",
      "Epoch 488, CIFAR-10 Batch 5:  Loss:  0.0367984  Accuracy:  0.634\n",
      "Epoch 489, CIFAR-10 Batch 1:  Loss:  0.0228798  Accuracy:  0.6348\n",
      "Epoch 489, CIFAR-10 Batch 2:  Loss:  0.022926  Accuracy:  0.6398\n",
      "Epoch 489, CIFAR-10 Batch 3:  Loss:  0.0232386  Accuracy:  0.647\n",
      "Epoch 489, CIFAR-10 Batch 4:  Loss:  0.0232486  Accuracy:  0.6434\n",
      "Epoch 489, CIFAR-10 Batch 5:  Loss:  0.0273886  Accuracy:  0.64\n",
      "Epoch 490, CIFAR-10 Batch 1:  Loss:  0.0276715  Accuracy:  0.6384\n",
      "Epoch 490, CIFAR-10 Batch 2:  Loss:  0.0289696  Accuracy:  0.641\n",
      "Epoch 490, CIFAR-10 Batch 3:  Loss:  0.0287624  Accuracy:  0.6488\n",
      "Epoch 490, CIFAR-10 Batch 4:  Loss:  0.0384577  Accuracy:  0.6338\n",
      "Epoch 490, CIFAR-10 Batch 5:  Loss:  0.0358962  Accuracy:  0.6406\n",
      "Epoch 491, CIFAR-10 Batch 1:  Loss:  0.0477829  Accuracy:  0.6324\n",
      "Epoch 491, CIFAR-10 Batch 2:  Loss:  0.0254011  Accuracy:  0.633\n",
      "Epoch 491, CIFAR-10 Batch 3:  Loss:  0.0322679  Accuracy:  0.6398\n",
      "Epoch 491, CIFAR-10 Batch 4:  Loss:  0.0191825  Accuracy:  0.6394\n",
      "Epoch 491, CIFAR-10 Batch 5:  Loss:  0.0306971  Accuracy:  0.6332\n",
      "Epoch 492, CIFAR-10 Batch 1:  Loss:  0.0398941  Accuracy:  0.6482\n",
      "Epoch 492, CIFAR-10 Batch 2:  Loss:  0.0196515  Accuracy:  0.6386\n",
      "Epoch 492, CIFAR-10 Batch 3:  Loss:  0.027108  Accuracy:  0.6442\n",
      "Epoch 492, CIFAR-10 Batch 4:  Loss:  0.0249717  Accuracy:  0.6446\n",
      "Epoch 492, CIFAR-10 Batch 5:  Loss:  0.0278707  Accuracy:  0.6252\n",
      "Epoch 493, CIFAR-10 Batch 1:  Loss:  0.0290736  Accuracy:  0.6364\n",
      "Epoch 493, CIFAR-10 Batch 2:  Loss:  0.0285405  Accuracy:  0.6418\n",
      "Epoch 493, CIFAR-10 Batch 3:  Loss:  0.017813  Accuracy:  0.637\n",
      "Epoch 493, CIFAR-10 Batch 4:  Loss:  0.0292184  Accuracy:  0.6448\n",
      "Epoch 493, CIFAR-10 Batch 5:  Loss:  0.0223815  Accuracy:  0.635\n",
      "Epoch 494, CIFAR-10 Batch 1:  Loss:  0.0252911  Accuracy:  0.6372\n",
      "Epoch 494, CIFAR-10 Batch 2:  Loss:  0.0281443  Accuracy:  0.6378\n",
      "Epoch 494, CIFAR-10 Batch 3:  Loss:  0.0265711  Accuracy:  0.6452\n",
      "Epoch 494, CIFAR-10 Batch 4:  Loss:  0.0357399  Accuracy:  0.6324\n",
      "Epoch 494, CIFAR-10 Batch 5:  Loss:  0.0338804  Accuracy:  0.6364\n",
      "Epoch 495, CIFAR-10 Batch 1:  Loss:  0.0354926  Accuracy:  0.6414\n",
      "Epoch 495, CIFAR-10 Batch 2:  Loss:  0.0282328  Accuracy:  0.6316\n",
      "Epoch 495, CIFAR-10 Batch 3:  Loss:  0.0248396  Accuracy:  0.656\n",
      "Epoch 495, CIFAR-10 Batch 4:  Loss:  0.0213543  Accuracy:  0.6406\n",
      "Epoch 495, CIFAR-10 Batch 5:  Loss:  0.0217984  Accuracy:  0.6302\n",
      "Epoch 496, CIFAR-10 Batch 1:  Loss:  0.027355  Accuracy:  0.6402\n",
      "Epoch 496, CIFAR-10 Batch 2:  Loss:  0.0258567  Accuracy:  0.6392\n",
      "Epoch 496, CIFAR-10 Batch 3:  Loss:  0.0356608  Accuracy:  0.6404\n",
      "Epoch 496, CIFAR-10 Batch 4:  Loss:  0.0233997  Accuracy:  0.6426\n",
      "Epoch 496, CIFAR-10 Batch 5:  Loss:  0.028829  Accuracy:  0.6302\n",
      "Epoch 497, CIFAR-10 Batch 1:  Loss:  0.0308147  Accuracy:  0.6396\n",
      "Epoch 497, CIFAR-10 Batch 2:  Loss:  0.0181128  Accuracy:  0.6422\n",
      "Epoch 497, CIFAR-10 Batch 3:  Loss:  0.0399212  Accuracy:  0.6358\n",
      "Epoch 497, CIFAR-10 Batch 4:  Loss:  0.030639  Accuracy:  0.6418\n",
      "Epoch 497, CIFAR-10 Batch 5:  Loss:  0.0200177  Accuracy:  0.6376\n",
      "Epoch 498, CIFAR-10 Batch 1:  Loss:  0.0194331  Accuracy:  0.6382\n",
      "Epoch 498, CIFAR-10 Batch 2:  Loss:  0.0239568  Accuracy:  0.6328\n",
      "Epoch 498, CIFAR-10 Batch 3:  Loss:  0.0235298  Accuracy:  0.6352\n",
      "Epoch 498, CIFAR-10 Batch 4:  Loss:  0.0292426  Accuracy:  0.646\n",
      "Epoch 498, CIFAR-10 Batch 5:  Loss:  0.0353482  Accuracy:  0.6334\n",
      "Epoch 499, CIFAR-10 Batch 1:  Loss:  0.0166678  Accuracy:  0.6402\n",
      "Epoch 499, CIFAR-10 Batch 2:  Loss:  0.019222  Accuracy:  0.6442\n",
      "Epoch 499, CIFAR-10 Batch 3:  Loss:  0.0367071  Accuracy:  0.6352\n",
      "Epoch 499, CIFAR-10 Batch 4:  Loss:  0.0270722  Accuracy:  0.6348\n",
      "Epoch 499, CIFAR-10 Batch 5:  Loss:  0.021013  Accuracy:  0.6396\n",
      "Epoch 500, CIFAR-10 Batch 1:  Loss:  0.0399901  Accuracy:  0.6326\n",
      "Epoch 500, CIFAR-10 Batch 2:  Loss:  0.0490327  Accuracy:  0.6412\n",
      "Epoch 500, CIFAR-10 Batch 3:  Loss:  0.0292939  Accuracy:  0.6434\n",
      "Epoch 500, CIFAR-10 Batch 4:  Loss:  0.02941  Accuracy:  0.6424\n",
      "Epoch 500, CIFAR-10 Batch 5:  Loss:  0.0228653  Accuracy:  0.6242\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6333999991416931\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZiWKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8207569e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
